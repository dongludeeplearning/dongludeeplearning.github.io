	<html>

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">	
	<title>Lu Dong</title>
	<meta content="LuDong, Lu Dong UB, lu dong ub, ludong,  https://dongludeeplearning.github.io/" name="keywords">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<script async="" src="http://www.google-analytics.com/analytics.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-66888300-1', 'auto');
		ga('send', 'pageview');
	</script>
	<script type="text/javascript" src="./index_files/jquery-1.12.4.min.js"></script>
	<style>
	a {color: #2471a3; text-decoration: none;}
	a:hover {color: #5499c7;}
	a.btn {background-color: #2471a3; color: #fff; font-size: 12px; margin-top: 4px; display: inline-block; padding: 0 4px; border-radius: 2px; line-height: 1.5em;}
	a.btn:hover {background-color: #174e74;}
	a.btn.btn-red {background-color: #d63a3a;}
	a.btn.btn-red:hover {background-color: #a02727;}
	a.btn.btn-orange {background-color: #e2700c;}
	a.btn.btn-orange:hover {background-color: #c46009;}
	a.btn.btn-dark {background-color: #345;}
	a.btn.btn-dark:hover {background-color: #234;}
	p {line-height: 1.5em;}
	.nobreak {white-space: nowrap;}
	.noselect {-webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none;}
	.bold {font-weight: bold;}
	.italic {font-style: italic;}
	.bulletpoints {line-height: 1.5em;}
	.row {box-sizing: border-box;}
	.row-media {display: block; float: left; width: 280px; height: 150px; background-position: center; background-size: contain; background-repeat: no-repeat; border-radius: 6px; border: 1px solid #def;}
	.row-text {display: block; float: left; margin-left: 16px; line-height: 1.5em; max-width: 552px;}
	.row-text span {line-height: inherit;}
	.clearfix {content: ""; clear: both; display: table;}
	.publication {margin-bottom: 32px; margin-left: 16px;}
	.press {width: 100px; height: 80px; border: 1px solid #def; margin-right: 12px; background-size: cover;}  
	.img-contain {background-size: contain !important;}
	.footer {background-color: #345; width: 100%}
	.footer-content {color: #fff; font-size: 10px; padding: 6px 0; max-width: 900px; margin: auto;}




	@media only screen and (max-width: 1150px) {
		.header-profile-picture, .header-text {display: block; margin: auto; text-align: center;}
		.header-profile-picture {margin-bottom: 12px; width: 140px; height: 140px;}
		body {font-size: 18px;}
		a.btn {font-size: 14px; padding: 2px 6px;}
	}
	
	@media only screen and (max-width: 900px) {
		.publication {margin-bottom: 46px;}
		.publication .row-media {width: 260px; height: 130px; margin: auto; margin-bottom: 12px; display: block;}
		.publication .row-text {display: block; width: 100%; margin-left: 0;}
		.press {display: block;}
	}
	</style>
</head>

<body>

<div id="layout-content" style="margin-top:25px">
<table cellpadding="11px">
	<tbody>
		<tr>
			<td width="720px">
				<div id="toptitle">
					<h1>Lu Dong (董璐) &nbsp; </h1>
				</div>
                <h3>Ph.D. Student</h3>       
				<p>
					<a href="https://engineering.buffalo.edu/computer-science-engineering.html">Department of Computer Science and Engineering</a><br>
					<a href="https://www.buffalo.edu/">University at Buffalo, SUNY (UB)</a><br>
					301 Davis Hall, Buffalo, New York, US, 14260 <br>
					<br>
					Email:  <a href="ludong@buffalo.edu"> ludong@buffalo.edu </a><br>
					<br>
          			<a href="./files/Lu_Dong_Resume251023.pdf"> <img src="./files/logo-cv.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://github.com/dongludeeplearning"> <img src="./files/logo-github.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
          			<a href="https://scholar.google.com/citations?user=48ReRMkAAAAJ&hl=en"> <img src="./files/logo-googlescholar.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://www.linkedin.com/in/lu-dong-71bbb0224/"> <img src="./files/logo-linkin.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://orcid.org/0009-0007-4036-7690"> <img src="./files/logo-orcid.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
                </p>
			</td>
			<td valign="middle">
				<img src="./files/LuDong.jpg" border="0" width="139"><br><br>
			</td>
		</tr>
	</tbody>
</table>









<h2>About Me</h2>
	<p style="text-align:justify;">I'm a final-year Ph.D. studentat the Department of Computer Science and Engineering (CSE), 
		<a href="https://www.buffalo.edu">University at Buffalo-The State University of New York (UB) </a>, 
		working with <a href="https://engineering.buffalo.edu/home/research/faculty/diverse-faculty.host.html/content/shared/engineering/computer-science-engineering/profiles/faculty/ladder/nwogu-ifeoma.html"> Prof. Ifeoma Nwogu </a> 
		at the<a href="https://ubhbml.github.io/index.html"> Human Behavior Modeling Lab</a>. 
		Prior to my Ph.D. studies, I obtained a Master's degree in Computer Science and Technology from the School of Computer Science and Technology 
		at <a href="http://en.xjtu.edu.cn/"> Xi'an Jiaotong University (XJTU) </a> in China, 
		where I was advised by  <a href="https://gr.xjtu.edu.cn/web/xyyang">Prof. Xinyu Yang </a> at the<a href="https://gr.xjtu.edu.cn/web/xyyang/11"> YLab </a>. 
	</p>
	<p style="text-align:justify;"> My research focuses on generative AI, 3D human modeling, and agentic intelligence—
		specifically advancing human-centered video generation and interactive 3D digital humans 
		endowed with domain expertise and empathetic social intelligence. 
		My work spans large language models (LLMs), 3D vision, generative models, and multimodal foundation models. 
		Key projects include sign-language generation, multi-person interaction, human–scene interaction, and human–robot collaboration. 
		Previously, I gained experience in reinforcement learning, recommendation systems, and data visualization. 
		I am now seeking full-time roles as a Research Scientist or Postdoctoral Researcher to continue advancing my research.
		<!-- 3D human and Agentic AI, aiming to equip 3D digital humans with with domain expertise and social intelligence.
		My work spans large language models (LLMs), 3D vision, generative models, and multimodal foundation models, 
		with projects on 3D sign language generation, multi-person motion synthesis, human-scene interaction with spatial reasoning, and human-robot
		collaboration under agentic AI frameworks. Previous project experience also include reinforcement learning, recommendation, and
		visualization. I am seeking full-time Research Scientist or Postdoctoral positions to continue advancing my research. -->
		<!-- My research focuses on 3D human modeling, equipping 3D lifelike
		avatars with domain-specific expertise and empathic social intelligence. Meanwhile, I develop methods for text-driven multi-person
		motion video generation, spatial-CoT reasoning for 3D human–scene interaction, and multi-agent frameworks for human–robot
		interaction. My work spans 3D computer vision, generative models, vision–language alignment, large language model fine-tuning,
		reinforcement learning, LLM multi-agent frameworks, and data science applications such as information retrieval, recommendation,
		and visualization. I am seeking full-time research scientist or postdoctoral opportunities to expand my work. -->
		<!-- My research focuses on computer vision (CV), large language models (LLMs), 3D human modeling (Embodied AI), 3D human scene/object interactions(HHI/HSI/HOI), Generative AI,
		AI-generated content (AIGC), and their applications in various aspects of human behavior, including sign language, education, and industry. 
		I am open to opportunities as an industry research scientist, faculty member, or postdoctoral researcher. -->
	</p>

<h2>News [<img src="./index_files//update.gif">]</h2>

<div style="overflow-y: scroll; height: 200px; max-width: 1050px; margin: 0 auto;">
  <table border="1" style="border-width: 0px; width: 100%;">
    <tbody>
      <tr>
        <td style="border-style: none; border-width: medium;">
          <ul>
			<li type="circle">2024.09: One paper on Agentic Frameworks for Human and Social Robot Interaction has been accepted by <strong> IROS 2025 </strong>. </li>
			<li type="circle">2025.05: Started my summer research intern journey at <strong>NEC Laboratories America, Inc. </strong> in Princeton, NJ. (on-site)</li>
			<li type="circle">2025.04: I successfully passed the Ph.D. dissertation proposal defense at CSE Department of UB.
			<li type="circle">2025.03: I was invited to give a talk on AI for Human Behavior at <strong> Women in Tech Western New York </strong>. </li>
            <li type="circle">2024.12: I’m honored to share that I have been awarded the <strong>Best AI Project Award </strong> at the 2024 CSE Poster Competition. </li>
            <li type="circle">2024.11: I will be presenting my work at the EMNLP 2024 Conference on November 12. See you in Miami! </li>
            <li type="circle">2024.10: After a competitive process, five outstanding candidates have advanced to the final phase of <strong>Research Acceleration </strong>. Congratulations to them and best wishes for their continued success! </li>
            <li type="circle">2024.10: We are launching an initiative called the <strong> Research Acceleration Program </strong>, aimed at helping early-stage AI researchers accelerate their growth, focusing on AIGC, Human Motion Generation, with applications in children's educational interactions, sign language synthesis, and social intelligence. UB master's and undergraduate students are encouraged to apply. If you're interested, please feel free to send me your resume. </li>
            <li type="circle">2024.10: Happy to be <strong> invited as a speaker </strong> for a panel discussion on 'AI Research and Career Development' at UB. </li>
            <li type="circle">2024.09: I’ll be presenting my work at ECCV 2024 on September 30 and October 4. See you in Milan, Italy!</li>
            <li type="circle">2024.09: One first-author paper on 3D sign language motion generation has been accepted by  <strong> EMNLP 2024 </strong>. </li>
            <li type="circle">2024.09: I’m honored to share that I have received the <strong> DEI Leadership Award </strong> at the IJCB 2024 Conference. </li>
            <li type="circle">2024.09: I’m excited to serve as the <strong> Local Student Chair </strong> for the IJCB 2024 Conference Organizing Team. </li>
            <li type="circle">2024.08: Two papers have been accepted by <strong> ECCV 2024</strong>: one on enhancing facial expression inference, and the other on open-domain multi-person motion generation. </li>
            <li type="circle">2024.08: I am honored to share that my work has been recognized by National AI Institute for Exceptional Education (AI4ExceptionalEd), the Institute for Artificial Intelligence and Data Science (IAD) and UB-CSE.
              <a href="https://www.linkedin.com/feed/update/urn:li:activity:7227312617697013760/">[Read] </a> <a href="https://www.linkedin.com/posts/ubcse_ubuffalo-signlanguagegeneration-biometrics-activity-7229188970151129088-lRk2/?utm_source=share&utm_medium=member_desktop">[More]</a>. 
            </li>
            <li type="circle">2024.05: I will present my work at FG 2024 Conference on May 27 and 30. See you in Istanbul, Turkey.</li>
            <li type="circle">2024.04: Two papers on sign language generation have been accepted by <strong>FG 2024</strong>.</li>
            <li type="circle">2024.03: I participated in the AI4EE: 
              <a href="https://www.linkedin.com/feed/update/urn:li:activity:7175489894306189312/">Year 1 Annual Review Site Visit </a>
              and presented our work in the event.</li>
            <li type="circle">2023.09: The National AI Institute for Exceptional Education 
              <a href="https://www.buffalo.edu/ai4exceptionaled.html">(AI4EE)</a> will support my research aimed at analyzing and modeling children's non-verbal behaviors, such as facial expressions and body gestures, to enhance educational performance.
            </li>
            <li type="circle">2023.08: I was invited to <strong>give a talk</strong> on Internships in the course CSE501, Fall2023, UB.</li>
            <li type="circle">2023.07: One paper on human motion synthesis has been accepted by <strong>ACM MM 2023</strong>.</li>
            <li type="circle">2023.06: I successfully passed my Oral Qualifying Exam (<strong>OQE</strong>) and became a PhD candidate at UB.</li>
            <li type="circle">2023.05: Started my research intern journey at <strong>InnoPeak Technology</strong> (OPPO US Research) in Seattle, WA. (on-site)</li>
            <li type="circle">2022.11: I was invited to be a <strong>judge</strong> of the 2022 <strong>UB Hacking Competition!</strong></li>
            <li type="circle">2022.10: I was invited to <strong>give a talk</strong> in the course CSE501, Fall2022, UB.</li>
            <li type="circle">2022.05: Started my research intern journey at <strong>InnoPeak Technology</strong> (OPPO US Research) in Palo Alto, CA. (on-site)</li>
            <li type="circle">2021.08: Started my Ph.D. journey following my advisor at <strong>UB</strong>, Buffalo, NY.</li>
          </ul>
          <br>
        </td>
      </tr>
    </tbody>
  </table>
</div>










<h2>Selected Research</h2>


<div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/autoMisty.png);"></div>
	<div class="row-text">
		<a href="https://wangxiaoshawn.github.io/AutoMisty.html" target="_blank"><strong>AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot</strong><br/></a>
		<strong>Lu Dong*</strong>, Xiao Wang*,  Sahana Rangasrinivasan, Ifeoma Nwogu, Srirangaraj Setlur, Venu Govindaraju. (*Equal contribution)<br/> 
		International Conference on Intelligent Robots and Systems <strong> [IROS2025]</strong> <br/> 
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href="https://arxiv.org/pdf/2503.06791">PDF</a>
		<a class="btn" href=" ">BibTeX</a>
    <a class="btn" href="https://wangxiaoshawn.github.io/AutoMisty.html" target="_blank"> Project Webpage </a>
	</div>
  </div>



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/EMNLP.gif);"></div>
	<div class="row-text">
		<a href="wSignGen.html" target="_blank"><strong>Word-Conditioned 3D American Sign Language Motion Generation</strong><br/></a>
    <strong>Lu Dong</strong>, Xiao Wang, Ifeoma Nwogu. <br/> 
    In Findings of the Association for Computational Linguistics <strong> [EMNLP 2024]</strong> <br/> 
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href="https://aclanthology.org/2024.findings-emnlp.584/">PDF</a>
		<a class="btn" href="https://aclanthology.org/2024.findings-emnlp.584/">BibTeX</a>
    <a class="btn" href="wSignGen.html" target="_blank"> Project Webpage </a>
	</div>
  </div>







  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/ECCVW.gif);"></div>
	<div class="row-text">
		<a href="Ig3D.html" target="_blank"><strong>Ig3D: Integrating 3D Face Representations in Facial Expression Inference</strong><br/></a>
    <strong>Lu Dong*</strong>, Xiao Wang*, Srirangaraj Setlur, Venu Govindaraju, Ifeoma Nwogu. <br/> 
    The 18th European Conference on Computer Vision <strong>[ECCV 2024]</strong> Workshop <br/> 
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href=" ">PDF</a>
		<a class="btn" href=" ">BibTeX</a>
    <a class="btn" href="Ig3D.html" target="_blank"> Project Webpage </a>
	</div>
  </div>


  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/SignAvatar_Quick.gif);"></div>
	<div class="row-text">
		<a href="SignAvatar.html" target="_blank"><strong>SignAvatar: Sign Language 3D Motion Reconstruction and Generation</strong><br/></a>
    <strong>Lu Dong</strong>, Lipisha Chaudhary, Fei Xu, Xiao Wang, Mason Lary, Ifeoma Nwogu. <br/>
		<span class="italic"></i>The 18th IEEE International Conference on Automatic Face and Gesture Recognition <strong> [FG 2024]</strong>. </span><br/>
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href="">PDF</a>
		<a class="btn" href=" ">BibTeX</a>
		<a class="btn" href="SignAvatar.html" target="_blank"> Project Webpage </a>
	</div>
  </div>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/MMPG.gif);"></div>
	<div class="row-text">
		<a href="https://shanmy.github.io/Multi-Motion/" target="_blank"><strong>Towards Open Domain Text-Driven Synthesis of Multi-Person Motions</strong><br/></a>
    Mengyi Shan, <strong>Lu Dong</strong>, Yutao Han, Yuan Yao, Tao Liu, Ifeoma Nwogu, Guo-Jun Qi, Mitch Hill. <br/>
    The 18th European Conference on Computer Vision <strong> [ECCV 2024] </strong> <br/>
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href="https://shanmy.github.io/Multi-Motion/static/pdfs/Multi_Person_Human_Motion_Diffusion_Model.pdf">PDF</a>
		<a class="btn" href=" ">BibTeX</a>
    <a class="btn" href="https://shanmy.github.io/Multi-Motion/" target="_blank"> Project Webpage </a>
	</div>
  </div>










  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/FG_fei.gif);"></div>
	<div class="row-text">
		<a href="SignAvatar.html"><strong>A Comparative Study of Video-based Human Representations for American Sign Language Alphabet Generation </strong><br/></a>
    FeiXu, Lipisha Chaudhary, <strong>Lu Dong</strong>, Srirangaraj Setlur, Venu Govindaraju, Ifeoma Nwogu. <br/>
		<span class="italic"></i>The 18th IEEE International Conference on Automatic Face and Gesture Recognition <strong>[FG 2024]</strong> Workshop. </span><br/>
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href="">PDF</a>
		<a class="btn" href=" ">BibTeX</a>
	</div>
  </div>



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/ATOM.gif);"></div>
	<div class="row-text">
		<a href="https://www.yhzhai.com/publication/mm-2023-atom/"><strong>Language-guided Human Motion Synthesis with Atomic Actions</strong><br/></a>
    Yuanhao Zhai, Mingzhen Huang, Tianyu Luan,  <strong>Lu Dong </strong>, Ifeoma Nwogu, Siwei Lyu, David Doermann, Junsong Yuan <br/>
		<span class="italic"></i>ACM Multimedia <strong>[MM 2023]</strong>. </span><br/>
    <span class="italic"></i>Please check title link for more details.</span><br/>
		<a class="btn btn-orange" href=" ">DOI</a> 
		<a class="btn btn-red" href="https://arxiv.org/abs/2308.09611">PDF</a>
		<a class="btn" href="bibs/xintianyou.txt">BibTeX</a>
    <a class="btn" href="https://www.yhzhai.com/publication/mm-2023-atom/"> Project Webpage </a>
	</div>
  </div>




  <h3>Previous</h3>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Music.gif);"></div>
	<div class="row-text">
		<a href="Music.html"><strong>Exploring the General Melodic Characteristics of XinTianYou Folk Songs</strong><br/></a>
		Juan Li, <strong>Lu Dong</strong>, Jianhang Ding, Xinyu Yang<br/>
		<span class="italic"></i>12th Sound and Music Computing Conference(<strong>SMC</strong>) </span><br/>
		<a class="btn btn-orange" href="https://zenodo.org/record/851035#.Y2nrz-yZPeo">DOI</a> 
		<a class="btn btn-red" href="https://www.semanticscholar.org/paper/Exploring-the-General-Melodic-Characteristics-of-Li-Dong/2ccbf8ede81c2f41320cd046d045898cd9325b84">PDF</a>
		<a class="btn" href="bibs/ATOM.txt">BibTeX</a>
    <a class="btn" href="Music.html">Project Webpage</a>
	</div>
  </div>




<h2>Selected Project</h2>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Human_Pose.gif);">   </div>
	<div class="row-text" align="justify">
	 <a href="Human_Pose.html">	<strong> A smooth 3D Human Pose Estimation pipeline for video in the wild </strong> <br/></a>
The majority of current models have been trained on limited close set data, which often results in subpar performance when applied to real-world video data. One of the major challenges in this context is the issue of self-occlusion, which refers to instances where parts of a subject's body obstruct or cover other parts. This results in a less smooth and precise performance. This project aims to tackle this issue.
<br/>
    <span class="italic"></i>Please check title link for more details.</span><br/>
	</div>
  </div>




  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Training_chatbot.gif);"></div>
	<div class="row-text" align="justify">
	 <a href="chatbot.html">	<strong> A Training ChatBot to facilitate medical knowledge dissemination in underdeveloped regions</strong> <br/></a>
This project is undertaken in providing for support for a non-profit organization with the aim of addressing the lack of medical knowledge in underdeveloped regions of India. This shortfall leads to an alarming number of preventable fatalities each year. We have developed a training chatbot to enhance the understanding of medical knowledge and local resources by volunteers and the local population. The chatbot will serve as an educational tool to improve medical literacy and ultimately, save lives.<br/>
    <span class="italic"></i>Please check title link for more details.</span><br/>
	</div>
  </div>	



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/covid_search_engine.gif);"></div>
	<div class="row-text" align="justify">
	 <a href="covid_search.html">	<strong> A Covid Analysis Search Engine: Dissecting Twitter data to analyze government & public attitude towards Covid and Vaccines</strong> <br/></a>
Do authoritative figures have any influence on the trend of COVID-19, what is the attitude of the general public toward COVID-19 and vaccines, whether changes in public attitudes are influenced by authoritative speech? The aim of this project is to design and develop a full-stack search engine that integrates advanced functionalities for retrieval, translation, sentiment analysis, attitude tracking, topic & area statistics, and disinformation detection from multiple dimensions. <br/>
    <span class="italic"></i>Please check title link for more details.</span><br/>
	</div>
  </div>		






  <h2>Experience</h2>

  <!-- Research Assistant -->
  <h3>Research Assistant</h3>
  <ul>
	<li>
	  <strong>Research Assistant</strong>, National AI Institute for Exceptional Education, University at Buffalo, SUNY (UB), Sep 2023 – Present
	</li>
	<li>
	  <strong>Research Assistant</strong>, Human Behavior Modeling Lab, University at Buffalo, SUNY (UB), Aug 2021 – Present
	</li>
	<li>
	  <strong>Research Assistant</strong>, Rochester Institute of Technology (RIT), Aug 2020 – May 2021
	</li>
	<li>
	  <strong>Research Assistant</strong>, Xi’an Jiaotong University (XJTU), Aug 2013 – May 2016
	</li>
  </ul>
  
  <!-- Internship -->
  <h3>Research Internship</h3>
  <ul>
	<li>
	  <strong>Research Intern</strong>, NEC Laboratories America, Inc., On-Site, May 2025 – Present
	</li>
	<li>
	  <strong>Research Intern</strong>, InnoPeak Technology, Seattle, WA, On-Site, June 2023 – Aug 2023
	</li>
	<li>
	  <strong>Research Intern</strong>, InnoPeak Technology, Palo Alto, CA, On-Site, May 2022 – Aug 2022
	</li>
  </ul>
  
  <!-- Full-Time
  <h3>Full-Time</h3>
  <ul>
	<li>
	  <strong>Senior Data Analyst</strong>, Shaanxi Haina Electronic Technology Co., Ltd., Sep 2016 – Apr 2020
	</li>
  </ul> -->
<!-- 
<h2>Experience</h2>
  <ul>
	<li type="disc">
		<strong>Research Internship</strong>  NEC Laboratories America, Inc.,On-Site, May 2025- Present.
		</li>
   <li type="disc">
    <strong>Research Internship </strong> InnoPeak Technology, Seattle, WA, On-Site, June 2023- Aug 2023.
		</li> 
	<li type="disc">
    <strong>Research Internship </strong> InnoPeak Technology, Palo Alto, CA, On-Site, May 2022- Aug 2022.
		</li>  

	<li type="disc">
		<strong>Research Assitant</strong> National AI Institute for Exceptional Education, University at Buaffalo SUNY (UB), Sep 2023- Now.
			</li>
  <li type="disc">
    <strong>Research Assitant</strong> Human Behavior Modeling Lab, University at Buaffalo SUNY (UB), Aug 2021- Now.
		</li>
  <li type="disc">
    <strong>Research Assitant</strong> Rochester Institute of Technology(RIT), Aug 2020- May 2021.
  	</li>   
  <li type="disc">
    <strong>Senior Data Analyst</strong> Shaanxi Haina Electronic Technology Co.,LT, Sep 2016- Apr 2020.
		</li>    
  <li type="disc">
    <strong>Research Assitant</strong> XI'An Jiaotong University (XJTU), Aug 2013- May 2016.
		</li>    
  </ul>	 -->






<h2>Selected Awards & Honors</h2>

  	<ul>
  		<li type="disc"> <strong> Best AI Project </strong>, UB 2024. </li>
  		<li type="disc"> <strong> IJCB Conference Leadership Award </strong>, 2024. </li>
  	  <li type="disc"> <strong>ECCV Conference Travel Grant </strong>, 2024. </li>
  	  <li type="disc"> <strong>National Graduate Academic Scholarship </strong>, 2013-2016. </li>
  	  <li type="disc"> <strong>National Endeavor Undergraduate Scholarship for Outstanding Students </strong>, 2010-2011</li>
	 <li type="disc"> <strong>Excellent Graduate Student Honor</strong>, 2014-2016 ; <strong>Excellent Undergraduate Student Honor</strong>, 2010-2011.</li>
  	  <li type="disc"> <strong>Silver Medal in Women’s 100m Hurdles,</strong> Shaanxi Provincial University Games, 2014. </li>
  	  <li type="disc"> <strong>Champion Team in Women’s Basketball (Captain) </strong>First-time achievement for the CS Department, 2010. </li>
  	  <br>
  	</ul>


<h2>Academic Services</h2>
	<h3>Conference  Organization & Reviewer</h3>
	<ul>
		<li type="disc">2024.09  Served as the Local Student Chair for IJCB 2024 Conference @ Buffalo, NY. </li>
		<li type="disc">ACL Rolling Review (ARR), February & July 2025; (Association for Computational Linguistics) </a></li>
		<li type="disc">ICCV 2025 (International Conference on Computer Vision) </a></li>
		<li type="disc">IEEE CAI 2025 (IEEE Conference on Artificial Intelligence) </a></li>
		<li type="disc">ACM MM 2022, 2023, 2024 (ACM International Conference on Multimedia)</a></li>
	</ul>
	<h3>Journal  Reviewer</h3>
	<ul>
		<li>Computer Vision and Image Understanding (CVIU), 2025</li>
		<li>Springer Nature: Machine Vision and Applications (SN MVA), 2024</li>
		<li>IEEE Transactions on Affective Computing (IEEE TAFFC), 2024</li>
	</ul>



  <h3>Invited Talks & Competition Judge/Reviewer</h3>
  	<ul>
		<li type="disc">2025.03: I was invited to give a talk on AI for Human Behavior at  Women in Tech Western New York.</li>
  		<li type="disc">2024.10: I was invited as a speaker for a panel discussion on 'AI Research and Career Development' at UB.</li>
  	  <li type="disc">2023.8: I was invited to give a talk on Internships in the course CSE501, Fall2023, UB.</li>
		<li type="disc">2022.11.06 Invited as a Judge for <a href="https://www.ubhacking.com/"> 2022 UB Hacking Competition. </a></li>
  	  <li type="disc">2022.10: Delivering a talk, introducing Human Pose Estimation and Ph.D Life Guidance in the course CSE-501, Fall2022.</li>
  	</ul>


 <h3>Membership</h3>

  	<ul>
  		<li type="disc">ACL Member; IEEE Student Member; IEEE Biometrics Council Member </a></li>
  	</ul>



<h2>Selected Photos</h2>

  	
  	<div style="width:100%; max-width:1024px; overflow-x:auto; border:1px solid #ddd; padding:10px; box-sizing:border-box;">
  	    <div style="display:flex; gap:10px;">
			<img src="./files/womenTech.jpg" alt="womenTech" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	    	<img src="./files/emnlp02.jpg" alt="emnlp02" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	    	<img src="./files/emnlp01.jpg" alt="emnlp01" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	        <img src="./files/ECCV01.jpeg" alt="ECCV01" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	        <img src="./files/FG01.jpeg" alt="FG01" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	        <img src="./files/FG02.jpeg" alt="FG02" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	        <img src="./files/FG03.jpeg" alt="FG03" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	        <img src="./files/99.jpeg" alt="99" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	        <img src="./files/66.png" alt="66" style="height:auto; max-height:200px; width:auto; max-width:100%;"/>
  	    </div>
  	</div>


 





<hr>
<table cellpadding="0px">
	<tbody>
		<tr>
			<td width="720px">
			<div id="clustrmaps-widget"></div>
				<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=7seMHMuQZlvELLsuK6wd30IX5cz7U58mGsA8iV8s6_o"></script></script>
				</script>
			<div class="jvectormap-tip"></div>
			</td>
			<td valign="top">
				<div style="clear:both;">
				<p align="right"><font size="2">Last Updated on December, 2024<br>
				</div>
				<div style="clear: both;">
				<p align="right"><font size="5"><img src="./index_files//happy.gif"></font></p>
				</div>
			</td>
		</tr>
	</tbody>
</table>







</body>
</html>