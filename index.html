​	<html>

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">	
	<title>Lu Dong</title>
	<meta content="LuDong, Lu Dong UB, lu dong ub, ludong,  https://dongludeeplearning.github.io/" name="keywords">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<script async="" src="http://www.google-analytics.com/analytics.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-66888300-1', 'auto');
		ga('send', 'pageview');
	</script>
	<script type="text/javascript" src="./index_files/jquery-1.12.4.min.js"></script>
	<style>
	a {color: #2471a3; text-decoration: none;}
	a:hover {color: #5499c7;}
	a.btn {background-color: #2471a3; color: #fff; font-size: 12px; margin-top: 4px; display: inline-block; padding: 0 4px; border-radius: 2px; line-height: 1.5em;}
	a.btn:hover {background-color: #174e74;}
	a.btn.btn-red {background-color: #d63a3a;}
	a.btn.btn-red:hover {background-color: #a02727;}
	a.btn.btn-orange {background-color: #e2700c;}
	a.btn.btn-orange:hover {background-color: #c46009;}
	a.btn.btn-dark {background-color: #345;}
	a.btn.btn-dark:hover {background-color: #234;}
	p {line-height: 1.5em;}
	.nobreak {white-space: nowrap;}
	.noselect {-webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none;}
	.bold {font-weight: bold;}
	.italic {font-style: italic;}
	.bulletpoints {line-height: 1.5em;}
	.row {box-sizing: border-box;}
	.row-media {display: block; float: left; width: 280px; height: 150px; background-position: center; background-size: contain; background-repeat: no-repeat; border-radius: 6px; border: 1px solid #def;}
	.row-text {display: block; float: left; margin-left: 16px; line-height: 1.5em; max-width: 552px;}
	.row-text span {line-height: inherit;}
	.clearfix {content: ""; clear: both; display: table;}
	.publication {margin-bottom: 32px; margin-left: 16px;}
	.press {width: 100px; height: 80px; border: 1px solid #def; margin-right: 12px; background-size: cover;}  
	.img-contain {background-size: contain !important;}
	.footer {background-color: #345; width: 100%}
	.footer-content {color: #fff; font-size: 10px; padding: 6px 0; max-width: 900px; margin: auto;}




	@media only screen and (max-width: 1150px) {
		.header-profile-picture, .header-text {display: block; margin: auto; text-align: center;}
		.header-profile-picture {margin-bottom: 12px; width: 140px; height: 140px;}
		body {font-size: 18px;}
		a.btn {font-size: 14px; padding: 2px 6px;}
	}
	
	@media only screen and (max-width: 900px) {
		.publication {margin-bottom: 46px;}
		.publication .row-media {width: 260px; height: 130px; margin: auto; margin-bottom: 12px; display: block;}
		.publication .row-text {display: block; width: 100%; margin-left: 0;}
		.press {display: block;}
	}
	</style>
</head>

<body>

<div id="layout-content" style="margin-top:25px">
<table cellpadding="11px">
	<tbody>
		<tr>
			<td width="720px">
				<div id="toptitle">
					<h1>Lu Dong (董璐) &nbsp; </h1>
				</div>
                <h3>Ph.D. Student</h3>       
				<p>
					<a href="https://engineering.buffalo.edu/computer-science-engineering.html">Department of Computer Science and Engineering</a><br>
					<a href="https://www.buffalo.edu/">University at Buffalo, SUNY (UB)</a><br>
					Davis Hall, Buffalo, New York, U.S.A<br>
					<br>
					Email:  <a href="dongludeeplearning@gmail.com">dongludeeplearning@gmail.com</a><br>
					<br>
					<a href="./files/ML_LuDONG1127.pdf"> <img src="./files/logo-cv.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://github.com/dongludeeplearning"> <img src="./files/logo-github.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://www.linkedin.com/in/lu-dong-71bbb0224/"> <img src="./files/logo-linkin.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
                </p>
			</td>
			<td valign="middle">
				<img src="./files/LuDong.jpg" border="0" width="139"><br><br>
			</td>
		</tr>
	</tbody>
</table>



<h2>About Me</h2>
	<p style="text-align:justify;">I am a Ph.D. student (2021-Present) at the Department of Computer Science and Engineering (CSE), <a href="https://www.buffalo.edu">University at Buffalo-The State University of New York (UB) </a>, under the supervision of  <a href="https://engineering.buffalo.edu/computer-science-engineering/people/faculty-directory.host.html/content/shared/engineering/computer-science-engineering/profiles/faculty/nwogu-ifeoma.html"> Prof. Ifeoma Nwogu </a> at the Human Behavior Modeling Lab. Prior to my Ph.D. studies, I obtained a Master's degree in Computer Science and Technology from the School of Computer Science and Technology at <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University (XJTU) </a> in China, where I was advised by  <a href="https://gr.xjtu.edu.cn/web/xyyang">Prof. Xinyu Yang </a> at the<a href="https://gr.xjtu.edu.cn/web/xyyang/11"> YLab </a>. Additionally, I hold two Bachelor's degrees from <a href="https://en.neepu.edu.cn/">  Northeast Electric Power University (NEEPU)</a> in China, one in Computer Science and Technology and another in Electrical Engineering and Automation.
	</p>
	<p style="text-align:justify;">My research interests are Human Pose Estimation, 3D Mesh Reconstruction, Robotics Physics Simulation,  Multimodal, Language-driven Motion, Sign Language Translation & Generation, and Video Understanding. My long-term goal is primarily in developing Computer Vision, Natural Language Processing, Reinforcement Learning, Statistic Machine Learning, and Mathematical Modeling to study multimodal human behavior and make generative models more effective in serving human needs. Besides, some of my early works are related to Speech Sentiment Analysis, Music Analysis with Machine Learning and Data Science techniques including Data Crawling, Pattern Mining, Information Retrieval, and Search Engine Optimization. Feel free to reach out to me if you are interested in collaborating. I am also actively looking for research intern positions.
	</p>


<h2>News [<img src="./index_files//update.gif">]</h2>
	<div  style="overflow-y: scroll; height:200px;">
	<table border="1" style="border-width: 0px;" width="1050">
	<tbody>
	<tr>
	<td style="border-style: none; border-width: medium;">
	<ul>
		<li type="circle">2022.11: I was invited to be the <strong>judge</strong> of the 2022  <strong>UB Hacking Competition! </strong> </li>
		<li type="circle">2022.10: I was invited to <strong> give a talk </strong> in the course CSE501, Fall2022, UB.</li>
		<li type="circle">2022.05: Start my internship at <strong> OPPO U.S. Research Center</strong> at Palo Alto, CA. (on-site)</li>
		<li type="circle">2021.08: Start my new Ph.D journey at <strong>UB</strong>, Buffalo, NY.</li>
		<li type="circle">2021.05: I passed the Ph.D Research Potential Assessment at RIT!</li>
		<li type="circle">2020.08: Start my new Ph.D journey at <strong>RIT</strong>, Rochester, NY.</li>
	</ul><br>
	</td>
	</tr>
	</tbody>
	</table>
	</div>

<h2>Selected Publications</h2>
  <h3>Under Review</h3>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/AtomVAE_Demo.gif);"></div>
	<div class="row-text" align="justify">
		<strong>AtomVAE: Towards Zero-Shot Text-to-Motion Synthesis (CVPR 2023)</strong> <br/>
Human motion synthesis is an important task due to its wide applications in computer graphics, human behavior understanding, human-robotic interactions, etc. Existing methods focus on closed-set human action synthesis, limiting their ability to generate actions of novel classes that are not seen during training. To address this problem, we propose AtomVAE for zero-shot text-to-motion synthesis, which aims to generate novel categories of actions that do not belong to the training set. Specifically, the proposed AtomVAE learns a mapping from the input text space that describes the action to the output motion space that represent the action, using limited number of training labels. To facilitate zero-shot text-to-motion synthesis, we propose to decompose seen actions into a set of atomic actions, such that by leveraging the similarity between input text and the learned atomic actions, our model can learn to compose novel actions using the atomic actions. The atomic actions are learned in an end-to-end manner, with diversity and sparsity constrains enforcing the atomicity and robustness. Our method not only achieves promising results on zero-shot synthesis, but also outperforms state-of-the-art approaches by a large margin in the traditional closed-set synthesis task.<br/>
	</div>
  </div>  



<h3>Ongoing</h3>



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/SignClip.png);"></div>
	<div class="row-text" align="justify">
		<strong>SignCLIP: A New Framework for Sign Language 3D Mesh Generation using CLIP (ICCV 2023)</strong> <br/>
Similar to any language, understanding and communicating sign language can be quite challenging. To make sign language spread over multiple ways like other languages, we proposed a framework to generate 3D mesh sign videos from the image or text. Specifically, we applied SMPL-X model to get the expressive pose including the upper body, face, and hands. Then the Transformer-based model has been applied to analyze and reconstruct the pose sequences. Moreover, we introduced the CLIP model to improve performance when matching the image, text and sign sequence. Our work builds a new baseline for the sign language generation community. Experiments showcase that our model not only achieves promising results on zero-shot generation but also proved can be better understood by the community.<br/>
	</div>
  </div>



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Agent4Sign2.png);"></div>
	<div class="row-text" align="justify">
		<strong>Agent4Sign:  Human Whole-Body 3D Pose Agent Estimation for Sign Language </strong> <br/>
In the sign language community, the traditional method of depicting the skeleton pose in vision is not effective enough for sign language personnel to comprehend. The use of 3D mesh enhances the visual aspect, however, it disregards the essential physical structure of the human body. To tackle these challenges, we propose the Agent4Sign framework. This framework incorporates the MUJOCO physics engine to improve the accuracy and realism of the representation. The proposed model leverages the mutual constraints between two methods to implement a self-supervised training process from videos with reinforcement learning, eliminating the reliance on 3D Ground Truth. Experiments showcase our approach effectively  improves sample efficiency and offers a more naturalistic and realistic representation.  Additionally, the proposed model also provides a path towards manipulating real robots, offering promising potential for real-world applications and advancements in the field.<br/>
	</div>
  </div>



  <h3>Previous</h3>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Music.gif);"></div>
	<div class="row-text">
		<a href="Music.html"><strong>Exploring the General Melodic Characteristics of XinTianYou Folk Songs</strong><br/></a>
		Juan Li, <strong>Lu Dong</strong>, Jianhang Ding, Xinyu Yang<br/>
		<span class="italic"></i>12th Sound and Music Computing Conference(<strong>SMC</strong>) </span><br/>
		<a class="btn btn-orange" href="https://zenodo.org/record/851035#.Y2nrz-yZPeo">DOI</a> 
		<a class="btn btn-red" href="https://github.com/dongludeeplearning/MIR/blob/master/Exploring%20the%20General%20Melodic%20Characteristics%20of%20XinTianYou%20Folk%20Songs.pdf">PDF</a>
		<a class="btn" href="bibs/xintianyou.txt">BibTeX</a>
	</div>
  </div>



<h2>Selected Project Demo</h2>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Human_Pose.gif);">   </div>
	<div class="row-text" align="justify">
	 <a href="Human_Pose.html">	<strong> A smooth 3D Human Pose Estimation pipeline for video in the wild</strong> <br/></a>
The majority of current models have been trained on limited close set data, which often results in subpar performance when applied to real-world video data. One of the major challenges in this context is the issue of self-occlusion, which refers to instances where parts of a subject's body obstruct or cover other parts. This results in a less smooth and precise performance. To tackle this issue, my approach involved a transformer-based pipeline that incorporates both self communication and cross communication mechanisms. Applying this technique in consecutive frames has resulted in a marked improvement in smooth and has been verified through extensive experimentation. The results of these experiments demonstrate that the proposed pipeline outperforms state-of-the-art performance on the Human3.6 dataset, as well as exhibiting the best performance on our in-house self-occlusion dataset.
<br/>
	</div>
  </div>



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/Training_chatbot.gif);"></div>
	<div class="row-text" align="justify">
	 <a href="chatbot.html">	<strong> A Training ChatBot to facilitate medical knowledge dissemination in underdeveloped regions</strong> <br/></a>
This project is undertaken in providing for support for a non-profit organization with the aim of addressing the lack of medical knowledge in underdeveloped regions of India. This shortfall leads to an alarming number of preventable fatalities each year. The non-profit organization has furnished us with relevant document data and we have developed a training chatbot to enhance the understanding of medical knowledge and local resources by volunteers and the local population. The chatbot will serve as an educational tool to improve medical literacy and ultimately, save lives.<br/>
	</div>
  </div>	



  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/covid_search_engine.gif);"></div>
	<div class="row-text" align="justify">
	 <a href="covid_search.html">	<strong> A Covid Analysis Search Engine: Dissecting Twitter data to analyze government & public attitude towards Covid and Vaccines</strong> <br/></a>
This search engine aims to explore the impact of authoritative figures on the trend of COVID-19 and the public's attitude towards COVID-19 and vaccines. To gather information, we utilized the Tweepy API to collect 50,000 tweet data from various countries and languages. We further use  some information retrieval technology to build up a database that supports ranking. Meanwhile, the backend implements data analysis from multiple dimensions and sentiment analysis using advanced NLP technology.  The user interface mimics the design of Google's UI, utilizing technologies such as HTML, CSS, Bootstrap, Javascript, and Ajax. Finally, our Flask server deployed on AWS EC2 cloud.<br/>
	</div>
  </div>		



<h2>Experience</h2>
  <ul>
	<li type="disc">
    <strong>Research Internship </strong> OPPO US Research Institute, Palo Alto, CA, On-Site, May 2022- Aug 2022.
		</li>  
  <li type="disc">
    <strong>Research Assitant</strong> University at Buaffalo SUNY (UB), Aug 2021- Now.
		</li>
  <li type="disc">
    <strong>Research Assitant</strong> Rochester Institute of Technology(RIT), Aug 2020- May 2021.
  	</li>   
  <li type="disc">
    <strong>Senior Data Analyst</strong> Shaanxi Haina Electronic Technology Co.,LT, Sep 2016- Apr 2020.
		</li>    
  <li type="disc">
    <strong>Research Assitant</strong> XI'An Jiaotong University (XJTU), Aug 2013- May 2016.
		</li>    
  </ul>	





<h2>Selected Awards & Honors</h2>
  <h3>Awards</h3>
  	<ul>
      <li type="disc"> <strong>Outstanding Leadership Award</strong>, Shaanxi Haina Electronic Technology Co.,LT. 2018 </li>
      <li type="disc"> <strong>National Graduate Academic Scholarship </strong>, Xi'an Jiaotong University (XJTU), 2013-2016. </li>
      <li type="disc"> <strong>Silver Metal</strong>, Universiade Women's Hurdle，Xi'an Jiaotong University (XJTU), 2014. </li>
      <li type="disc"> <strong>National Encouragement Scholarship</strong>, Northeast Electric Power University (NEEPU), 2010.(top 5%) </li>
      <li type="disc"> <strong>The Academic Scholarship </strong>, Northeast Electric Power University (NEEPU), 2010. (top 10%) </li>
      <li type="disc"> <strong>Champion & MVP </strong>, College Women's Basketball of NEEPU, 2010. </li>
      <br>
</ul>      
  <h3>Honors</h3>
  	<ul>
      <li type="disc"> <strong>Excellent Postgraduate Student </strong>, XI'an Jiaotong University (XJTU), 2014 & 2015. (top 10%)</li>
      <li type="disc"> <strong>Excellent Student Cadre </strong>, Northeast Electric Power University (NEEPU), 2010. (top 3 /120)</li> 
      <li type="disc"> <strong>Excellent Undergraduate Student </strong>,Northeast Electric Power University (NEEPU).(top 10%)</li> 
</ul>





<h2>Academic Services</h2>
  <h3>Competition</h3>

  	<ul>
  		<li type="disc">2022.11.06 I was invited as a <strong> Judge </strong> for <a href="https://www.ubhacking.com/"> 2022 UB Hacking Competition. </a></li>
  	</ul>



  <h3>Talks</h3>
  	<ul>
  		<li type="disc">2022.10.18: Delivering a talk, introducing Human Pose Estimation and Ph.D Life Guidance in the course CSE-501, Fall2022.</li>
  	</ul>





<h2>Selected Photos</h2>

  	<ul>
  		<div style="width:960px;overflow-x:scroll">
  		<div style="width:2280px">
        <img src="./files/66.png"  border="0" width="390"/>
        <img src="./files/33.jpeg"  border="0" width="390"/>
        <img src="./files/intern.jpeg"  border="0" width="360"/>
 		 	 	<img src="./files/11.png" border="0" width="380"/>
        <img src="./files/77.png" border="0" width="290"/>
 		  </div>
			</div>
  	</ul>


<hr>
<table cellpadding="0px">
	<tbody>
		<tr>
			<td width="720px">
			<div id="clustrmaps-widget"></div>
				<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=7seMHMuQZlvELLsuK6wd30IX5cz7U58mGsA8iV8s6_o"></script></script>
				</script>
			<div class="jvectormap-tip"></div>
			</td>
			<td valign="top">
				<div style="clear:both;">
				<p align="right"><font size="2">Last Updated on Feb, 2023<br>
				Published with <a href="https://pages.github.com/">GitHub Pages</a></font></p>
				</div>
				<div style="clear: both;">
				<p align="right"><font size="5"><img src="./index_files//raccoon.gif"></font></p>
				</div>
			</td>
		</tr>
	</tbody>
</table>



</body>
</html>