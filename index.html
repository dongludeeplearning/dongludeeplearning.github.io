<html>

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">	
	<title>Lu Dong</title>
	<meta content="LuDong, Lu Dong UB, lu dong ub, ludong,  https://dongludeeplearning.github.io/" name="keywords">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<script async="" src="http://www.google-analytics.com/analytics.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-66888300-1', 'auto');
		ga('send', 'pageview');
	</script>
	<script type="text/javascript" src="./index_files/jquery-1.12.4.min.js"></script>
	<style>
	a {color: #2471a3; text-decoration: none;}
	a:hover {color: #5499c7;}
	a.btn {background-color: #2471a3; color: #fff; font-size: 12px; margin-top: 4px; display: inline-block; padding: 0 4px; border-radius: 2px; line-height: 1.5em;}
	a.btn:hover {background-color: #174e74;}
	a.btn.btn-red {background-color: #d63a3a;}
	a.btn.btn-red:hover {background-color: #a02727;}
	a.btn.btn-orange {background-color: #e2700c;}
	a.btn.btn-orange:hover {background-color: #c46009;}
	a.btn.btn-dark {background-color: #345;}
	a.btn.btn-dark:hover {background-color: #234;}
	p {line-height: 1.5em;}
	.nobreak {white-space: nowrap;}
	.noselect {-webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none;}
	.bold {font-weight: bold;}
	.italic {font-style: italic;}
	.bulletpoints {line-height: 1.5em;}
	.row {box-sizing: border-box;}
	.row-media {display: block; float: left; width: 180px; height: 90px; background-position: center; background-size: contain; background-repeat: no-repeat; border-radius: 6px; border: 1px solid #def;}
	.row-text {display: block; float: left; margin-left: 16px; line-height: 1.5em; max-width: 662px;}
	.row-text span {line-height: inherit;}
	.clearfix {content: ""; clear: both; display: table;}
	.publication {margin-bottom: 32px; margin-left: 16px;}
	.press {width: 100px; height: 80px; border: 1px solid #def; margin-right: 12px; background-size: cover;}  
	.img-contain {background-size: contain !important;}
	.footer {background-color: #345; width: 100%}
	.footer-content {color: #fff; font-size: 10px; padding: 6px 0; max-width: 900px; margin: auto;}


	@media only screen and (max-width: 1150px) {
		.header-profile-picture, .header-text {display: block; margin: auto; text-align: center;}
		.header-profile-picture {margin-bottom: 12px; width: 140px; height: 140px;}
		body {font-size: 18px;}
		a.btn {font-size: 14px; padding: 2px 6px;}
	}
	
	@media only screen and (max-width: 900px) {
		.publication {margin-bottom: 46px;}
		.publication .row-media {width: 260px; height: 130px; margin: auto; margin-bottom: 12px; display: block;}
		.publication .row-text {display: block; width: 100%; margin-left: 0;}
		.press {display: block;}
	}
	</style>
</head>


<body>
<div id="layout-content" style="margin-top:25px">
<table cellpadding="11px">
	<tbody>
		<tr>
			<td width="720px">
				<div id="toptitle">
					<h1>Lu Dong (董璐) &nbsp; </h1>
				</div>
                <h3>Ph.D. Student</h3>       
				<p>
					<a href="https://engineering.buffalo.edu/computer-science-engineering.html">Department of Computer Science and Engineering</a><br>
					<a href="https://www.buffalo.edu/">University at Buffalo, SUNY (UB)</a><br>
					Davis Hall, Buffalo, New York, U.S.A<br>
					<br>
					Email:  <a href="dongludeeplearning@gmail.com">dongludeeplearning@gmail.com</a><br>
					<br>
					<a href="./files/ML_LuDONG1127.pdf"> <img src="./files/logo-cv.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://github.com/dongludeeplearning"> <img src="./files/logo-github.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
					<a href="https://www.linkedin.com/in/lu-dong-71bbb0224/"> <img src="./files/logo-linkin.png" height="30px" style="margin-bottom:-3px; margin-right: 10px"> </a>
                </p>
			</td>
			<td valign="middle">
				<img src="./files/LuDong.jpg" border="0" width="139"><br><br>
			</td>
		</tr>
	</tbody>
</table>



<h2>About Me</h2>
	<p style="text-align:justify;">I am a Ph.D. student (2021-Present) at the Department of Computer Science and Engineering (CSE), <a href="https://www.buffalo.edu">University at Buffalo-The State University of New York (UB) </a>, working with <a href="https://engineering.buffalo.edu/computer-science-engineering/people/faculty-directory.host.html/content/shared/engineering/computer-science-engineering/profiles/faculty/nwogu-ifeoma.html"> Prof. Ifeoma Nwogu </a> at the Human Behavior Modeling Lab. Prior to my Ph.D. study, I received my Master's degree at the School of Computer Science and Technology, <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University (XJTU) </a>, China, where I was advised by  <a href="https://gr.xjtu.edu.cn/web/xyyang">Prof. Xinyu Yang </a> at the<a href="https://gr.xjtu.edu.cn/web/xyyang/11"> YLab </a>. Moreover, two Bachelor's degree obtained from <a href="https://en.neepu.edu.cn/">  Northeast Electric Power University (NEEPU)</a>, China, respectively, Bachelor of Computer Science and Technology & Bachelor of  Electrical Engineering and Automation.
	</p>
	<p style="text-align:justify;">I work on Machine Learning (<strong>ML</strong>) and Computer Vision (<strong>CV</strong>). My research interests are Multimodal, Human Pose Estimation, 3D Mesh Reconstruction, Robotics Physics Simulation, Language-driven Motion, Sign Language Translation & Generation, and Video Understanding, primarily in developing Computer Vision, Natural Language Processing, Reinforcement Learning, Statistic Machine Learning, and Mathematical Modeling to study different human behavior and make generative models more effective in serving human needs. My early works are related to Speech Sentiment Analysis, Music Analysis with Machine Learning and well-versed in data science techniques including Data Crawling, Pattern Mining, Information Retrieval, and Search Engine Optimization. Feel free to reach out to me if you are interested in collaborating. I am also actively looking for research intern positions. 
	</p>


<h2>News [<img src="./index_files//update.gif">]</h2>
	<div  style="overflow-y: scroll; height:200px;">
	<table border="1" style="border-width: 0px;" width="1050">
	<tbody>
	<tr>
	<td style="border-style: none; border-width: medium;">
	<ul>
		<li type="circle">2022.11: I was invited to be the <strong>judge</strong> of the 2022  <strong>UB Hacking Competition! </strong> </li>
		<li type="circle">2022.10: I was invited to <strong> give a talk </strong> in the course CSE501, Fall2022, UB.</li>
		<li type="circle">2022.05: Start my internship at <strong> OPPO U.S. Research Center</strong> at Palo Alto, CA. (on-site)</li>
		<li type="circle">2021.08: Start my new Ph.D journey at <strong>UB</strong>, Buffalo, NY.</li>
		<li type="circle">2021.05: I passed the Ph.D Research Potential Assessment at RIT!</li>
		<li type="circle">2020.08: Start my new Ph.D journey at <strong>RIT</strong>, Rochester, NY.</li>
	</ul><br>
	</td>
	</tr>
	</tbody>
	</table>
	</div>

<h2>Selected Publications</h2>
  <h3>Under Review</h3>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/unseen.png);"></div>
	<div class="row-text" align="justify">
    <strong>AtomVAE: Towards Zero-Shot Text-to-Motion Synthesis (CVPR2023)</strong> <br/>
		Abstract: Human motion synthesis is an important task due to its wide applications in computer graphics, human behavior understanding, human-robotic interactions, etc. Existing methods focus on closed-set human action synthesis, limiting their ability to generate actions of novel classes that are not seen during training. To address this problem, we propose AtomVAE for zero-shot text-to-motion synthesis, which aims to generate novel categories of actions that do not belong to the training set. Specifically, the proposed AtomVAE learns a mapping from the input text space that describes the action to the output motion space that represent the action, using limited number of training labels. To facilitate zero-shot text-to-motion synthesis, we propose to decompose seen actions into a set of atomic actions, such that by leveraging the similarity between input text and the learned atomic actions, our model can learn to compose novel actions using the atomic actions. The atomic actions are learned in an end-to-end manner, with diversity and sparsity constrains enforcing the atomicity and robustness. Our method not only achieves promising results on zero-shot synthesis, but also outperforms state-of-the-art approaches by a large margin in the traditional closed-set synthesis task.<br/>
	</div>
  </div>





<h3>Ongoing</h3>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/demo.png);"></div>
	<div class="row-text" align="justify">
		SignCLIP: From General Language to Sign Language<br/>
		Sign language generation not only promotes communication between the hearing-impaired community and the general public but also further improves the mutual understanding between humans and robots.  We propose SignDict, a text-to-sign video approach to deal with challenges in pose estimation and the design of generative networks to further improve the performance.<br/>
	</div>
  </div>


<div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/demo.png);"></div>
	<div class="row-text" align="justify">
		Whole3DEst:  Human Whole-Body 3D Pose Estimation under Self-supervision<br/>
		Existing self-supervised 3D human pose estimation schemes rely heavily on weak supervision without faces and hands, which inevitably leads to insufficient body language understanding and poor results in self-occluded realistic scenes. We propose a novel self-supervised approach that allows us to explicitly generate 3D human full-body estimates without 3D ground truth data. Specifically, we take advantage of physical simulations to learn dual co-evolved pose estimation networks.<br/>
	</div>
  </div>





  <h3>Previous</h3>

  <div class="publication row clearfix">
	<div class="row-media" style="background-image: url(files/pubs/xintianyou.png);"></div>
	<div class="row-text">
		Exploring the General Melodic Characteristics of XinTianYou Folk Songs<br/>
		Juan Li, <strong>Lu Dong</strong>, Jianhang Ding, Xinyu Yang<br/>
		<span class="italic"></i>12th Sound and Music Computing Conference(<strong>SMC</strong>) </span><br/>
		<a class="btn btn-orange" href="https://zenodo.org/record/851035#.Y2nrz-yZPeo">DOI</a> 
		<a class="btn btn-red" href="https://github.com/dongludeeplearning/MIR/blob/master/Exploring%20the%20General%20Melodic%20Characteristics%20of%20XinTianYou%20Folk%20Songs.pdf">PDF</a>
		<a class="btn" href="bibs/xintianyou.txt">BibTeX</a>
	</div>
  </div>


<h2>Selected Project Demo</h2>
<video controls="controls" width="100%">
  <source src="./files/pubs/Pre_lu_vibe_result.mp4" type="video/mp4">
  <source src="./files/pubs/Pre_lu_vibe_result.webm" type=‘video/webm‘ />
  <source src="./files/pubs/Pre_lu_vibe_result.ogg" type="video/ogg">
Your browser does not support the video tag.
</video>





​	






<h2>Experience</h2>
  <ul>
	<li type="disc">
    <strong>Research Internship </strong> OPPO US Research Center, Palo Alto, CA, On-Site, May 2022- Aug 2022.
		</li>  
  <li type="disc">
    <strong>Research Assitant</strong> University at Buaffalo SUNY (UB), Aug 2021- Now.
		</li>
  <li type="disc">
    <strong>Research Assitant</strong> Rochester Institute of Technology(RIT), Aug 2020- May 2021.
  	</li>   
  <li type="disc">
    <strong>Senior Data Analyst</strong> Shaanxi Haina Electronic Technology Co.,LT, Sep 2016- Apr 2020.
		</li>    
  <li type="disc">
    <strong>Research Assitant</strong> XI'An Jiaotong University (XJTU), Aug 2013- May 2016.
		</li>    
  </ul>	





<h2>Selected Awards & Honors</h2>
  <h3>Awards</h3>
  	<ul>
      <li type="disc"> <strong>Outstanding Leadership Award</strong>, Shaanxi Haina Electronic Technology Co.,LT. 2018 </li>
      <li type="disc"> <strong>National Graduate Academic Scholarship </strong>, Xi'an Jiaotong University (XJTU), 2013-2016. </li>
      <li type="disc"> <strong>Silver Metal</strong>, Universiade Women's Hurdle，Xi'an Jiaotong University (XJTU), 2014. </li>
      <li type="disc"> <strong>National Encouragement Scholarship</strong>, Northeast Electric Power University (NEEPU), 2010.(top 5%) </li>
      <li type="disc"> <strong>The Academic Scholarship </strong>, Northeast Electric Power University (NEEPU), 2010. (top 10%) </li>
      <li type="disc"> <strong>Champion & MVP </strong>, College Women's Basketball of NEEPU, 2010. </li>
      <br>
</ul>      
  <h3>Honors</h3>
  	<ul>
      <li type="disc"> <strong>Excellent Postgraduate Student </strong>, XI'an Jiaotong University (XJTU), 2014 & 2015. (top 10%)</li>
      <li type="disc"> <strong>Excellent Student Cadre </strong>, Northeast Electric Power University (NEEPU), 2010. </li>
      <li type="disc"> <strong>Excellent Undergraduate Student </strong>,Northeast Electric Power University (NEEPU).</li>
</ul>




<h2>Academic Services</h2>
  <h3>Competition</h3>

  	<ul>
  		<li type="disc">2022.11.06 I was invited as a <strong> Judge </strong> for <a href="https://www.ubhacking.com/"> 2022 UB Hacking Competition. </a></li>
  	</ul>



  <h3>Talks</h3>
  	<ul>
  		<li type="disc">2022.10.18: Delivering a talk, introducing Human Pose Estimation and Ph.D Life Guidance in the course CSE-501, Fall2022.</li>
  	</ul>





<h2>Selected Photos</h2>

  	<ul>
  		<div style="width:960px;overflow-x:scroll">
  		<div style="width:2280px">
        <img src="./files/66.png"  border="0" width="390"/>
        <img src="./files/33.jpeg"  border="0" width="390"/>
        <img src="./files/intern.jpeg"  border="0" width="360"/>
 		 	 	<img src="./files/11.png" border="0" width="380"/>
        <img src="./files/77.png" border="0" width="290"/>
 		  </div>
			</div>
  	</ul>



<hr>
<table cellpadding="0px">
	<tbody>
		<tr>
			<td width="720px">
			<div id="clustrmaps-widget"></div>
				<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=7seMHMuQZlvELLsuK6wd30IX5cz7U58mGsA8iV8s6_o"></script></script>
				</script>
			<div class="jvectormap-tip"></div>
			</td>
			<td valign="top">
				<div style="clear:both;">
				<p align="right"><font size="2">Last Updated on Nov, 2022<br>
				Published with <a href="https://pages.github.com/">GitHub Pages</a></font></p>
				</div>
				<div style="clear: both;">
				<p align="right"><font size="5"><img src="./index_files//raccoon.gif"></font></p>
				</div>
			</td>
		</tr>
	</tbody>
</table>


</body>
</html>