<html>

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">	
	<title>Lu Dong</title>
	<meta content="LuDong, Lu Dong UB, lu dong ub, ludong,  https://dongludeeplearning.github.io/" name="keywords">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<script async="" src="http://www.google-analytics.com/analytics.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-66888300-1', 'auto');
		ga('send', 'pageview');
	</script>
	<script type="text/javascript" src="./index_files/jquery-1.12.4.min.js"></script>
	<style>
	a {color: #2471a3; text-decoration: none;}
	a:hover {color: #5499c7;}
	a.btn {background-color: #2471a3; color: #fff; font-size: 12px; margin-top: 4px; display: inline-block; padding: 0 4px; border-radius: 2px; line-height: 1.5em;}
	a.btn:hover {background-color: #174e74;}
	a.btn.btn-red {background-color: #d63a3a;}
	a.btn.btn-red:hover {background-color: #a02727;}
	a.btn.btn-orange {background-color: #e2700c;}
	a.btn.btn-orange:hover {background-color: #c46009;}
	a.btn.btn-dark {background-color: #345;}
	a.btn.btn-dark:hover {background-color: #234;}
	p {line-height: 1.5em;}
	.nobreak {white-space: nowrap;}
	.noselect {-webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none;}
	.bold {font-weight: bold;}
	.italic {font-style: italic;}
	.bulletpoints {line-height: 1.5em;}
	.row {box-sizing: border-box;}
	.row-media {display: block; float: left; width: 280px; height: 150px; background-position: center; background-size: contain; background-repeat: no-repeat; border-radius: 6px; border: 1px solid #def;}
	.row-text {display: block; float: left; margin-left: 16px; line-height: 1.5em; max-width: 552px;}
	.row-text span {line-height: inherit;}
	.clearfix {content: ""; clear: both; display: table;}
	.publication {margin-bottom: 32px; margin-left: 16px;}
	.press {width: 100px; height: 80px; border: 1px solid #def; margin-right: 12px; background-size: cover;}  
	.img-contain {background-size: contain !important;}
	.footer {background-color: #345; width: 100%}
	.footer-content {color: #fff; font-size: 10px; padding: 6px 0; max-width: 900px; margin: auto;}




	@media only screen and (max-width: 1150px) {
		.header-profile-picture, .header-text {display: block; margin: auto; text-align: center;}
		.header-profile-picture {margin-bottom: 12px; width: 140px; height: 140px;}
		body {font-size: 18px;}
		a.btn {font-size: 14px; padding: 2px 6px;}
	}
	
	@media only screen and (max-width: 900px) {
		.publication {margin-bottom: 46px;}
		.publication .row-media {width: 260px; height: 130px; margin: auto; margin-bottom: 12px; display: block;}
		.publication .row-text {display: block; width: 100%; margin-left: 0;}
		.press {display: block;}
	}
	</style>
</head>

<body>







<h1 style="text-align: center; margin-top: 100px;">Ig3D: Integrating 3D Face Representations in Facial Expression Inference</h1>
<h3 style="text-align: center; margin-top: 10px;"> <a href="https://dongludeeplearning.github.io/" target="_blank">Lu Dong</a>*, <a href="https://wangxiaoshawn.github.io/" target="_blank">Xiao Wang</a>*, <a href="https://www.buffalo.edu/cubs/members/srirangaraj-setlur.html" target="_blank">Srirangaraj Setlur</a>, <a href="https://www.buffalo.edu/research/about-us/leadership/venu-govindaraju.html" target="_blank">Venu Govindaraju</a>, <a href="https://engineering.buffalo.edu/home/research/faculty/diverse-faculty.host.html/content/shared/engineering/computer-science-engineering/profiles/faculty/ladder/nwogu-ifeoma.html" target="_blank">Ifeoma Nwogu</a></h3>
<h4 style="text-align: center; margin-top: 10px;">*Equal Contribution</h4>
<h4 style="text-align: center; margin-top: 10px;"> National AI Institute for Exceptional Education</h4>
<h4 style="text-align: center; margin-top: 10px;"> University at Buffalo, SUNY</h4>
<div style="text-align: center; margin-top: 30px;">
  <a href="" target="_blank">Paper</a> |
  <a href="https://arxiv.org/abs/2408.16907" target="_blank">ArXiv</a> |
  <a href="https://github.com/dongludeeplearning/Ig3D" target="_blank"> Code </a> |
  <a href="./files/ECCVWPoster.pdf" target="_blank"> Poster </a>
</div>










<img src="./files/pubs/Ig3D_3DMM.png" alt="Ig3D Project" width="100%">

<img src="./files/pubs/Ig3D_framework.png" alt="Ig3D Project" width="100%">








<h3>Abstract</h3>

<p style="text-align:justify;">  Reconstructing 3D faces with facial geometry from single images has allowed for major advances in animation, generative models, and virtual reality. However, this ability to represent faces with their 3D features is not as fully explored by the facial expression inference (FEI) community.
This study therefore aims to investigate the impacts of integrating such 3D representations into the FEI task, specifically for facial expression classification and face-based valence-arousal (VA) estimation.
To accomplish this, we first assess the performance of two 3D face representations (both based on the 3D morphable model, FLAME) for the FEI tasks. We further explore two fusion architectures, intermediate fusion and late fusion, for integrating the 3D face representations with existing 2D inference frameworks. To evaluate our proposed architecture, we extract the corresponding 3D representations and perform extensive tests on the AffectNet and RAF-DB datasets.  
Our experimental results demonstrate that our proposed method outperforms the state-of-the-art AffectNet VA estimation and RAF-DB classification tasks. Moreover, our method can act as a complement to other existing methods to boost performance in many emotion inference tasks. 
</p>




<h4 style="text-align: left; margin-left: 20px;"> Type: Conference Workshop Paper</h4>
<h4 style="text-align: left; margin-left: 20px;"> Publication: Springer ECCV 2024</h4>
<h4 style="text-align: left; margin-left: 20px;"> Special Thanks: We thank <a href="https://www.yhzhai.com/" target="_blank">Yuanhao Zhai</a> for the insightful discussions. </h4>

â€‹        

<h2 class="title is-3">BibTeX</h2>

<pre><code style="font-size: 16px;">
@article{dong2024ig3d,
  title={Ig3D: Integrating 3D Face Representations in Facial Expression Inference},
  author={Dong, Lu and Wang, Xiao and Setlur, Srirangaraj and Govindaraju, Venu and Nwogu, Ifeoma},
  journal={arXiv preprint arXiv:2408.16907},
  year={2024}
}
</code></pre>



















<hr>
<table cellpadding="0px">
	<tbody>
		<tr>
			<td width="720px">
			<div id="clustrmaps-widget"></div>
				<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=7seMHMuQZlvELLsuK6wd30IX5cz7U58mGsA8iV8s6_o"></script></script>
				</script>
			<div class="jvectormap-tip"></div>
			</td>
			<td valign="top">
				<div style="clear:both;">
				<p align="right"><font size="2">Last Updated on September, 2024<br>
				</div>
				<div style="clear: both;">
				<p align="right"><font size="5"><img src="./index_files//happy.gif"></font></p>
				</div>
			</td>
		</tr>
	</tbody>
</table>





</body>
</html>