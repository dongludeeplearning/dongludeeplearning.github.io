<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ JMLR 2022
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        William Fedus, Barret Zoph, Noam Shazeer<br>
        <span style="opacity:0.8">Google Research</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we scale neural language models to trillion parameters while maintaining computational efficiency through sparse activation, overcoming the limitations of traditional dense models and previous mixture-of-experts approaches?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•é€šè¿‡ç¨€ç–æ¿€æ´»å°†ç¥ç»è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°ä¸‡äº¿å‚æ•°ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œå…‹æœä¼ ç»Ÿå¯†é›†æ¨¡å‹å’Œä¹‹å‰ä¸“å®¶æ··åˆæ–¹æ³•çš„é™åˆ¶ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Switch Transformer Architecture:</b> Introduced a simplified and efficient Mixture-of-Experts (MoE) routing algorithm that routes each token to exactly one expert, reducing communication costs and improving training stability compared to previous MoE approaches.</div>
            <div class="lang-zh" style="display:none"><b>Switch Transformeræ¶æ„ï¼š</b>å¼•å…¥äº†ç®€åŒ–å’Œé«˜æ•ˆçš„ä¸“å®¶æ··åˆ(MoE)è·¯ç”±ç®—æ³•ï¼Œå°†æ¯ä¸ªtokenç²¾ç¡®è·¯ç”±åˆ°ä¸€ä¸ªä¸“å®¶ï¼Œé™ä½äº†é€šä¿¡æˆæœ¬å¹¶æé«˜äº†è®­ç»ƒç¨³å®šæ€§ï¼Œç›¸è¾ƒäºä¹‹å‰çš„MoEæ–¹æ³•ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scalability to Trillion Parameters:</b> Demonstrated the ability to train models with up to a trillion parameters using efficient combinations of data, model, and expert parallelism, achieving 4x speedup over dense T5-XXL models.</div>
            <div class="lang-zh" style="display:none"><b>æ‰©å±•åˆ°ä¸‡äº¿å‚æ•°ï¼š</b>å±•ç¤ºäº†ä½¿ç”¨æ•°æ®ã€æ¨¡å‹å’Œä¸“å®¶å¹¶è¡Œçš„é«˜æ•ˆç»„åˆæ¥è®­ç»ƒæœ€å¤šä¸‡äº¿å‚æ•°æ¨¡å‹çš„èƒ½åŠ›ï¼Œå®ç°äº†æ¯”å¯†é›†T5-XXLæ¨¡å‹4å€çš„åŠ é€Ÿã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Training Stability Improvements:</b> Developed techniques including simplified routing, expert capacity management, and lower-precision training (bfloat16) that enable stable training of large sparse models for the first time.</div>
            <div class="lang-zh" style="display:none"><b>è®­ç»ƒç¨³å®šæ€§æ”¹è¿›ï¼š</b>å¼€å‘äº†æŠ€æœ¯åŒ…æ‹¬ç®€åŒ–çš„è·¯ç”±ã€ä¸“å®¶å®¹é‡ç®¡ç†å’Œä½ç²¾åº¦è®­ç»ƒ(bfloat16)ï¼Œé¦–æ¬¡å®ç°äº†å¤§å‹ç¨€ç–æ¨¡å‹çš„ç¨³å®šè®­ç»ƒã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Superior Scaling Properties:</b> Achieved up to 7x pre-training speed improvements over T5 models with the same computational resources, with benefits extending to multilingual settings across all 101 languages.</div>
            <div class="lang-zh" style="display:none"><b>å“è¶Šçš„æ‰©å±•ç‰¹æ€§ï¼š</b>å®ç°äº†æ¯”T5æ¨¡å‹7å€çš„é¢„è®­ç»ƒé€Ÿåº¦æå‡ï¼Œä½¿ç”¨ç›¸åŒçš„è®¡ç®—èµ„æºï¼Œç›Šå¤„æ‰©å±•åˆ°å¤šè¯­è¨€è®¾ç½®ä¸­çš„æ‰€æœ‰101ç§è¯­è¨€ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Knowledge Distillation:</b> Showed that large sparse Switch Transformers can be effectively distilled into small dense models, reducing model size by up to 99% while preserving 30% of the quality gains.</div>
            <div class="lang-zh" style="display:none"><b>çŸ¥è¯†è’¸é¦ï¼š</b>å±•ç¤ºäº†å¤§å‹ç¨€ç–Switch Transformerså¯ä»¥æœ‰æ•ˆè’¸é¦åˆ°å°å‹å¯†é›†æ¨¡å‹ï¼Œå°†æ¨¡å‹å¤§å°å‡å°‘é«˜è¾¾99%ï¼ŒåŒæ—¶ä¿ç•™30%çš„è´¨é‡æå‡ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>MoE Routing Complexity:</b> Traditional Mixture-of-Experts models suffer from complex routing algorithms that introduce communication overhead, training instability, and load balancing issues.</div>
            <div class="lang-zh" style="display:none"><b>MoEè·¯ç”±å¤æ‚åº¦ï¼š</b>ä¼ ç»Ÿçš„ä¸“å®¶æ··åˆæ¨¡å‹é­å—å¤æ‚è·¯ç”±ç®—æ³•çš„å½±å“ï¼Œä¼šå¼•å…¥é€šä¿¡å¼€é”€ã€è®­ç»ƒä¸ç¨³å®šæ€§å’Œè´Ÿè½½å¹³è¡¡é—®é¢˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Training Instability:</b> Sparse models with conditional computation are prone to training instabilities, gradient vanishing/exploding, and expert collapse, making them difficult to scale to large sizes.</div>
            <div class="lang-zh" style="display:none"><b>è®­ç»ƒä¸ç¨³å®šï¼š</b>å…·æœ‰æ¡ä»¶è®¡ç®—çš„ç¨€ç–æ¨¡å‹å®¹æ˜“å‡ºç°è®­ç»ƒä¸ç¨³å®šã€æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å’Œä¸“å®¶å´©æºƒç­‰é—®é¢˜ï¼Œä½¿å…¶éš¾ä»¥æ‰©å±•åˆ°å¤§å‹æ¨¡å‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Communication Costs:</b> Distributing sparse computations across multiple devices introduces significant communication overhead that can outweigh the computational benefits of sparsity.</div>
            <div class="lang-zh" style="display:none"><b>é€šä¿¡æˆæœ¬ï¼š</b>è·¨å¤šä¸ªè®¾å¤‡åˆ†å¸ƒç¨€ç–è®¡ç®—ä¼šå¼•å…¥æ˜¾è‘—çš„é€šä¿¡å¼€é”€ï¼Œå¯èƒ½è¶…è¿‡ç¨€ç–æ€§çš„è®¡ç®—ç›Šå¤„ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Precision and Scale Trade-offs:</b> Achieving both numerical stability with lower precision formats and massive scale requires careful balancing of model architecture, training techniques, and distributed computing strategies.</div>
            <div class="lang-zh" style="display:none"><b>ç²¾åº¦å’Œè§„æ¨¡æƒè¡¡ï¼š</b>å®ç°ä½ç²¾åº¦æ ¼å¼çš„æ•°å€¼ç¨³å®šæ€§å’Œå¤§è§„æ¨¡éœ€è¦ä»”ç»†å¹³è¡¡æ¨¡å‹æ¶æ„ã€è®­ç»ƒæŠ€æœ¯å’Œåˆ†å¸ƒå¼è®¡ç®—ç­–ç•¥ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Load Balancing:</b> Ensuring even distribution of computation across experts while maintaining model quality is challenging, especially when expert capacities are limited or routing decisions are made independently.</div>
            <div class="lang-zh" style="display:none"><b>è´Ÿè½½å¹³è¡¡ï¼š</b>ç¡®ä¿è·¨ä¸“å®¶çš„å‡åŒ€è®¡ç®—åˆ†å¸ƒåŒæ—¶ä¿æŒæ¨¡å‹è´¨é‡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å½“ä¸“å®¶å®¹é‡æœ‰é™æˆ–è·¯ç”±å†³ç­–ç‹¬ç«‹åšå‡ºæ—¶ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The Switch Transformer introduces a simplified MoE approach for efficient sparse language modeling:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Simplified Routing:</b> Each token is routed to exactly one expert based on a learned routing function, eliminating the need for load balancing and reducing communication costs compared to top-k routing.</li>
                <li style="margin-bottom:6px;"><b>Efficient Implementation:</b> All-to-all communication is replaced with scatter and gather operations, and expert computation is organized to minimize memory transfers and maximize hardware utilization.</li>
                <li style="margin-bottom:6px;"><b>Capacity Management:</b> A "no-token-left-behind" mechanism ensures all tokens are processed by allowing experts to exceed their capacity when necessary, preventing token dropping.</li>
                <li style="margin-bottom:6px;"><b>Training Techniques:</b> Selective precision training with bfloat16, expert dropout for regularization, and improved initialization schemes that enable stable training of trillion-parameter models.</li>
                <li style="margin-bottom:6px;"><b>Parallelism Strategy:</b> Combines data, model, and expert parallelism to efficiently distribute computation across thousands of accelerators, enabling unprecedented model scale.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>Switch Transformerå¼•å…¥äº†ä¸€ç§ç®€åŒ–çš„MoEæ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆçš„ç¨€ç–è¯­è¨€å»ºæ¨¡ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>ç®€åŒ–è·¯ç”±ï¼š</b>æ¯ä¸ªtokenåŸºäºå­¦ä¹ çš„è·¯ç”±å‡½æ•°ç²¾ç¡®è·¯ç”±åˆ°ä¸€ä¸ªä¸“å®¶ï¼Œæ¶ˆé™¤äº†è´Ÿè½½å¹³è¡¡çš„éœ€æ±‚ï¼Œå¹¶é™ä½äº†ä¸top-kè·¯ç”±ç›¸æ¯”çš„é€šä¿¡æˆæœ¬ã€‚</li>
                <li style="margin-bottom:6px;"><b>é«˜æ•ˆå®ç°ï¼š</b>å°†all-to-allé€šä¿¡æ›¿æ¢ä¸ºscatterå’Œgatheræ“ä½œï¼Œå¹¶ç»„ç»‡ä¸“å®¶è®¡ç®—ä»¥æœ€å°åŒ–å†…å­˜ä¼ è¾“å¹¶æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>å®¹é‡ç®¡ç†ï¼š</b>"no-token-left-behind"æœºåˆ¶ç¡®ä¿æ‰€æœ‰tokenéƒ½è¢«å¤„ç†ï¼Œå…è®¸ä¸“å®¶åœ¨å¿…è¦æ—¶è¶…è¿‡å…¶å®¹é‡ï¼Œé˜²æ­¢tokenä¸¢å¤±ã€‚</li>
                <li style="margin-bottom:6px;"><b>è®­ç»ƒæŠ€æœ¯ï¼š</b>ä½¿ç”¨bfloat16çš„é€‰æ‹©æ€§ç²¾åº¦è®­ç»ƒã€ä¸“å®¶dropoutè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥åŠæ”¹è¿›çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œä½¿ä¸‡äº¿å‚æ•°æ¨¡å‹çš„ç¨³å®šè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¹¶è¡Œç­–ç•¥ï¼š</b>ç»“åˆæ•°æ®ã€æ¨¡å‹å’Œä¸“å®¶å¹¶è¡Œï¼Œä»¥é«˜æ•ˆæ–¹å¼è·¨æ•°åƒä¸ªåŠ é€Ÿå™¨åˆ†å¸ƒè®¡ç®—ï¼Œå®ç°å‰æ‰€æœªæœ‰çš„æ¨¡å‹è§„æ¨¡ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/switch_transformers_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <div style="height:1px; background:rgba(255,255,255,.10); margin:12px 0;"></div>
            <img src="Figures/switch_transformers_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            
            <p>Switch Transformers demonstrate superior scaling properties with increasing sparsity (more experts), achieving up to 7x pre-training speedups over dense T5 models while maintaining sample efficiency.</p>
            <p>Note: The architecture enables training of trillion-parameter models through simplified MoE routing, reduced communication costs, and efficient distributed training techniques.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_switch_transformers.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_switch_transformers.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The Switch Transformer's success stems from fundamental improvements to sparse model training and scaling:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Simplified Routing Architecture:</b> By routing each token to exactly one expert instead of multiple experts, the Switch Transformer eliminates complex load balancing requirements and reduces communication overhead by 90% compared to traditional MoE approaches.</li>
                <li style="margin-bottom:6px;"><b>Efficient Expert Computation:</b> Organizing experts to maximize hardware utilization and minimizing memory transfers allows sparse models to achieve computational efficiency that rivals or exceeds dense models at scale.</li>
                <li style="margin-bottom:6px;"><b>Robust Training Techniques:</b> Capacity management prevents token dropping, selective precision training enables stable bfloat16 training, and improved initialization schemes prevent expert collapse, making trillion-parameter training feasible.</li>
                <li style="margin-bottom:6px;"><b>Parallelism Innovation:</b> The combination of data, model, and expert parallelism provides near-perfect scaling efficiency across thousands of accelerators, enabling unprecedented model sizes that were previously impossible.</li>
                <li style="margin-bottom:6px;"><b>Quality Preservation:</b> Distillation techniques allow the benefits of massive sparse models to be transferred to smaller dense models, making the approach practical for deployment while retaining significant quality improvements.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>Switch Transformerçš„æˆåŠŸæºäºå¯¹ç¨€ç–æ¨¡å‹è®­ç»ƒå’Œæ‰©å±•çš„åŸºæœ¬æ”¹è¿›ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>ç®€åŒ–è·¯ç”±æ¶æ„ï¼š</b>é€šè¿‡å°†æ¯ä¸ªtokenç²¾ç¡®è·¯ç”±åˆ°ä¸€ä¸ªä¸“å®¶è€Œä¸æ˜¯å¤šä¸ªä¸“å®¶ï¼ŒSwitch Transformeræ¶ˆé™¤äº†å¤æ‚çš„è´Ÿè½½å¹³è¡¡è¦æ±‚ï¼Œå¹¶å°†é€šä¿¡å¼€é”€é™ä½äº†90%ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„MoEæ–¹æ³•ã€‚</li>
                <li style="margin-bottom:6px;"><b>é«˜æ•ˆä¸“å®¶è®¡ç®—ï¼š</b>ç»„ç»‡ä¸“å®¶ä»¥æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡å¹¶æœ€å°åŒ–å†…å­˜ä¼ è¾“ï¼Œä½¿ç¨€ç–æ¨¡å‹èƒ½å¤Ÿåœ¨è§„æ¨¡ä¸Šå®ç°ä¸å¯†é›†æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„è®¡ç®—æ•ˆç‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>é²æ£’è®­ç»ƒæŠ€æœ¯ï¼š</b>å®¹é‡ç®¡ç†é˜²æ­¢tokenä¸¢å¤±ï¼Œé€‰æ‹©æ€§ç²¾åº¦è®­ç»ƒå®ç°ç¨³å®šçš„bfloat16è®­ç»ƒï¼Œæ”¹è¿›çš„åˆå§‹åŒ–æ–¹æ¡ˆé˜²æ­¢ä¸“å®¶å´©æºƒï¼Œä½¿ä¸‡äº¿å‚æ•°è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¹¶è¡Œåˆ›æ–°ï¼š</b>æ•°æ®ã€æ¨¡å‹å’Œä¸“å®¶å¹¶è¡Œçš„ç»„åˆæä¾›äº†è·¨æ•°åƒä¸ªåŠ é€Ÿå™¨çš„è¿‘ä¹å®Œç¾çš„æ‰©å±•æ•ˆç‡ï¼Œå®ç°å‰æ‰€æœªæœ‰çš„æ¨¡å‹è§„æ¨¡ã€‚</li>
                <li style="margin-bottom:6px;"><b>è´¨é‡ä¿ç•™ï¼š</b>è’¸é¦æŠ€æœ¯å…è®¸å°†å¤§å‹ç¨€ç–æ¨¡å‹çš„ç›Šå¤„è½¬ç§»åˆ°è¾ƒå°çš„å¯†é›†æ¨¡å‹ï¼Œä½¿è¯¥æ–¹æ³•åœ¨éƒ¨ç½²æ—¶å®ç”¨ï¼ŒåŒæ—¶ä¿ç•™æ˜¾è‘—çš„è´¨é‡æ”¹è¿›ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This groundbreaking work establishes Switch Transformers as a fundamental breakthrough in scaling neural language models, demonstrating that sparsity through Mixture-of-Experts can achieve unprecedented parameter counts while maintaining computational efficiency. By simplifying the MoE routing algorithm to route each token to exactly one expert, the authors eliminate the communication and training instabilities that have plagued previous sparse approaches. The result is a model that can scale to trillion parametersâ€”orders of magnitude beyond previous limitsâ€”while achieving up to 7x pre-training speed improvements and 4x speedups over dense baselines. The training innovations, including bfloat16 precision training, capacity management, and expert regularization, make large-scale sparse training practical for the first time. The multilingual evaluation across 101 languages and successful distillation to smaller models further demonstrate the broad applicability of the approach. Switch Transformers represent a pivotal advancement in machine learning architecture, showing that intelligent sparsity can unlock the next generation of language models. The work not only advances the theoretical understanding of how to scale neural networks but provides practical techniques that have been adopted in subsequent large language models. By enabling efficient training of models with outrageous parameter counts, Switch Transformers open new frontiers in natural language processing and pave the way for more capable AI systems.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å¼€åˆ›æ€§å·¥ä½œå°†Switch Transformersç¡®ç«‹ä¸ºæ‰©å±•ç¥ç»è¯­è¨€æ¨¡å‹çš„åŸºæœ¬çªç ´ï¼Œè¯æ˜é€šè¿‡ä¸“å®¶æ··åˆçš„ç¨€ç–æ€§å¯ä»¥å®ç°å‰æ‰€æœªæœ‰çš„å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å°†MoEè·¯ç”±ç®—æ³•ç®€åŒ–ä¸ºå°†æ¯ä¸ªtokenç²¾ç¡®è·¯ç”±åˆ°ä¸€ä¸ªä¸“å®¶ï¼Œä½œè€…æ¶ˆé™¤äº†å›°æ‰°ä¹‹å‰ç¨€ç–æ–¹æ³•çš„é€šä¿¡å’Œè®­ç»ƒä¸ç¨³å®šæ€§ã€‚ç»“æœæ˜¯ä¸€ä¸ªå¯ä»¥æ‰©å±•åˆ°ä¸‡äº¿å‚æ•°çš„æ¨¡å‹â€”â€”æ¯”ä¹‹å‰æé™é«˜å‡ºå‡ ä¸ªæ•°é‡çº§â€”â€”åŒæ—¶å®ç°é«˜è¾¾7å€çš„é¢„è®­ç»ƒé€Ÿåº¦æå‡å’Œæ¯”å¯†é›†åŸºçº¿4å€çš„é€Ÿåº¦æå‡ã€‚åŒ…æ‹¬bfloat16ç²¾åº¦è®­ç»ƒã€å®¹é‡ç®¡ç†å’Œä¸“å®¶æ­£åˆ™åŒ–åœ¨å†…çš„è®­ç»ƒåˆ›æ–°ï¼Œé¦–æ¬¡ä½¿å¤§è§„æ¨¡ç¨€ç–è®­ç»ƒå˜å¾—å®ç”¨ã€‚è·¨101ç§è¯­è¨€çš„å¤šè¯­è¨€è¯„ä¼°å’ŒæˆåŠŸè’¸é¦åˆ°è¾ƒå°æ¨¡å‹è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚Switch Transformersä»£è¡¨äº†æœºå™¨å­¦ä¹ æ¶æ„çš„å…³é”®è¿›æ­¥ï¼Œè¡¨æ˜æ™ºèƒ½ç¨€ç–æ€§å¯ä»¥è§£é”ä¸‹ä¸€ä»£è¯­è¨€æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ¨è¿›äº†å¯¹å¦‚ä½•æ‰©å±•ç¥ç»ç½‘ç»œçš„ç†è®ºç†è§£ï¼Œè¿˜æä¾›äº†å·²è¢«åç»­å¤§å‹è¯­è¨€æ¨¡å‹é‡‡ç”¨çš„å®ç”¨æŠ€æœ¯ã€‚é€šè¿‡å®ç°å…·æœ‰æƒŠäººå‚æ•°æ•°é‡çš„æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒï¼ŒSwitch Transformersä¸ºè‡ªç„¶è¯­è¨€å¤„ç†å¼€è¾Ÿäº†æ–°å‰æ²¿ï¼Œå¹¶ä¸ºæ›´å¼ºå¤§çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2101.03961" target="_blank" style="color:#8bffcf;">2101.03961</a></p>
          <p><b>JAX Code:</b> <a href="https://github.com/kyegomez/SwitchTransformers" target="_blank" style="color:#8bffcf;">https://github.com/kyegomez/SwitchTransformers</a></p>
          <p><b>TensorFlow Code:</b> <a href="https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py" target="_blank" style="color:#8bffcf;">tensorflow/mesh</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# Switch Transformer Implementation<br>
<br>
// Simplified MoE Routing - Each token to exactly one expert<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">SwitchRouter</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Routes each token to exactly one expert based on learned routing function"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, num_experts, hidden_dim):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.router = nn.Linear(hidden_dim, num_experts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_experts = num_experts<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">route</span>(self, inputs):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Route each token to one expert"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logits = self.router(inputs)  <span style="color:#6a9955;"># (batch_size, seq_len, num_experts)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert_indices = torch.argmax(logits, dim=-1)  <span style="color:#6a9955;"># (batch_size, seq_len)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> expert_indices<br>
<br>
// Expert Capacity Management - No Token Left Behind<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">ExpertCapacityManager</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Manages expert capacity to prevent token dropping"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, num_experts, capacity_factor=1.0):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.capacity_factor = capacity_factor<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_experts = num_experts<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">get_capacity</span>(self, batch_size, seq_len):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Calculate per-expert capacity"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokens_per_expert = (batch_size * seq_len) // self.num_experts<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;capacity = <span style="color:#569cd6;">int</span>(tokens_per_expert * self.capacity_factor)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> capacity<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">enforce_capacity</span>(self, expert_indices, batch_size, seq_len):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Allow experts to exceed capacity to prevent token dropping"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;capacity = self.get_capacity(batch_size, seq_len)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># In Switch Transformer, we allow flexible capacity</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># No tokens are dropped - experts can handle more if needed</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> expert_indices  <span style="color:#6a9955;"># Return unchanged</span><br>
<br>
// Switch Transformer Layer<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">SwitchTransformerLayer</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""A transformer layer with Switch routing instead of dense FFN"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, hidden_dim, num_experts, expert_capacity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.router = SwitchRouter(num_experts, hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.capacity_manager = ExpertCapacityManager(num_experts, expert_capacity)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Create multiple expert FFNs</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.experts = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim, hidden_dim * 4),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim * 4, hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;) <span style="color:#569cd6;">for</span> _ <span style="color:#569cd6;">in</span> range(num_experts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Forward pass with expert routing"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size, seq_len, hidden_dim = x.shape<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Route tokens to experts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert_indices = self.router.route(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert_indices = self.capacity_manager.enforce_capacity(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert_indices, batch_size, seq_len)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Process tokens through assigned experts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = torch.zeros_like(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> expert_idx <span style="color:#569cd6;">in</span> range(len(self.experts)):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask = (expert_indices == expert_idx)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">if</span> mask.any():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert_input = x[mask]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert_output = self.experts[expert_idx](expert_input)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs[mask] = expert_output<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> outputs<br>
<br>
// Distributed Training with Expert Parallelism<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">DistributedSwitchTransformer</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Switch Transformer with data, model, and expert parallelism"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, model_config, num_devices, expert_parallelism=True):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model_config = model_config<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_devices = num_devices<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.expert_parallelism = expert_parallelism<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Initialize distributed components</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self._setup_distributed_training()<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">_setup_distributed_training</span>(self):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Setup data, model, and expert parallelism"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">if</span> self.expert_parallelism:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Distribute experts across devices</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;experts_per_device = self.model_config.num_experts // self.num_devices<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Implement efficient all-to-all communication for expert routing</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self._setup_expert_communication()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Standard data and model parallelism</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self._setup_data_parallelism()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self._setup_model_parallelism()<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
