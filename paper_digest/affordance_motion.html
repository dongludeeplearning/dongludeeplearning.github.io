<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ CVPR 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang<br>
        <span style="opacity:0.8">BIGAI, Tsinghua University, Peking University, BIT</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to generate language-guided human motions that are both semantically faithful to descriptions and physically plausible within complex 3D environments?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•ç”Ÿæˆæ—¢è¯­ä¹‰ä¸Šå¿ å®äºæè¿°ã€åˆåœ¨å¤æ‚3Dç¯å¢ƒä¸­ç‰©ç†ä¸Šåˆç†çš„è¯­è¨€å¼•å¯¼äººç±»è¿åŠ¨ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Two-Stage Affordance Framework:</b> Novel two-stage approach using scene affordance as intermediate representation, bridging 3D scene grounding and conditional motion generation for language-guided HSI.</div>
            <div class="lang-zh" style="display:none"><b>ä¸¤é˜¶æ®µå¯åŠæ€§æ¡†æ¶ï¼š</b>æ–°é¢–çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œä½¿ç”¨åœºæ™¯å¯åŠæ€§ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œè¿æ¥3Dåœºæ™¯groundingå’Œæ¡ä»¶è¿åŠ¨ç”Ÿæˆï¼Œç”¨äºè¯­è¨€å¼•å¯¼çš„HSIã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Affordance Diffusion Model (ADM):</b> Perceiver-based diffusion model that predicts explicit affordance maps from 3D scenes and language descriptions, enabling precise interaction region grounding.</div>
            <div class="lang-zh" style="display:none"><b>å¯åŠæ€§æ‰©æ•£æ¨¡å‹(ADM)ï¼š</b>åŸºäºPerceiverçš„æ‰©æ•£æ¨¡å‹ï¼Œä»3Dåœºæ™¯å’Œè¯­è¨€æè¿°é¢„æµ‹æ˜¾å¼çš„å¯åŠæ€§åœ°å›¾ï¼Œå®ç°ç²¾ç¡®çš„äº¤äº’åŒºåŸŸgroundingã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Affordance-to-Motion Diffusion Model (AMDM):</b> Transformer-based conditional diffusion model that synthesizes physically plausible human motions using predicted affordance maps and language inputs.</div>
            <div class="lang-zh" style="display:none"><b>å¯åŠæ€§åˆ°è¿åŠ¨æ‰©æ•£æ¨¡å‹(AMDM)ï¼š</b>åŸºäºTransformerçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨é¢„æµ‹çš„å¯åŠæ€§åœ°å›¾å’Œè¯­è¨€è¾“å…¥åˆæˆç‰©ç†ä¸Šåˆç†çš„äººç±»è¿åŠ¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Superior Performance and Generalization:</b> Consistently outperforms baselines on HumanML3D and HUMANISE benchmarks, with exceptional generalization to unseen language-scene pairs despite limited training data.</div>
            <div class="lang-zh" style="display:none"><b>å“è¶Šæ€§èƒ½å’Œæ³›åŒ–ï¼š</b>åœ¨HumanML3Då’ŒHUMANISEåŸºå‡†ä¸ŠæŒç»­ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå°½ç®¡è®­ç»ƒæ•°æ®æœ‰é™ï¼Œä½†å¯¹æœªè§è¯­è¨€-åœºæ™¯é…å¯¹å…·æœ‰å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Scene Affordance as Intermediate Representation:</b> Demonstrates that affordance maps effectively overcome multimodal conditioning challenges and enable robust motion generation with geometric awareness.</div>
            <div class="lang-zh" style="display:none"><b>åœºæ™¯å¯åŠæ€§ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼š</b>è¯æ˜å¯åŠæ€§åœ°å›¾æœ‰æ•ˆå…‹æœå¤šæ¨¡æ€æ¡ä»¶æŒ‘æˆ˜ï¼Œå¹¶å®ç°å…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„é²æ£’è¿åŠ¨ç”Ÿæˆã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/affordance_motion_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Joint Multimodal Modeling:</b> Lack of powerful generative models that can jointly model natural language, 3D scenes, and human motion for coherent HSI generation.</div>
            <div class="lang-zh" style="display:none"><b>è”åˆå¤šæ¨¡æ€å»ºæ¨¡ï¼š</b>ç¼ºä¹èƒ½å¤Ÿè”åˆå»ºæ¨¡è‡ªç„¶è¯­è¨€ã€3Dåœºæ™¯å’Œäººç±»è¿åŠ¨çš„å¼ºå¤§ç”Ÿæˆæ¨¡å‹ï¼Œä»¥å®ç°è¿è´¯çš„HSIç”Ÿæˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Data Scarcity:</b> Generative models require extensive high-quality paired language-scene-motion datasets, but existing datasets lack comprehensive HSI descriptions and diversity.</div>
            <div class="lang-zh" style="display:none"><b>æ•°æ®ç¨€ç¼ºï¼š</b>ç”Ÿæˆæ¨¡å‹éœ€è¦å¤§é‡çš„ä¼˜è´¨é…å¯¹è¯­è¨€-åœºæ™¯-è¿åŠ¨æ•°æ®é›†ï¼Œä½†ç°æœ‰æ•°æ®é›†ç¼ºä¹å…¨é¢çš„HSIæè¿°å’Œå¤šæ ·æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scene Grounding Complexity:</b> Difficulty in generating motions that are both descriptive-faithful to language and physically grounded in specific 3D scene locations.</div>
            <div class="lang-zh" style="display:none"><b>åœºæ™¯Groundingå¤æ‚æ€§ï¼š</b>éš¾ä»¥ç”Ÿæˆæ—¢å¯¹è¯­è¨€å…·æœ‰æè¿°æ€§å¿ å®ã€åˆåœ¨ç‰¹å®š3Dåœºæ™¯ä½ç½®ç‰©ç†ä¸Šgroundedçš„è¿åŠ¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Geometric Interaction Understanding:</b> Challenge in modeling the sophisticated interplay between human motions and 3D scene geometries for plausible interactions.</div>
            <div class="lang-zh" style="display:none"><b>å‡ ä½•äº¤äº’ç†è§£ï¼š</b>å»ºæ¨¡äººç±»è¿åŠ¨ä¸3Dåœºæ™¯å‡ ä½•ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ä»¥å®ç°åˆç†äº¤äº’çš„æŒ‘æˆ˜ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Generalization to Unseen Scenarios:</b> Models struggle to generalize from limited training data to novel language descriptions and unseen 3D scene configurations.</div>
            <div class="lang-zh" style="display:none"><b>æ³›åŒ–åˆ°æœªè§åœºæ™¯ï¼š</b>æ¨¡å‹éš¾ä»¥ä»æœ‰é™è®­ç»ƒæ•°æ®æ³›åŒ–åˆ°æ–°çš„è¯­è¨€æè¿°å’Œæœªè§çš„3Dåœºæ™¯é…ç½®ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scene Affordance Representation:</b> Defines affordance maps as distance fields between human skeleton joints and 3D scene surface points, providing geometric interaction priors for motion generation.</div>
            <div class="lang-zh" style="display:none"><b>åœºæ™¯å¯åŠæ€§è¡¨ç¤ºï¼š</b>å°†å¯åŠæ€§åœ°å›¾å®šä¹‰ä¸ºäººç±»éª¨éª¼å…³èŠ‚ä¸3Dåœºæ™¯è¡¨é¢ç‚¹ä¹‹é—´çš„è·ç¦»åœºï¼Œä¸ºè¿åŠ¨ç”Ÿæˆæä¾›å‡ ä½•äº¤äº’å…ˆéªŒã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Affordance Diffusion Model (ADM):</b> Perceiver-based conditional diffusion model that predicts language-grounded affordance maps from 3D scenes and textual descriptions.</div>
            <div class="lang-zh" style="display:none"><b>å¯åŠæ€§æ‰©æ•£æ¨¡å‹(ADM)ï¼š</b>åŸºäºPerceiverçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»3Dåœºæ™¯å’Œæ–‡æœ¬æè¿°é¢„æµ‹è¯­è¨€groundedçš„å¯åŠæ€§åœ°å›¾ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Affordance-to-Motion Diffusion Model (AMDM):</b> Transformer-based architecture with affordance encoder that generates human motions conditioned on predicted affordance maps and language inputs.</div>
            <div class="lang-zh" style="display:none"><b>å¯åŠæ€§åˆ°è¿åŠ¨æ‰©æ•£æ¨¡å‹(AMDM)ï¼š</b>å¸¦æœ‰å¯åŠæ€§ç¼–ç å™¨çš„åŸºäºTransformerçš„æ¶æ„ï¼Œåœ¨é¢„æµ‹çš„å¯åŠæ€§åœ°å›¾å’Œè¯­è¨€è¾“å…¥æ¡ä»¶ä¸‹ç”Ÿæˆäººç±»è¿åŠ¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Two-Stage Training Pipeline:</b> Progressive training where ADM learns to predict affordance maps, followed by AMDM learning to generate motions conditioned on these maps.</div>
            <div class="lang-zh" style="display:none"><b>ä¸¤é˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼š</b>æ¸è¿›å¼è®­ç»ƒï¼Œå…¶ä¸­ADMå­¦ä¹ é¢„æµ‹å¯åŠæ€§åœ°å›¾ï¼Œç„¶åAMDMå­¦ä¹ åœ¨è¿™äº›åœ°å›¾æ¡ä»¶ä¸‹ç”Ÿæˆè¿åŠ¨ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Geometric-Aware Motion Synthesis:</b> Leverages affordance maps to ensure generated motions respect physical constraints and geometric relationships in 3D environments.</div>
            <div class="lang-zh" style="display:none"><b>å‡ ä½•æ„ŸçŸ¥è¿åŠ¨åˆæˆï¼š</b>åˆ©ç”¨å¯åŠæ€§åœ°å›¾ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨å°Šé‡3Dç¯å¢ƒä¸­çš„ç‰©ç†çº¦æŸå’Œå‡ ä½•å…³ç³»ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/affordance_motion_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2403.18036.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2403.18036.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Affordance as Geometric Bridge:</b> Scene affordance maps serve as an elegant intermediate representation that bridges the semantic gap between language understanding and geometric motion constraints, enabling robust multimodal conditioning.
            </div>
            <div class="lang-zh" style="display:none">
                <b>å¯åŠæ€§ä½œä¸ºå‡ ä½•æ¡¥æ¢ï¼š</b>åœºæ™¯å¯åŠæ€§åœ°å›¾ä½œä¸ºä¸€ä¸ªä¼˜é›…çš„ä¸­é—´è¡¨ç¤ºï¼Œæ¡¥æ¥äº†è¯­è¨€ç†è§£ä¸å‡ ä½•è¿åŠ¨çº¦æŸä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œå®ç°é²æ£’çš„å¤šæ¨¡æ€æ¡ä»¶ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Data Efficiency through Intermediate Representation:</b> By decomposing the complex multimodal task into affordance prediction and motion generation stages, the approach significantly reduces the data requirements while maintaining strong generalization capabilities.
            </div>
            <div class="lang-zh" style="display:none">
                <b>é€šè¿‡ä¸­é—´è¡¨ç¤ºæé«˜æ•°æ®æ•ˆç‡ï¼š</b>é€šè¿‡å°†å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡åˆ†è§£ä¸ºå¯åŠæ€§é¢„æµ‹å’Œè¿åŠ¨ç”Ÿæˆé˜¶æ®µï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†æ•°æ®éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Physical Plausibility via Geometric Awareness:</b> The affordance-based approach ensures that generated motions respect physical constraints and geometric relationships, leading to more realistic and contextually appropriate human-scene interactions.
            </div>
            <div class="lang-zh" style="display:none">
                <b>é€šè¿‡å‡ ä½•æ„ŸçŸ¥å®ç°ç‰©ç†åˆç†æ€§ï¼š</b>åŸºäºå¯åŠæ€§çš„æ–¹æ³•ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨å°Šé‡ç‰©ç†çº¦æŸå’Œå‡ ä½•å…³ç³»ï¼Œä»è€Œå®ç°æ›´çœŸå®å’Œä¸Šä¸‹æ–‡é€‚å½“çš„äººç±»-åœºæ™¯äº¤äº’ã€‚
            </div>
          </li>
        </ul>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because the affordance representation elegantly decomposes the complex problem of multimodal motion generation. By first predicting where interactions can occur (affordance maps) and then generating motions within those constraints, the model achieves superior physical plausibility and semantic faithfulness. This two-stage approach is particularly effective when training data is limited, as it leverages geometric priors to guide the learning process.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºå¯åŠæ€§è¡¨ç¤ºä¼˜é›…åœ°åˆ†è§£äº†å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆçš„å¤æ‚é—®é¢˜ã€‚é€šè¿‡é¦–å…ˆé¢„æµ‹äº¤äº’å¯ä»¥å‘ç”Ÿçš„ä½ç½®ï¼ˆå¯åŠæ€§åœ°å›¾ï¼‰ï¼Œç„¶ååœ¨è¿™äº›çº¦æŸå†…ç”Ÿæˆè¿åŠ¨ï¼Œè¯¥æ¨¡å‹å®ç°äº†å“è¶Šçš„ç‰©ç†åˆç†æ€§å’Œè¯­ä¹‰å¿ å®æ€§ã€‚å½“è®­ç»ƒæ•°æ®æœ‰é™æ—¶ï¼Œè¿™ç§ä¸¤é˜¶æ®µæ–¹æ³•ç‰¹åˆ«æœ‰æ•ˆï¼Œå› ä¸ºå®ƒåˆ©ç”¨å‡ ä½•å…ˆéªŒæ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            This work introduces a novel two-stage framework that leverages scene affordance as an intermediate representation to address the challenges of language-guided human motion generation in 3D environments. By decomposing the complex multimodal task into affordance prediction and motion synthesis stages, our approach achieves superior performance on established benchmarks while demonstrating remarkable generalization capabilities to unseen scenarios. The Affordance Diffusion Model and Affordance-to-Motion Diffusion Model work synergistically to ensure that generated motions are both semantically faithful to language descriptions and physically plausible within 3D scenes. Experimental results validate the effectiveness of our approach, showing consistent improvements over state-of-the-art methods on HumanML3D and HUMANISE datasets, along with strong generalization to novel language-scene combinations. This work establishes scene affordance as a powerful intermediate representation for multimodal motion generation tasks and opens new avenues for controllable human-scene interaction synthesis.
          </div>
          <div class="lang-zh" style="display:none">
            æœ¬å·¥ä½œå¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨åœºæ™¯å¯åŠæ€§ä½œä¸ºä¸­é—´è¡¨ç¤ºæ¥è§£å†³3Dç¯å¢ƒä¸­è¯­è¨€å¼•å¯¼äººç±»è¿åŠ¨ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡åˆ†è§£ä¸ºå¯åŠæ€§é¢„æµ‹å’Œè¿åŠ¨åˆæˆé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å·²å»ºç«‹çš„åŸºå‡†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶å±•ç¤ºäº†å¯¹æ‰‹æœªè§åœºæ™¯çš„å“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚å¯åŠæ€§æ‰©æ•£æ¨¡å‹å’Œå¯åŠæ€§åˆ°è¿åŠ¨æ‰©æ•£æ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨æ—¢å¯¹è¯­è¨€æè¿°å…·æœ‰è¯­ä¹‰å¿ å®æ€§ï¼Œåˆåœ¨3Dåœºæ™¯ä¸­ç‰©ç†ä¸Šåˆç†ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨HumanML3Då’ŒHUMANISEæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºå¯¹æœ€å…ˆè¿›æ–¹æ³•çš„æŒç»­æ”¹è¿›ï¼Œä»¥åŠå¯¹æ–°é¢–è¯­è¨€-åœºæ™¯ç»„åˆçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå°†åœºæ™¯å¯åŠæ€§ç¡®ç«‹ä¸ºå¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆä»»åŠ¡çš„å¼ºå¤§ä¸­é—´è¡¨ç¤ºï¼Œå¹¶ä¸ºå¯æ§äººç±»-åœºæ™¯äº¤äº’åˆæˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="font-family:ui-monospace,monospace;font-size:13px;color:rgba(232,236,255,.8);background:rgba(0,0,0,.3);padding:16px;border-radius:8px;border:1px solid rgba(255,255,255,.05);">
          <div class="lang-en">
            <pre><code>import torch
import torch.nn as nn
from diffusers import DDPMScheduler, UNet2DConditionModel

class AffordanceMotion(nn.Module):
    """Two-Stage Affordance-Based Motion Generation"""

    def __init__(self, scene_dim=6, motion_dim=135, affordance_dim=24):
        super().__init__()

        # Stage 1: Affordance Diffusion Model (ADM)
        self.affordance_encoder = AffordanceEncoder(scene_dim, affordance_dim)
        self.affordance_diffusion = AffordanceDiffusionModel()

        # Stage 2: Affordance-to-Motion Diffusion Model (AMDM)
        self.affordance_conditioner = AffordanceConditioner(affordance_dim)
        self.motion_diffusion = MotionDiffusionModel(motion_dim)

    def forward(self, scene, text, motion_length=60):
        """
        Two-stage motion generation pipeline

        Args:
            scene: 3D scene point cloud [B, N, 6] (x,y,z,r,g,b)
            text: language description
            motion_length: target motion sequence length

        Returns:
            motion: generated human motion [B, T, 135]
        """

        # Stage 1: Predict affordance map
        affordance_map = self.predict_affordance(scene, text, motion_length)

        # Stage 2: Generate motion conditioned on affordance
        motion = self.generate_motion(affordance_map, text, motion_length)

        return motion

    def predict_affordance(self, scene, text, seq_length):
        """Stage 1: Generate language-grounded affordance map"""
        # Encode scene and text
        scene_features = self.affordance_encoder.scene_encoder(scene)
        text_features = self.affordance_encoder.text_encoder(text)

        # Start from Gaussian noise
        noise = torch.randn(scene.size(0), seq_length, self.affordance_encoder.affordance_dim)

        # Denoising process with text conditioning
        affordance_map = self.affordance_diffusion.denoise(
            noise, scene_features, text_features, num_steps=50
        )

        return affordance_map

    def generate_motion(self, affordance_map, text, seq_length):
        """Stage 2: Generate motion conditioned on affordance"""
        # Condition motion generation on affordance
        affordance_condition = self.affordance_conditioner(affordance_map)

        # Start from Gaussian noise
        noise = torch.randn(affordance_map.size(0), seq_length, 135)

        # Denoising with affordance and text conditioning
        motion = self.motion_diffusion.denoise(
            noise, affordance_condition, text, num_steps=50
        )

        return motion

class AffordanceEncoder(nn.Module):
    """Encodes scene and text into affordance representation"""

    def __init__(self, scene_dim=6, affordance_dim=24):
        super().__init__()
        self.affordance_dim = affordance_dim

        # Scene encoder (PointNet-like)
        self.scene_encoder = nn.Sequential(
            nn.Linear(scene_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(1),
            nn.Flatten(),
            nn.Linear(256, 512)
        )

        # Text encoder (CLIP-like)
        self.text_encoder = nn.Sequential(
            nn.Embedding(49408, 512),  # CLIP vocabulary
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(512, 8, batch_first=True),
                num_layers=6
            ),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten()
        )

    def forward(self, scene, text):
        scene_feat = self.scene_encoder(scene.transpose(1, 2))
        text_feat = self.text_encoder(text)
        return scene_feat, text_feat

class AffordanceDiffusionModel(nn.Module):
    """Diffusion model for affordance map generation"""

    def __init__(self, affordance_dim=24):
        super().__init__()
        self.affordance_dim = affordance_dim

        # Perceiver-based architecture for point cloud processing
        self.latent_dim = 256
        self.num_latents = 64

        # Learnable latent queries
        self.latents = nn.Parameter(torch.randn(self.num_latents, self.latent_dim))

        # Cross-attention layers
        self.cross_attn = nn.MultiheadAttention(self.latent_dim, 8, batch_first=True)
        self.self_attn = nn.MultiheadAttention(self.latent_dim, 8, batch_first=True)

        # Diffusion timestep embedding
        self.time_embed = nn.Sequential(
            nn.Linear(1, self.latent_dim),
            nn.ReLU(),
            nn.Linear(self.latent_dim, self.latent_dim)
        )

        # Denoising network
        self.denoise_net = UNet2DConditionModel(
            sample_size=32,
            in_channels=affordance_dim,
            out_channels=affordance_dim,
            layers_per_block=2,
            block_out_channels=(128, 256, 512),
            down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D"),
            up_block_types=("UpBlock2D", "UpBlock2D", "UpBlock2D"),
        )

    def denoise(self, noisy_affordance, scene_features, text_features, num_steps=50):
        """Denoising process for affordance generation"""
        scheduler = DDPMScheduler(num_train_timesteps=1000)

        # Process scene and text through Perceiver
        scene_latents = self.process_scene(scene_features)
        text_latents = self.process_text(text_features)

        # Concatenate conditions
        conditions = torch.cat([scene_latents, text_latents], dim=1)

        current_sample = noisy_affordance

        for t in reversed(range(num_steps)):
            timestep = torch.tensor([t / num_steps]).to(current_sample.device)

            # Predict noise
            noise_pred = self.denoise_net(
                current_sample.unsqueeze(-1).unsqueeze(-1),  # Add spatial dims
                timestep,
                encoder_hidden_states=conditions
            ).sample.squeeze(-1).squeeze(-1)

            # Denoising step
            current_sample = scheduler.step(noise_pred, t, current_sample).prev_sample

        return current_sample

    def process_scene(self, scene_features):
        """Process scene features through Perceiver"""
        batch_size = scene_features.size(0)
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # Cross-attention with scene
        attended, _ = self.cross_attn(latents, scene_features, scene_features)
        attended, _ = self.self_attn(attended, attended, attended)

        return attended

    def process_text(self, text_features):
        """Process text features through Perceiver"""
        batch_size = text_features.size(0)
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # Cross-attention with text
        attended, _ = self.cross_attn(latents, text_features, text_features)
        attended, _ = self.self_attn(attended, attended, attended)

        return attended

class MotionDiffusionModel(nn.Module):
    """Diffusion model for motion generation conditioned on affordance"""

    def __init__(self, motion_dim=135):
        super().__init__()

        # Affordance conditioning
        self.affordance_proj = nn.Sequential(
            nn.Linear(24, 256),
            nn.ReLU(),
            nn.Linear(256, motion_dim)
        )

        # Motion diffusion network
        self.motion_net = UNet2DConditionModel(
            sample_size=32,
            in_channels=motion_dim,
            out_channels=motion_dim,
            layers_per_block=2,
            block_out_channels=(128, 256, 512),
            down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D"),
            up_block_types=("UpBlock2D", "UpBlock2D", "UpBlock2D"),
        )

        # Text conditioning
        self.text_encoder = nn.Sequential(
            nn.Embedding(49408, 512),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(512, motion_dim)
        )

    def denoise(self, noisy_motion, affordance_condition, text, num_steps=50):
        """Denoising process for motion generation"""
        scheduler = DDPMScheduler(num_train_timesteps=1000)

        # Process conditions
        affordance_cond = self.affordance_proj(affordance_condition)
        text_cond = self.text_encoder(text)

        # Combine conditions
        combined_condition = affordance_cond + text_cond

        current_sample = noisy_motion

        for t in reversed(range(num_steps)):
            timestep = torch.tensor([t / num_steps]).to(current_sample.device)

            # Predict noise with conditioning
            noise_pred = self.motion_net(
                current_sample.unsqueeze(-1).unsqueeze(-1),
                timestep,
                encoder_hidden_states=combined_condition.unsqueeze(1)
            ).sample.squeeze(-1).squeeze(-1)

            # Denoising step
            current_sample = scheduler.step(noise_pred, t, current_sample).prev_sample

        return current_sample

# Training and inference
def train_affordance_motion(model, train_dataset):
    """Training pipeline for the two-stage model"""

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    scheduler = DDPMScheduler(num_train_timesteps=1000)

    for epoch in range(100):
        for batch in train_dataset:
            scene, text, gt_motion = batch['scene'], batch['text'], batch['motion']

            # Compute affordance map from ground truth motion
            affordance_map = compute_affordance_from_motion(scene, gt_motion)

            # Stage 1: Train affordance prediction
            pred_affordance = model.predict_affordance(scene, text, gt_motion.size(1))
            affordance_loss = F.mse_loss(pred_affordance, affordance_map)

            # Stage 2: Train motion generation
            pred_motion = model.generate_motion(affordance_map, text, gt_motion.size(1))
            motion_loss = F.mse_loss(pred_motion, gt_motion)

            # Combined loss
            total_loss = affordance_loss + motion_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

    return model

def compute_affordance_from_motion(scene, motion):
    """Compute affordance map from ground truth motion and scene"""
    # Extract joint positions from motion
    joint_positions = motion.view(motion.size(0), motion.size(1), -1, 3)  # [B, T, J, 3]

    # Compute distance field between joints and scene points
    scene_points = scene[:, :, :3]  # [B, N, 3]

    distances = []
    for t in range(motion.size(1)):
        joint_pos_t = joint_positions[:, t]  # [B, J, 3]
        dist_t = torch.cdist(joint_pos_t, scene_points)  # [B, J, N]
        distances.append(dist_t)

    # Stack and compute affordance
    distance_field = torch.stack(distances, dim=1)  # [B, T, J, N]

    # Normalize to affordance map
    affordance_map = torch.exp(-distance_field / 0.5)  # Gaussian decay
    affordance_map = affordance_map.mean(dim=-1)  # Average over scene points

    return affordance_map

# Usage example
if __name__ == "__main__":
    model = AffordanceMotion()

    # Training
    model = train_affordance_motion(model, train_dataset)

    # Inference
    scene = load_3d_scene("living_room.ply")
    text = "A person walks to the chair and sits down"

    generated_motion = model(scene, text)
    print(f"Generated motion: {generated_motion.shape}")  # [B, T, 135]</code></pre>
          </div>
          <div class="lang-zh" style="display:none">
            <pre><code>import torch
import torch.nn as nn
from diffusers import DDPMScheduler, UNet2DConditionModel

class AffordanceMotion(nn.Module):
    """ä¸¤é˜¶æ®µåŸºäºå¯åŠæ€§çš„è¿åŠ¨ç”Ÿæˆ"""

    def __init__(self, scene_dim=6, motion_dim=135, affordance_dim=24):
        super().__init__()

        # ç¬¬ä¸€é˜¶æ®µï¼šå¯åŠæ€§æ‰©æ•£æ¨¡å‹(ADM)
        self.affordance_encoder = AffordanceEncoder(scene_dim, affordance_dim)
        self.affordance_diffusion = AffordanceDiffusionModel()

        # ç¬¬äºŒé˜¶æ®µï¼šå¯åŠæ€§åˆ°è¿åŠ¨æ‰©æ•£æ¨¡å‹(AMDM)
        self.affordance_conditioner = AffordanceConditioner(affordance_dim)
        self.motion_diffusion = MotionDiffusionModel(motion_dim)

    def forward(self, scene, text, motion_length=60):
        """
        ä¸¤é˜¶æ®µè¿åŠ¨ç”Ÿæˆæµæ°´çº¿

        Args:
            scene: 3Dåœºæ™¯ç‚¹äº‘ [B, N, 6] (x,y,z,r,g,b)
            text: è¯­è¨€æè¿°
            motion_length: ç›®æ ‡è¿åŠ¨åºåˆ—é•¿åº¦

        Returns:
            motion: ç”Ÿæˆçš„äººç±»è¿åŠ¨ [B, T, 135]
        """

        # ç¬¬ä¸€é˜¶æ®µï¼šé¢„æµ‹å¯åŠæ€§åœ°å›¾
        affordance_map = self.predict_affordance(scene, text, motion_length)

        # ç¬¬äºŒé˜¶æ®µï¼šåœ¨å¯åŠæ€§æ¡ä»¶ä¸‹ç”Ÿæˆè¿åŠ¨
        motion = self.generate_motion(affordance_map, text, motion_length)

        return motion

    def predict_affordance(self, scene, text, seq_length):
        """ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆè¯­è¨€groundedçš„å¯åŠæ€§åœ°å›¾"""
        # ç¼–ç åœºæ™¯å’Œæ–‡æœ¬
        scene_features = self.affordance_encoder.scene_encoder(scene)
        text_features = self.affordance_encoder.text_encoder(text)

        # ä»é«˜æ–¯å™ªå£°å¼€å§‹
        noise = torch.randn(scene.size(0), seq_length, self.affordance_encoder.affordance_dim)

        # å¸¦æ–‡æœ¬æ¡ä»¶çš„å»å™ªè¿‡ç¨‹
        affordance_map = self.affordance_diffusion.denoise(
            noise, scene_features, text_features, num_steps=50
        )

        return affordance_map

    def generate_motion(self, affordance_map, text, seq_length):
        """ç¬¬äºŒé˜¶æ®µï¼šåœ¨å¯åŠæ€§æ¡ä»¶ä¸‹ç”Ÿæˆè¿åŠ¨"""
        # åŸºäºå¯åŠæ€§è¿›è¡Œè¿åŠ¨ç”Ÿæˆæ¡ä»¶åŒ–
        affordance_condition = self.affordance_conditioner(affordance_map)

        # ä»é«˜æ–¯å™ªå£°å¼€å§‹
        noise = torch.randn(affordance_map.size(0), seq_length, 135)

        # å¸¦å¯åŠæ€§å’Œæ–‡æœ¬æ¡ä»¶çš„å»å™ª
        motion = self.motion_diffusion.denoise(
            noise, affordance_condition, text, num_steps=50
        )

        return motion

class AffordanceEncoder(nn.Module):
    """å°†åœºæ™¯å’Œæ–‡æœ¬ç¼–ç ä¸ºå¯åŠæ€§è¡¨ç¤º"""

    def __init__(self, scene_dim=6, affordance_dim=24):
        super().__init__()
        self.affordance_dim = affordance_dim

        # åœºæ™¯ç¼–ç å™¨(PointNet-like)
        self.scene_encoder = nn.Sequential(
            nn.Linear(scene_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(1),
            nn.Flatten(),
            nn.Linear(256, 512)
        )

        # æ–‡æœ¬ç¼–ç å™¨(CLIP-like)
        self.text_encoder = nn.Sequential(
            nn.Embedding(49408, 512),  # CLIPè¯æ±‡è¡¨
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(512, 8, batch_first=True),
                num_layers=6
            ),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten()
        )

    def forward(self, scene, text):
        scene_feat = self.scene_encoder(scene.transpose(1, 2))
        text_feat = self.text_encoder(text)
        return scene_feat, text_feat

class AffordanceDiffusionModel(nn.Module):
    """å¯åŠæ€§åœ°å›¾ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹"""

    def __init__(self, affordance_dim=24):
        super().__init__()
        self.affordance_dim = affordance_dim

        # åŸºäºPerceiverçš„æ¶æ„å¤„ç†ç‚¹äº‘
        self.latent_dim = 256
        self.num_latents = 64

        # å¯å­¦ä¹ çš„æ½œåœ¨æŸ¥è¯¢
        self.latents = nn.Parameter(torch.randn(self.num_latents, self.latent_dim))

        # äº¤å‰æ³¨æ„åŠ›å±‚
        self.cross_attn = nn.MultiheadAttention(self.latent_dim, 8, batch_first=True)
        self.self_attn = nn.MultiheadAttention(self.latent_dim, 8, batch_first=True)

        # æ‰©æ•£æ—¶é—´æ­¥åµŒå…¥
        self.time_embed = nn.Sequential(
            nn.Linear(1, self.latent_dim),
            nn.ReLU(),
            nn.Linear(self.latent_dim, self.latent_dim)
        )

        # å»å™ªç½‘ç»œ
        self.denoise_net = UNet2DConditionModel(
            sample_size=32,
            in_channels=affordance_dim,
            out_channels=affordance_dim,
            layers_per_block=2,
            block_out_channels=(128, 256, 512),
            down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D"),
            up_block_types=("UpBlock2D", "UpBlock2D", "UpBlock2D"),
        )

    def denoise(self, noisy_affordance, scene_features, text_features, num_steps=50):
        """å¯åŠæ€§ç”Ÿæˆçš„å»å™ªè¿‡ç¨‹"""
        scheduler = DDPMScheduler(num_train_timesteps=1000)

        # é€šè¿‡Perceiverå¤„ç†åœºæ™¯å’Œæ–‡æœ¬
        scene_latents = self.process_scene(scene_features)
        text_latents = self.process_text(text_features)

        # è¿æ¥æ¡ä»¶
        conditions = torch.cat([scene_latents, text_latents], dim=1)

        current_sample = noisy_affordance

        for t in reversed(range(num_steps)):
            timestep = torch.tensor([t / num_steps]).to(current_sample.device)

            # é¢„æµ‹å™ªå£°
            noise_pred = self.denoise_net(
                current_sample.unsqueeze(-1).unsqueeze(-1),  # æ·»åŠ ç©ºé—´ç»´åº¦
                timestep,
                encoder_hidden_states=conditions
            ).sample.squeeze(-1).squeeze(-1)

            # å»å™ªæ­¥éª¤
            current_sample = scheduler.step(noise_pred, t, current_sample).prev_sample

        return current_sample

    def process_scene(self, scene_features):
        """é€šè¿‡Perceiverå¤„ç†åœºæ™¯ç‰¹å¾"""
        batch_size = scene_features.size(0)
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # ä¸åœºæ™¯äº¤å‰æ³¨æ„åŠ›
        attended, _ = self.cross_attn(latents, scene_features, scene_features)
        attended, _ = self.self_attn(attended, attended, attended)

        return attended

    def process_text(self, text_features):
        """é€šè¿‡Perceiverå¤„ç†æ–‡æœ¬ç‰¹å¾"""
        batch_size = text_features.size(0)
        latents = self.latents.unsqueeze(0).repeat(batch_size, 1, 1)

        # ä¸æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›
        attended, _ = self.cross_attn(latents, text_features, text_features)
        attended, _ = self.self_attn(attended, attended, attended)

        return attended

class MotionDiffusionModel(nn.Module):
    """åŸºäºå¯åŠæ€§æ¡ä»¶çš„è¿åŠ¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹"""

    def __init__(self, motion_dim=135):
        super().__init__()

        # å¯åŠæ€§æ¡ä»¶åŒ–
        self.affordance_proj = nn.Sequential(
            nn.Linear(24, 256),
            nn.ReLU(),
            nn.Linear(256, motion_dim)
        )

        # è¿åŠ¨æ‰©æ•£ç½‘ç»œ
        self.motion_net = UNet2DConditionModel(
            sample_size=32,
            in_channels=motion_dim,
            out_channels=motion_dim,
            layers_per_block=2,
            block_out_channels=(128, 256, 512),
            down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D"),
            up_block_types=("UpBlock2D", "UpBlock2D", "UpBlock2D"),
        )

        # æ–‡æœ¬æ¡ä»¶åŒ–
        self.text_encoder = nn.Sequential(
            nn.Embedding(49408, 512),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(512, motion_dim)
        )

    def denoise(self, noisy_motion, affordance_condition, text, num_steps=50):
        """è¿åŠ¨ç”Ÿæˆçš„å»å™ªè¿‡ç¨‹"""
        scheduler = DDPMScheduler(num_train_timesteps=1000)

        # å¤„ç†æ¡ä»¶
        affordance_cond = self.affordance_proj(affordance_condition)
        text_cond = self.text_encoder(text)

        # ç»„åˆæ¡ä»¶
        combined_condition = affordance_cond + text_cond

        current_sample = noisy_motion

        for t in reversed(range(num_steps)):
            timestep = torch.tensor([t / num_steps]).to(current_sample.device)

            # å¸¦æ¡ä»¶é¢„æµ‹å™ªå£°
            noise_pred = self.motion_net(
                current_sample.unsqueeze(-1).unsqueeze(-1),
                timestep,
                encoder_hidden_states=combined_condition.unsqueeze(1)
            ).sample.squeeze(-1).squeeze(-1)

            # å»å™ªæ­¥éª¤
            current_sample = scheduler.step(noise_pred, t, current_sample).prev_sample

        return current_sample

# è®­ç»ƒå’Œæ¨ç†
def train_affordance_motion(model, train_dataset):
    """ä¸¤é˜¶æ®µæ¨¡å‹çš„è®­ç»ƒæµæ°´çº¿"""

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    scheduler = DDPMScheduler(num_train_timesteps=1000)

    for epoch in range(100):
        for batch in train_dataset:
            scene, text, gt_motion = batch['scene'], batch['text'], batch['motion']

            # ä»çœŸå®è¿åŠ¨è®¡ç®—å¯åŠæ€§åœ°å›¾
            affordance_map = compute_affordance_from_motion(scene, gt_motion)

            # ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒå¯åŠæ€§é¢„æµ‹
            pred_affordance = model.predict_affordance(scene, text, gt_motion.size(1))
            affordance_loss = F.mse_loss(pred_affordance, affordance_map)

            # ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒè¿åŠ¨ç”Ÿæˆ
            pred_motion = model.generate_motion(affordance_map, text, gt_motion.size(1))
            motion_loss = F.mse_loss(pred_motion, gt_motion)

            # ç»„åˆæŸå¤±
            total_loss = affordance_loss + motion_loss

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

    return model

def compute_affordance_from_motion(scene, motion):
    """ä»çœŸå®è¿åŠ¨å’Œåœºæ™¯è®¡ç®—å¯åŠæ€§åœ°å›¾"""
    # ä»è¿åŠ¨ä¸­æå–å…³èŠ‚ä½ç½®
    joint_positions = motion.view(motion.size(0), motion.size(1), -1, 3)  # [B, T, J, 3]

    # è®¡ç®—å…³èŠ‚ä¸åœºæ™¯ç‚¹ä¹‹é—´çš„è·ç¦»åœº
    scene_points = scene[:, :, :3]  # [B, N, 3]

    distances = []
    for t in range(motion.size(1)):
        joint_pos_t = joint_positions[:, t]  # [B, J, 3]
        dist_t = torch.cdist(joint_pos_t, scene_points)  # [B, J, N]
        distances.append(dist_t)

    # å †å å¹¶è®¡ç®—å¯åŠæ€§
    distance_field = torch.stack(distances, dim=1)  # [B, T, J, N]

    # å½’ä¸€åŒ–ä¸ºå¯åŠæ€§åœ°å›¾
    affordance_map = torch.exp(-distance_field / 0.5)  # é«˜æ–¯è¡°å‡
    affordance_map = affordance_map.mean(dim=-1)  # å¯¹åœºæ™¯ç‚¹æ±‚å¹³å‡

    return affordance_map

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = AffordanceMotion()

    # è®­ç»ƒ
    model = train_affordance_motion(model, train_dataset)

    # æ¨ç†
    scene = load_3d_scene("living_room.ply")
    text = "ä¸€ä¸ªäººèµ°åˆ°æ¤…å­æ—è¾¹åä¸‹æ¥"

    generated_motion = model(scene, text)
    print(f"ç”Ÿæˆäº†è¿åŠ¨: {generated_motion.shape}")  # [B, T, 135]</code></pre>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://afford-motion.github.io" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Project Website
        </a>
      </div>
    </div>
</section>
</body>
</html>
