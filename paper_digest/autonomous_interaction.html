<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ SIGGRAPH Asia 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Lingo: Autonomous Character-Scene Interaction Synthesis from Text Instruction</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Nan Jiang*, Zimo He*, Zi Wang, Hongjie Li, Yixin Chen, Siyuan Huangâ€ , Yixin Zhuâ€ <br>
        <span style="opacity:0.8">Institute for AI, Peking University & National Key Lab of General AI, BIGAI</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en"> 
            <b>One-Sentence Problem:</b> How to autonomously synthesize multi-stage character-scene interactions directly from a single text instruction and goal location without manual waypoint specification?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•ä»å•ä¸ªæ–‡æœ¬æŒ‡ä»¤å’Œç›®æ ‡ä½ç½®ç›´æ¥è‡ªä¸»åˆæˆå¤šé˜¶æ®µè§’è‰²-åœºæ™¯äº¤äº’ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ç‚¹ï¼Ÿ
        </div>
      </div>
    </div>
  
    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Autonomous Synthesis Framework:</b> A comprehensive framework that synthesizes multi-stage scene-aware interaction motions directly from single text instruction and goal location, eliminating the need for manual waypoint specification.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªä¸»åˆæˆæ¡†æ¶ï¼š</b>ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œç›´æ¥ä»å•ä¸ªæ–‡æœ¬æŒ‡ä»¤å’Œç›®æ ‡ä½ç½®åˆæˆå¤šé˜¶æ®µåœºæ™¯æ„ŸçŸ¥äº¤äº’è¿åŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ç‚¹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Autonomous Scheduler:</b> An autoregressive diffusion model combined with an autonomous scheduler that predicts transitions for each action stage, enabling seamless multi-stage motion generation.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªä¸»è°ƒåº¦å™¨ï¼š</b>è‡ªå›å½’æ‰©æ•£æ¨¡å‹ç»“åˆè‡ªä¸»è°ƒåº¦å™¨ï¼Œå¯ä»¥é¢„æµ‹æ¯ä¸ªåŠ¨ä½œé˜¶æ®µçš„è½¬æ¢ï¼Œå®ç°æ— ç¼çš„å¤šé˜¶æ®µè¿åŠ¨ç”Ÿæˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Local Scene Perception:</b> A scene representation that considers local perception at both start and goal locations, ensuring synthesized motions are seamlessly integrated within complex 3D environments.</div>
            <div class="lang-zh" style="display:none"><b>å±€éƒ¨åœºæ™¯æ„ŸçŸ¥ï¼š</b>ä¸€ç§åœºæ™¯è¡¨ç¤ºï¼ŒåŒæ—¶è€ƒè™‘èµ·å§‹å’Œç›®æ ‡ä½ç½®çš„å±€éƒ¨æ„ŸçŸ¥ï¼Œç¡®ä¿åˆæˆçš„è¿åŠ¨åœ¨å¤æ‚3Dç¯å¢ƒä¸­æ— ç¼é›†æˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Language-Motion Integration:</b> Enhanced motion coherence by integrating frame embeddings with language input, providing better alignment between textual descriptions and generated motions.</div>
            <div class="lang-zh" style="display:none"><b>è¯­è¨€-è¿åŠ¨é›†æˆï¼š</b>é€šè¿‡å°†å¸§åµŒå…¥ä¸è¯­è¨€è¾“å…¥é›†æˆæ¥å¢å¼ºè¿åŠ¨è¿è´¯æ€§ï¼Œæä¾›æ–‡æœ¬æè¿°ä¸ç”Ÿæˆè¿åŠ¨ä¹‹é—´æ›´å¥½çš„å¯¹é½ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Comprehensive Dataset:</b> A 16-hour motion-captured dataset comprising motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions.</div>
            <div class="lang-zh" style="display:none"><b>ç»¼åˆæ•°æ®é›†ï¼š</b>ä¸€ä¸ª16å°æ—¶çš„è¿åŠ¨æ•æ‰æ•°æ®é›†ï¼ŒåŒ…å«120ä¸ªå®¤å†…åœºæ™¯ä¸­çš„è¿åŠ¨åºåˆ—ï¼Œæ¶µç›–40ç§è¿åŠ¨ç±»å‹ï¼Œæ¯ç§éƒ½æ ‡æ³¨äº†ç²¾ç¡®çš„è¯­è¨€æè¿°ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/autonomous_interaction_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Stage Coordination:</b> Automatically determining stage transitions and waypoints from high-level text instructions without manual intervention.</div>
            <div class="lang-zh" style="display:none"><b>å¤šé˜¶æ®µåè°ƒï¼š</b>ä»é«˜å±‚æ–‡æœ¬æŒ‡ä»¤è‡ªåŠ¨ç¡®å®šé˜¶æ®µè½¬æ¢å’Œè·¯å¾„ç‚¹ï¼Œè€Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scene-Aware Synthesis:</b> Ensuring generated motions respect environmental constraints and maintain physical plausibility across complex 3D scenes.</div>
            <div class="lang-zh" style="display:none"><b>åœºæ™¯æ„ŸçŸ¥åˆæˆï¼š</b>ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨å°Šé‡ç¯å¢ƒçº¦æŸå¹¶åœ¨å¤æ‚3Dåœºæ™¯ä¸­ä¿æŒç‰©ç†åˆç†æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Language-Motion Alignment:</b> Bridging the gap between natural language descriptions and precise motion sequences with proper temporal and semantic coherence.</div>
            <div class="lang-zh" style="display:none"><b>è¯­è¨€-è¿åŠ¨å¯¹é½ï¼š</b>å¼¥åˆè‡ªç„¶è¯­è¨€æè¿°ä¸ç²¾ç¡®è¿åŠ¨åºåˆ—ä¹‹é—´çš„å·®è·ï¼Œå¹¶ä¿æŒé€‚å½“çš„æ—¶é—´å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Data Annotation Complexity:</b> Creating comprehensive datasets with precise language descriptions for diverse motion types and scene configurations.</div>
            <div class="lang-zh" style="display:none"><b>æ•°æ®æ ‡æ³¨å¤æ‚æ€§ï¼š</b>ä¸ºå¤šæ ·åŒ–çš„è¿åŠ¨ç±»å‹å’Œåœºæ™¯é…ç½®åˆ›å»ºå…·æœ‰ç²¾ç¡®è¯­è¨€æè¿°çš„ç»¼åˆæ•°æ®é›†ã€‚</div>
          </li>
        </ul>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Autoregressive Diffusion Model:</b> Employs a diffusion model to synthesize the next motion segment autoregressively, conditioned on current state and language instruction.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼š</b>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è‡ªå›å½’åœ°åˆæˆä¸‹ä¸€ä¸ªè¿åŠ¨ç‰‡æ®µï¼Œä»¥å½“å‰çŠ¶æ€å’Œè¯­è¨€æŒ‡ä»¤ä¸ºæ¡ä»¶ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Autonomous Stage Scheduler:</b> Predicts transitions between action stages automatically, determining when to switch from locomotion to reaching or interaction phases.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªä¸»é˜¶æ®µè°ƒåº¦å™¨ï¼š</b>è‡ªåŠ¨é¢„æµ‹åŠ¨ä½œé˜¶æ®µä¹‹é—´çš„è½¬æ¢ï¼Œç¡®å®šä½•æ—¶ä»è¿åŠ¨åˆ‡æ¢åˆ°ä¼¸æ‰‹æˆ–äº¤äº’é˜¶æ®µã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Local Scene Representation:</b> Constructs scene representations considering local perception at both start and goal positions, enabling environment-aware motion planning.</div>
            <div class="lang-zh" style="display:none"><b>å±€éƒ¨åœºæ™¯è¡¨ç¤ºï¼š</b>æ„å»ºè€ƒè™‘èµ·å§‹å’Œç›®æ ‡ä½ç½®å±€éƒ¨æ„ŸçŸ¥çš„åœºæ™¯è¡¨ç¤ºï¼Œå®ç°ç¯å¢ƒæ„ŸçŸ¥çš„è¿åŠ¨è§„åˆ’ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Language-Frame Integration:</b> Combines frame-level embeddings with language input through cross-modal attention to enhance temporal coherence and semantic alignment.</div>
            <div class="lang-zh" style="display:none"><b>è¯­è¨€-å¸§é›†æˆï¼š</b>é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å°†å¸§çº§åµŒå…¥ä¸è¯­è¨€è¾“å…¥ç›¸ç»“åˆï¼Œä»¥å¢å¼ºæ—¶é—´è¿è´¯æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚</div>
          </li>
        </ol>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/autonomous_interaction_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            
            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2410.03187.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2410.03187.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                The framework successfully automates multi-stage character-scene interaction synthesis from simple text instructions, eliminating the need for manual waypoint specification and stage transitions.
            </div>
            <div class="lang-zh" style="display:none">
                è¯¥æ¡†æ¶æˆåŠŸå®ç°äº†ä»ç®€å•æ–‡æœ¬æŒ‡ä»¤è‡ªä¸»åˆæˆå¤šé˜¶æ®µè§’è‰²-åœºæ™¯äº¤äº’ï¼Œæ¶ˆé™¤äº†æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ç‚¹å’Œé˜¶æ®µè½¬æ¢çš„éœ€æ±‚ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                Local scene perception and language-motion integration ensure that generated motions are both physically plausible and semantically aligned with textual descriptions.
            </div>
            <div class="lang-zh" style="display:none">
                å±€éƒ¨åœºæ™¯æ„ŸçŸ¥å’Œè¯­è¨€-è¿åŠ¨é›†æˆç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨æ—¢ç‰©ç†åˆç†åˆä¸æ–‡æœ¬æè¿°è¯­ä¹‰å¯¹é½ã€‚
            </div>
          </li>
          <li>
            <div class="lang-en">
                The comprehensive dataset and autonomous scheduler enable robust generalization across diverse motion types and complex 3D environments, advancing the field of interactive character animation.
            </div>
            <div class="lang-zh" style="display:none">
                ç»¼åˆæ•°æ®é›†å’Œè‡ªä¸»è°ƒåº¦å™¨èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„è¿åŠ¨ç±»å‹å’Œå¤æ‚3Dç¯å¢ƒä¸­å®ç°é²æ£’æ³›åŒ–ï¼Œæ¨åŠ¨äº¤äº’å¼è§’è‰²åŠ¨ç”»é¢†åŸŸçš„å‘å±•ã€‚
            </div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="font-family:ui-monospace,monospace;font-size:13px;color:rgba(232,236,255,.8);background:rgba(0,0,0,.3);padding:16px;border-radius:8px;border:1px solid rgba(255,255,255,.05);">
          <div class="lang-en">
            <pre><code># Lingo: Autonomous Character-Scene Interaction Synthesis
import torch
import torch.nn as nn
from diffusion_model import AutoregressiveDiffusion
from scene_encoder import LocalSceneEncoder
from scheduler import AutonomousScheduler

class Lingo(nn.Module):
    def __init__(self, motion_dim=135, scene_dim=512, text_dim=768):
        super().__init__()
        self.diffusion = AutoregressiveDiffusion(motion_dim)
        self.scene_encoder = LocalSceneEncoder()  # Local perception at start/goal
        self.text_encoder = nn.TransformerEncoder(text_dim, 6)  # CLIP or similar
        self.scheduler = AutonomousScheduler()  # Predicts stage transitions
        self.cross_modal_attn = nn.MultiheadAttention(motion_dim, 8)  # Language-motion fusion

    def forward(self, text, goal_pos, current_motion, scene_start, scene_goal):
        # Encode inputs
        text_emb = self.text_encoder(text)  # [B, seq_len, text_dim]
        scene_emb = self.scene_encoder(scene_start, scene_goal)  # [B, scene_dim]

        # Autoregressive motion synthesis
        motion_pred = self.diffusion(current_motion, text_emb, scene_emb)

        # Language-motion integration via attention
        motion_enhanced, _ = self.cross_modal_attn(
            motion_pred.transpose(0,1),  # [seq_len, B, motion_dim]
            text_emb.transpose(0,1),     # [text_len, B, text_dim]
            text_emb.transpose(0,1)
        )

        return motion_enhanced.transpose(0,1)

    def autonomous_synthesis(self, text, start_pos, goal_pos, scene):
        """Generate multi-stage interaction from text instruction"""
        trajectory = [start_pos]
        motion_sequence = []

        while not self.at_goal(trajectory[-1], goal_pos):
            # Current stage assessment
            current_stage = self.scheduler.predict_stage(text, trajectory, goal_pos)

            # Local scene perception for current context
            scene_local = self.scene_encoder.extract_local(scene, trajectory[-1], goal_pos)

            # Generate next motion segment
            current_motion = self.get_current_motion_state()
            next_motion = self(text, goal_pos, current_motion, scene_local, scene_local)

            # Execute and update trajectory
            trajectory.append(self.execute_motion(next_motion))
            motion_sequence.append(next_motion)

            # Check for stage transition
            if self.scheduler.should_transition(text, trajectory, goal_pos):
                continue  # Continue to next stage

        return motion_sequence

    def at_goal(self, current_pos, goal_pos, threshold=0.1):
        return torch.norm(current_pos - goal_pos) < threshold

    def get_current_motion_state(self):
        # Return current character motion state (pose, velocity, etc.)
        return torch.randn(1, 135)  # Placeholder

    def execute_motion(self, motion):
        # Simulate motion execution and return new position
        return torch.randn(3)  # Placeholder

# Autonomous Stage Scheduler
class AutonomousScheduler(nn.Module):
    def __init__(self):
        super().__init__()
        self.stage_predictor = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 4)  # 4 possible stages: approach, reach, interact, depart
        )

    def predict_stage(self, text, trajectory, goal_pos):
        # Analyze text and trajectory to predict current action stage
        trajectory_emb = self.embed_trajectory(trajectory)
        text_emb = self.embed_text(text)
        combined = torch.cat([trajectory_emb, text_emb], dim=-1)

        stage_logits = self.stage_predictor(combined)
        return torch.argmax(stage_logits, dim=-1)

    def should_transition(self, text, trajectory, goal_pos):
        # Determine if current stage should end and next should begin
        current_stage = self.predict_stage(text, trajectory, goal_pos)

        # Simple heuristic: transition if close to stage completion
        return self.stage_completion_score(trajectory, goal_pos) > 0.8

    def stage_completion_score(self, trajectory, goal_pos):
        # Compute how close current stage is to completion
        current_pos = trajectory[-1]
        distance_to_goal = torch.norm(current_pos - goal_pos)
        return min(1.0, 1.0 / (1.0 + distance_to_goal))

# Local Scene Encoder
class LocalSceneEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.pointnet = PointNet()  # Process 3D scene geometry
        self.position_encoder = nn.Linear(3, 128)  # Encode start/goal positions

    def forward(self, scene_start, scene_goal):
        # Extract local geometry around start and goal positions
        local_start = self.extract_local_geometry(scene_start)
        local_goal = self.extract_local_geometry(scene_goal)

        # Encode positions
        pos_start = self.position_encoder(scene_start['position'])
        pos_goal = self.position_encoder(scene_goal['position'])

        # Combine local geometry and position encodings
        scene_emb = torch.cat([local_start, local_goal, pos_start, pos_goal], dim=-1)
        return scene_emb

    def extract_local_geometry(self, scene_patch):
        # Process local 3D scene patch
        return self.pointnet(scene_patch['points'])

# Training with multi-stage objectives
def train_lingo(model, batch):
    text, goal_pos, gt_motion, scene_start, scene_goal = batch

    # Forward pass
    pred_motion = model(text, goal_pos, gt_motion[:, :-1], scene_start, scene_goal)

    # Motion reconstruction loss
    motion_loss = nn.MSELoss()(pred_motion, gt_motion[:, 1:])

    # Stage transition loss
    stage_targets = model.scheduler.get_stage_targets(text, gt_motion, goal_pos)
    stage_loss = nn.CrossEntropyLoss()(model.scheduler.predict_stages(text, gt_motion, goal_pos), stage_targets)

    # Scene awareness loss
    scene_loss = model.compute_scene_penetration_loss(pred_motion, scene_start, scene_goal)

    total_loss = motion_loss + 0.1 * stage_loss + 0.05 * scene_loss
    return total_loss

# Usage example
if __name__ == "__main__":
    model = Lingo()

    # Example: "Walk to the chair and sit down"
    text_instruction = "Walk to the chair and sit down"
    goal_position = torch.tensor([2.0, 0.0, 0.0])  # Chair position
    start_scene = {'points': torch.randn(1000, 3), 'position': torch.zeros(3)}
    goal_scene = {'points': torch.randn(1000, 3), 'position': goal_position}

    # Generate autonomous interaction
    motion_sequence = model.autonomous_synthesis(
        text_instruction, torch.zeros(3), goal_position,
        {'start': start_scene, 'goal': goal_scene}
    )

    print(f"Generated {len(motion_sequence)} motion segments")</code></pre>
          </div>
          <div class="lang-zh" style="display:none">
            <pre><code># Lingo: è‡ªä¸»è§’è‰²-åœºæ™¯äº¤äº’åˆæˆ
import torch
import torch.nn as nn
from diffusion_model import AutoregressiveDiffusion
from scene_encoder import LocalSceneEncoder
from scheduler import AutonomousScheduler

class Lingo(nn.Module):
    def __init__(self, motion_dim=135, scene_dim=512, text_dim=768):
        super().__init__()
        self.diffusion = AutoregressiveDiffusion(motion_dim)  # è‡ªå›å½’æ‰©æ•£æ¨¡å‹
        self.scene_encoder = LocalSceneEncoder()  # å±€éƒ¨åœºæ™¯ç¼–ç å™¨
        self.text_encoder = nn.TransformerEncoder(text_dim, 6)  # æ–‡æœ¬ç¼–ç å™¨
        self.scheduler = AutonomousScheduler()  # è‡ªä¸»é˜¶æ®µè°ƒåº¦å™¨
        self.cross_modal_attn = nn.MultiheadAttention(motion_dim, 8)  # è·¨æ¨¡æ€æ³¨æ„åŠ›

    def forward(self, text, goal_pos, current_motion, scene_start, scene_goal):
        # ç¼–ç è¾“å…¥
        text_emb = self.text_encoder(text)  # æ–‡æœ¬åµŒå…¥
        scene_emb = self.scene_encoder(scene_start, scene_goal)  # åœºæ™¯åµŒå…¥

        # è‡ªå›å½’è¿åŠ¨åˆæˆ
        motion_pred = self.diffusion(current_motion, text_emb, scene_emb)

        # è¯­è¨€-è¿åŠ¨é›†æˆ
        motion_enhanced, _ = self.cross_modal_attn(
            motion_pred.transpose(0,1),  # [seq_len, B, motion_dim]
            text_emb.transpose(0,1),     # [text_len, B, text_dim]
            text_emb.transpose(0,1)
        )

        return motion_enhanced.transpose(0,1)

    def autonomous_synthesis(self, text, start_pos, goal_pos, scene):
        """ä»æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆå¤šé˜¶æ®µäº¤äº’"""
        trajectory = [start_pos]
        motion_sequence = []

        while not self.at_goal(trajectory[-1], goal_pos):
            # å½“å‰é˜¶æ®µè¯„ä¼°
            current_stage = self.scheduler.predict_stage(text, trajectory, goal_pos)

            # å½“å‰ä¸Šä¸‹æ–‡çš„å±€éƒ¨åœºæ™¯æ„ŸçŸ¥
            scene_local = self.scene_encoder.extract_local(scene, trajectory[-1], goal_pos)

            # ç”Ÿæˆä¸‹ä¸€ä¸ªè¿åŠ¨ç‰‡æ®µ
            current_motion = self.get_current_motion_state()
            next_motion = self(text, goal_pos, current_motion, scene_local, scene_local)

            # æ‰§è¡Œå¹¶æ›´æ–°è½¨è¿¹
            trajectory.append(self.execute_motion(next_motion))
            motion_sequence.append(next_motion)

            # æ£€æŸ¥é˜¶æ®µè½¬æ¢
            if self.scheduler.should_transition(text, trajectory, goal_pos):
                continue  # ç»§ç»­åˆ°ä¸‹ä¸€é˜¶æ®µ

        return motion_sequence

    def at_goal(self, current_pos, goal_pos, threshold=0.1):
        return torch.norm(current_pos - goal_pos) < threshold

# è‡ªä¸»é˜¶æ®µè°ƒåº¦å™¨
class AutonomousScheduler(nn.Module):
    def __init__(self):
        super().__init__()
        self.stage_predictor = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 4)  # 4ä¸ªå¯èƒ½é˜¶æ®µï¼šæ¥è¿‘ã€ä¼¸æ‰‹ã€äº¤äº’ã€ç¦»å¼€
        )

    def predict_stage(self, text, trajectory, goal_pos):
        # åˆ†ææ–‡æœ¬å’Œè½¨è¿¹é¢„æµ‹å½“å‰åŠ¨ä½œé˜¶æ®µ
        trajectory_emb = self.embed_trajectory(trajectory)
        text_emb = self.embed_text(text)
        combined = torch.cat([trajectory_emb, text_emb], dim=-1)

        stage_logits = self.stage_predictor(combined)
        return torch.argmax(stage_logits, dim=-1)

    def should_transition(self, text, trajectory, goal_pos):
        # ç¡®å®šå½“å‰é˜¶æ®µæ˜¯å¦åº”è¯¥ç»“æŸï¼Œä¸‹ä¸€é˜¶æ®µæ˜¯å¦åº”è¯¥å¼€å§‹
        return self.stage_completion_score(trajectory, goal_pos) > 0.8

# å±€éƒ¨åœºæ™¯ç¼–ç å™¨
class LocalSceneEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.pointnet = PointNet()  # å¤„ç†3Dåœºæ™¯å‡ ä½•
        self.position_encoder = nn.Linear(3, 128)  # ç¼–ç èµ·å§‹/ç›®æ ‡ä½ç½®

    def forward(self, scene_start, scene_goal):
        # æå–èµ·å§‹å’Œç›®æ ‡ä½ç½®å‘¨å›´çš„å±€éƒ¨å‡ ä½•
        local_start = self.extract_local_geometry(scene_start)
        local_goal = self.extract_local_geometry(scene_goal)

        # ç¼–ç ä½ç½®
        pos_start = self.position_encoder(scene_start['position'])
        pos_goal = self.position_encoder(scene_goal['position'])

        # ç»„åˆå±€éƒ¨å‡ ä½•å’Œä½ç½®ç¼–ç 
        scene_emb = torch.cat([local_start, local_goal, pos_start, pos_goal], dim=-1)
        return scene_emb

# å¸¦å¤šé˜¶æ®µç›®æ ‡çš„è®­ç»ƒ
def train_lingo(model, batch):
    text, goal_pos, gt_motion, scene_start, scene_goal = batch

    # å‰å‘ä¼ æ’­
    pred_motion = model(text, goal_pos, gt_motion[:, :-1], scene_start, scene_goal)

    # è¿åŠ¨é‡å»ºæŸå¤±
    motion_loss = nn.MSELoss()(pred_motion, gt_motion[:, 1:])

    # é˜¶æ®µè½¬æ¢æŸå¤±
    stage_targets = model.scheduler.get_stage_targets(text, gt_motion, goal_pos)
    stage_loss = nn.CrossEntropyLoss()(model.scheduler.predict_stages(text, gt_motion, goal_pos), stage_targets)

    # åœºæ™¯æ„ŸçŸ¥æŸå¤±
    scene_loss = model.compute_scene_penetration_loss(pred_motion, scene_start, scene_goal)

    total_loss = motion_loss + 0.1 * stage_loss + 0.05 * scene_loss
    return total_loss

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = Lingo()

    # ç¤ºä¾‹ï¼š"èµ°åˆ°æ¤…å­æ—è¾¹åä¸‹"
    text_instruction = "Walk to the chair and sit down"
    goal_position = torch.tensor([2.0, 0.0, 0.0])  # æ¤…å­ä½ç½®

    # ç”Ÿæˆè‡ªä¸»äº¤äº’
    motion_sequence = model.autonomous_synthesis(
        text_instruction, torch.zeros(3), goal_position,
        {'start': start_scene, 'goal': goal_scene}
    )

    print(f"ç”Ÿæˆäº† {len(motion_sequence)} ä¸ªè¿åŠ¨ç‰‡æ®µ")</code></pre>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://github.com/mileret/lingo-release" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          GitHub Repo
        </a>
      </div>

    </div>
  </section>
  <script>
    function toggleLang(btn) {
      const container = btn.closest('div');
      const enElements = container.querySelectorAll('.lang-en');
      const zhElements = container.querySelectorAll('.lang-zh');
      
      enElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
      
      zhElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
    }

    function openLocalPdf(filename) {
      window.open('pdfs/' + filename, '_blank');
    }

    function delLocalPdf(filename) {
      // User will implement this themselves
      alert("Please implement the delete logic or delete the file manually: pdfs/" + filename);
    }
  </script>
</body>
</html>

