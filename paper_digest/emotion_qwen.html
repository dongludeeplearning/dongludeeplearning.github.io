<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Emotion-Qwen: A Unified Framework for Emotion and Vision Understanding</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Zihao Han, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang, Zheng Lian, Zhi-Qi Cheng, Xiaojiang Peng<br>
        <span style="opacity:0.8">Shenzhen Technology University, Stanford University, University of Washington, UESTC, Skyworth Digital, Xiaopai Tech, CASIA</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we develop a unified multimodal framework that excels in emotion understanding while preserving general vision-language reasoning capabilities, avoiding catastrophic forgetting during fine-tuning?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œåœ¨æƒ…æ„Ÿç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒä¸€èˆ¬çš„è§†è§‰-è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œé¿å…åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Unified Emotion-Vision Framework:</b> Introduced Emotion-Qwen, a multimodal framework that simultaneously achieves robust emotion understanding and preserves general VL reasoning capabilities, overcoming catastrophic forgetting in emotion-specific fine-tuning.</div>
            <div class="lang-zh" style="display:none"><b>ç»Ÿä¸€çš„æƒ…æ„Ÿ-è§†è§‰æ¡†æ¶ï¼š</b>å¼•å…¥äº†Emotion-Qwenï¼Œä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼ŒåŒæ—¶å®ç°å¼ºå¤§çš„æƒ…æ„Ÿç†è§£å¹¶ä¿æŒä¸€èˆ¬çš„VLæ¨ç†èƒ½åŠ›ï¼Œå…‹æœäº†æƒ…æ„Ÿç‰¹å®šå¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Hybrid Compressor with MoE:</b> Developed a novel Hybrid Compressor based on Mixture-of-Experts architecture that dynamically routes inputs to balance emotion-specific processing and general multimodal reasoning through specialized expert routing.</div>
            <div class="lang-zh" style="display:none"><b>åŸºäºMoEçš„æ··åˆå‹ç¼©å™¨ï¼š</b>å¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ··åˆå‹ç¼©å™¨ï¼ŒåŸºäºä¸“å®¶æ··åˆæ¶æ„ï¼Œé€šè¿‡ä¸“é—¨çš„ä¸“å®¶è·¯ç”±åŠ¨æ€è·¯ç”±è¾“å…¥ï¼Œä»¥å¹³è¡¡æƒ…æ„Ÿç‰¹å®šå¤„ç†å’Œä¸€èˆ¬å¤šæ¨¡æ€æ¨ç†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Three-Stage Pre-training Pipeline:</b> Proposed a structured three-stage pre-training approach leveraging extensive general and emotion-focused datasets to enhance multimodal representation robustness and model adaptability.</div>
            <div class="lang-zh" style="display:none"><b>ä¸‰é˜¶æ®µé¢„è®­ç»ƒç®¡é“ï¼š</b>æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‰é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å¹¿æ³›çš„ä¸€èˆ¬å’Œæƒ…æ„Ÿèšç„¦æ•°æ®é›†æ¥å¢å¼ºå¤šæ¨¡æ€è¡¨ç¤ºé²æ£’æ€§å’Œæ¨¡å‹é€‚åº”æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Video Emotion Reasoning Dataset:</b> Created VER dataset, a large-scale bilingual resource with over 40K video clips annotated with detailed context-aware emotional descriptions, facilitating fine-grained emotional reasoning research.</div>
            <div class="lang-zh" style="display:none"><b>è§†é¢‘æƒ…æ„Ÿæ¨ç†æ•°æ®é›†ï¼š</b>åˆ›å»ºäº†VERæ•°æ®é›†ï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„åŒè¯­èµ„æºï¼ŒåŒ…å«è¶…è¿‡40Kä¸ªè§†é¢‘å‰ªè¾‘ï¼Œæ ‡æ³¨äº†è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æƒ…æ„Ÿæè¿°ï¼Œä¿ƒè¿›ç»†ç²’åº¦æƒ…æ„Ÿæ¨ç†ç ”ç©¶ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>State-of-the-Art Performance:</b> Achieved superior results across multiple emotion recognition and reasoning benchmarks while maintaining competitive performance on general VL tasks, demonstrating the effectiveness of the unified approach.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›çš„æ€§èƒ½ï¼š</b>åœ¨å¤šä¸ªæƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†å“è¶Šç»“æœï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬VLä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ï¼Œè¯æ˜äº†ç»Ÿä¸€æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Catastrophic Forgetting in Fine-tuning:</b> Large Multimodal Models suffer from catastrophic forgetting when fine-tuned on emotion-specific tasks, losing their general VL reasoning capabilities while gaining emotion expertise.</div>
            <div class="lang-zh" style="display:none"><b>å¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼š</b>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é’ˆå¯¹æƒ…æ„Ÿç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶ä¼šé­å—ç¾éš¾æ€§é—å¿˜ï¼Œå¤±å»å…¶ä¸€èˆ¬VLæ¨ç†èƒ½åŠ›çš„åŒæ—¶è·å¾—æƒ…æ„Ÿä¸“ä¸šçŸ¥è¯†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Balancing Emotion and General Tasks:</b> Developing a unified framework that can handle both specialized emotion understanding and general multimodal reasoning without compromising performance in either domain.</div>
            <div class="lang-zh" style="display:none"><b>å¹³è¡¡æƒ…æ„Ÿå’Œä¸€èˆ¬ä»»åŠ¡ï¼š</b>å¼€å‘ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†ä¸“é—¨çš„æƒ…æ„Ÿç†è§£å’Œä¸€èˆ¬å¤šæ¨¡æ€æ¨ç†ï¼Œè€Œä¸åœ¨ä»»ä½•ä¸€ä¸ªé¢†åŸŸå¦¥åæ€§èƒ½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Dynamic Input Routing:</b> Designing an effective routing mechanism in MoE architecture that can intelligently distribute processing between emotion-specific experts and general-purpose experts based on input characteristics.</div>
            <div class="lang-zh" style="display:none"><b>åŠ¨æ€è¾“å…¥è·¯ç”±ï¼š</b>åœ¨MoEæ¶æ„ä¸­è®¾è®¡æœ‰æ•ˆçš„è·¯ç”±æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥ç‰¹å¾æ™ºèƒ½åœ°åœ¨æƒ…æ„Ÿç‰¹å®šä¸“å®¶å’Œé€šç”¨ä¸“å®¶ä¹‹é—´åˆ†é…å¤„ç†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multimodal Emotion Integration:</b> Effectively integrating visual, textual, auditory, and contextual cues for comprehensive emotion understanding while maintaining computational efficiency.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€æƒ…æ„Ÿé›†æˆï¼š</b>æœ‰æ•ˆåœ°æ•´åˆè§†è§‰ã€æ–‡æœ¬ã€å¬è§‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢ä»¥å®ç°å…¨é¢çš„æƒ…æ„Ÿç†è§£ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Large-scale Emotion Dataset Creation:</b> Creating high-quality, context-aware emotional annotations for large-scale video datasets to support fine-grained emotional reasoning research.</div>
            <div class="lang-zh" style="display:none"><b>å¤§è§„æ¨¡æƒ…æ„Ÿæ•°æ®é›†åˆ›å»ºï¼š</b>ä¸ºå¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†åˆ›å»ºé«˜è´¨é‡ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æƒ…æ„Ÿæ ‡æ³¨ï¼Œä»¥æ”¯æŒç»†ç²’åº¦æƒ…æ„Ÿæ¨ç†ç ”ç©¶ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>Emotion-Qwen addresses the challenge of unified emotion and vision understanding through:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Hybrid Compressor with MoE:</b> A Mixture-of-Experts architecture that dynamically routes inputs between emotion-specific experts and general multimodal experts to balance specialized emotion processing with broad reasoning capabilities.</li>
                <li style="margin-bottom:6px;"><b>Three-Stage Pre-training:</b> Stage 1 focuses on general multimodal alignment, Stage 2 incorporates emotion-specific datasets, and Stage 3 fine-tunes on high-quality emotion reasoning data for robust performance.</li>
                <li style="margin-bottom:6px;"><b>Video Emotion Reasoning Dataset:</b> Large-scale bilingual dataset with 40K+ video clips featuring detailed context-aware emotional descriptions, supporting fine-grained emotional reasoning tasks.</li>
                <li style="margin-bottom:6px;"><b>Unified Architecture:</b> Single framework that handles both emotion-specific tasks and general VL reasoning, avoiding the need for separate models and reducing computational overhead.</li>
                <li style="margin-bottom:6px;"><b>Dynamic Routing Mechanism:</b> Intelligent routing system that analyzes input characteristics to determine optimal expert allocation, ensuring efficient processing of diverse multimodal inputs.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>Emotion-Qwené€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³ç»Ÿä¸€æƒ…æ„Ÿå’Œè§†è§‰ç†è§£çš„æŒ‘æˆ˜ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>åŸºäºMoEçš„æ··åˆå‹ç¼©å™¨ï¼š</b>ä¸“å®¶æ··åˆæ¶æ„ï¼ŒåŠ¨æ€åœ°åœ¨æƒ…æ„Ÿç‰¹å®šä¸“å®¶å’Œä¸€èˆ¬å¤šæ¨¡æ€ä¸“å®¶ä¹‹é—´è·¯ç”±è¾“å…¥ï¼Œä»¥å¹³è¡¡ä¸“é—¨çš„æƒ…æ„Ÿå¤„ç†å’Œå¹¿æ³›çš„æ¨ç†èƒ½åŠ›ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä¸‰é˜¶æ®µé¢„è®­ç»ƒï¼š</b>ç¬¬ä¸€é˜¶æ®µä¸“æ³¨äºä¸€èˆ¬å¤šæ¨¡æ€å¯¹é½ï¼Œç¬¬äºŒé˜¶æ®µçº³å…¥æƒ…æ„Ÿç‰¹å®šæ•°æ®é›†ï¼Œç¬¬ä¸‰é˜¶æ®µåœ¨é«˜è´¨é‡æƒ…æ„Ÿæ¨ç†æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒä»¥å®ç°ç¨³å¥æ€§èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>è§†é¢‘æƒ…æ„Ÿæ¨ç†æ•°æ®é›†ï¼š</b>å¤§è§„æ¨¡åŒè¯­æ•°æ®é›†ï¼ŒåŒ…å«40K+è§†é¢‘å‰ªè¾‘ï¼Œå…·æœ‰è¯¦ç»†çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æƒ…æ„Ÿæè¿°ï¼Œæ”¯æŒç»†ç²’åº¦æƒ…æ„Ÿæ¨ç†ä»»åŠ¡ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»Ÿä¸€æ¶æ„ï¼š</b>å•ä¸€æ¡†æ¶åŒæ—¶å¤„ç†æƒ…æ„Ÿç‰¹å®šä»»åŠ¡å’Œä¸€èˆ¬VLæ¨ç†ï¼Œé¿å…éœ€è¦å•ç‹¬æ¨¡å‹å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚</li>
                <li style="margin-bottom:6px;"><b>åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼š</b>æ™ºèƒ½è·¯ç”±ç³»ç»Ÿåˆ†æè¾“å…¥ç‰¹å¾ä»¥ç¡®å®šæœ€ä½³ä¸“å®¶åˆ†é…ï¼Œç¡®ä¿å¯¹å¤šæ ·åŒ–å¤šæ¨¡æ€è¾“å…¥çš„é«˜æ•ˆå¤„ç†ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/emotion_qwen_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
            <p>Emotion-Qwen framework overview showing the unified approach for emotion understanding and general VL reasoning, highlighting the MoE-based Hybrid Compressor.</p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆæ€»ä½“ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/emotion_qwen_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p>Detailed architecture of Emotion-Qwen showing the three-stage pre-training pipeline and the Hybrid Compressor with MoE routing mechanism.</p>
            <p>Note: The framework achieves SOTA performance on emotion recognition benchmarks while maintaining competitive results on general VL tasks.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_emotion_qwen.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_emotion_qwen.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of Emotion-Qwen lies in its principled approach to balancing specialization and generalization:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>MoE-based Dynamic Routing:</b> The Hybrid Compressor intelligently routes inputs to appropriate experts, allowing specialized emotion processing without sacrificing general capabilities, unlike traditional fine-tuning approaches that cause catastrophic forgetting.</li>
                <li style="margin-bottom:6px;"><b>Progressive Pre-training Strategy:</b> The three-stage pipeline builds foundational multimodal understanding before specializing in emotion tasks, ensuring robust representations that can handle both specific and general scenarios.</li>
                <li style="margin-bottom:6px;"><b>Unified Architecture Efficiency:</b> Single framework eliminates the need for separate models, reducing computational overhead and enabling seamless integration of emotion understanding with general VL reasoning.</li>
                <li style="margin-bottom:6px;"><b>Large-scale Emotion Dataset:</b> VER dataset provides comprehensive context-aware annotations that enable fine-grained emotional reasoning, addressing the limitations of existing emotion datasets that lack depth and scale.</li>
                <li style="margin-bottom:6px;"><b>Expert Balancing Mechanism:</b> The routing system dynamically adjusts expert utilization based on task requirements, ensuring optimal performance across emotion-specific and general multimodal tasks.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>Emotion-Qwençš„æˆåŠŸåœ¨äºå…¶å¹³è¡¡ä¸“ä¸šåŒ–å’Œæ³›åŒ–çš„åŸºæœ¬æ–¹æ³•ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>åŸºäºMoEçš„åŠ¨æ€è·¯ç”±ï¼š</b>æ··åˆå‹ç¼©å™¨æ™ºèƒ½åœ°å°†è¾“å…¥è·¯ç”±åˆ°é€‚å½“çš„ä¸“å®¶ï¼Œå…è®¸ä¸“é—¨çš„æƒ…æ„Ÿå¤„ç†è€Œä¸ç‰ºç‰²ä¸€èˆ¬èƒ½åŠ›ï¼Œè¿™ä¸å¯¼è‡´ç¾éš¾æ€§é—å¿˜çš„ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ä¸åŒã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¸è¿›å¼é¢„è®­ç»ƒç­–ç•¥ï¼š</b>ä¸‰é˜¶æ®µç®¡é“åœ¨ä¸“é—¨å¤„ç†æƒ…æ„Ÿä»»åŠ¡ä¹‹å‰å»ºç«‹åŸºç¡€çš„å¤šæ¨¡æ€ç†è§£ï¼Œç¡®ä¿èƒ½å¤Ÿå¤„ç†ç‰¹å®šå’Œä¸€èˆ¬åœºæ™¯çš„ç¨³å¥è¡¨ç¤ºã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»Ÿä¸€æ¶æ„æ•ˆç‡ï¼š</b>å•ä¸€æ¡†æ¶æ¶ˆé™¤äº†å¯¹å•ç‹¬æ¨¡å‹çš„éœ€æ±‚ï¼Œå‡å°‘äº†è®¡ç®—å¼€é”€å¹¶å®ç°äº†æƒ…æ„Ÿç†è§£ä¸ä¸€èˆ¬VLæ¨ç†çš„æ— ç¼é›†æˆã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤§è§„æ¨¡æƒ…æ„Ÿæ•°æ®é›†ï¼š</b>VERæ•°æ®é›†æä¾›å…¨é¢çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ ‡æ³¨ï¼Œèƒ½å¤Ÿè¿›è¡Œç»†ç²’åº¦æƒ…æ„Ÿæ¨ç†ï¼Œè§£å†³äº†ç°æœ‰æƒ…æ„Ÿæ•°æ®é›†ç¼ºä¹æ·±åº¦å’Œè§„æ¨¡çš„å±€é™æ€§ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä¸“å®¶å¹³è¡¡æœºåˆ¶ï¼š</b>è·¯ç”±ç³»ç»Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´ä¸“å®¶åˆ©ç”¨ç‡ï¼Œç¡®ä¿åœ¨æƒ…æ„Ÿç‰¹å®šå’Œä¸€èˆ¬å¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>Emotion-Qwen represents a significant advancement in multimodal emotion understanding by successfully unifying specialized emotion processing with general vision-language reasoning. The framework's innovative Hybrid Compressor with MoE architecture overcomes the critical challenge of catastrophic forgetting, enabling models to excel in emotion-specific tasks while maintaining competitive performance on broader multimodal benchmarks. The carefully designed three-stage pre-training pipeline and the comprehensive VER dataset establish new standards for emotion understanding research, providing researchers with both a powerful framework and high-quality resources for future investigations. By demonstrating superior performance across multiple emotion recognition and reasoning tasks while preserving general VL capabilities, Emotion-Qwen validates the effectiveness of unified approaches in multimodal learning. The work opens new avenues for developing more robust and versatile multimodal systems that can handle both specialized domain tasks and general reasoning challenges. The framework's ability to dynamically balance emotion-specific processing with general multimodal reasoning through intelligent routing represents a significant step toward more adaptable and efficient multimodal AI systems. Future work can build upon this foundation to explore even more sophisticated routing mechanisms and larger-scale emotion datasets, further advancing the field of affective multimodal understanding.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>Emotion-Qwené€šè¿‡æˆåŠŸåœ°å°†ä¸“é—¨çš„æƒ…æ„Ÿå¤„ç†ä¸ä¸€èˆ¬çš„è§†è§‰-è¯­è¨€æ¨ç†ç»Ÿä¸€èµ·æ¥ï¼Œä»£è¡¨äº†å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„é‡è¦è¿›æ­¥ã€‚è¯¥æ¡†æ¶çš„åˆ›æ–°æ··åˆå‹ç¼©å™¨ä¸MoEæ¶æ„å…‹æœäº†ç¾éš¾æ€§é—å¿˜çš„å…³é”®æŒ‘æˆ˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æƒ…æ„Ÿç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨æ›´å¹¿æ³›çš„å¤šæ¨¡æ€åŸºå‡†ä¸Šä¿æŒç«äº‰åŠ›ã€‚ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µé¢„è®­ç»ƒç®¡é“å’Œå…¨é¢çš„VERæ•°æ®é›†ä¸ºæƒ…æ„Ÿç†è§£ç ”ç©¶å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†å¼ºå¤§çš„æ¡†æ¶å’Œé«˜è´¨é‡èµ„æºï¼Œç”¨äºæœªæ¥çš„ç ”ç©¶ã€‚é€šè¿‡åœ¨å¤šä¸ªæƒ…æ„Ÿè¯†åˆ«å’Œæ¨ç†ä»»åŠ¡ä¸­å±•ç¤ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒä¸€èˆ¬VLèƒ½åŠ›ï¼ŒEmotion-QwenéªŒè¯äº†å¤šæ¨¡æ€å­¦ä¹ ä¸­ç»Ÿä¸€æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘æ›´å¼ºå¤§å’Œå¤šåŠŸèƒ½çš„ç³»ç»Ÿå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œè¿™äº›ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†ä¸“é—¨çš„é¢†åŸŸä»»åŠ¡å’Œä¸€èˆ¬æ¨ç†æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ™ºèƒ½è·¯ç”±åŠ¨æ€å¹³è¡¡æƒ…æ„Ÿç‰¹å®šå¤„ç†ä¸ä¸€èˆ¬å¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ï¼Œä»£è¡¨äº†æœç€æ›´é€‚åº”æ€§å’Œé«˜æ•ˆçš„å¤šæ¨¡æ€AIç³»ç»Ÿçš„é‡è¦ä¸€æ­¥ã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥åœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ„å»ºï¼Œæ¢ç´¢æ›´å¤æ‚çš„è·¯ç”±æœºåˆ¶å’Œæ›´å¤§è§„æ¨¡çš„æƒ…æ„Ÿæ•°æ®é›†ï¼Œè¿›ä¸€æ­¥æ¨è¿›æƒ…æ„Ÿå¤šæ¨¡æ€ç†è§£é¢†åŸŸã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2505.06685" target="_blank" style="color:#8bffcf;">2505.06685</a></p>
          <p><b>Code:</b> <a href="https://anonymous.4open.science/r/Emotion-Qwen-Anonymous" target="_blank" style="color:#8bffcf;">Anonymous Repository</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# Emotion-Qwen Hybrid Compressor Implementation<br>
<br>
// Hybrid Compressor with Mixture-of-Experts<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">HybridCompressor</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Hybrid Compressor with MoE for emotion and general multimodal processing"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, hidden_dim, num_emotion_experts=4, num_general_experts=4):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.hidden_dim = hidden_dim<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_emotion_experts = num_emotion_experts<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_general_experts = num_general_experts<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Emotion-specific experts for emotion understanding</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.emotion_experts = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim, hidden_dim * 2),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim * 2, hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;) <span style="color:#569cd6;">for</span> _ <span style="color:#569cd6;">in</span> range(num_emotion_experts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># General experts for VL reasoning</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.general_experts = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim, hidden_dim * 2),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim * 2, hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;) <span style="color:#569cd6;">for</span> _ <span style="color:#569cd6;">in</span> range(num_general_experts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Router for dynamic expert selection</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.router = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim, hidden_dim // 2),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim // 2, num_emotion_experts + num_general_experts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Emotion detection module</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.emotion_detector = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim, hidden_dim // 4),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(hidden_dim // 4, 1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sigmoid()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Dynamic routing based on emotion content"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size, seq_len, _ = x.shape<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x_flat = x.view(-1, self.hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Detect emotion content</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotion_score = self.emotion_detector(x_flat)  <span style="color:#6a9955;"># (batch_size * seq_len, 1)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Get routing probabilities</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;route_logits = self.router(x_flat)  <span style="color:#6a9955;"># (batch_size * seq_len, num_experts)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;route_weights = F.softmax(route_logits, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Split weights for emotion and general experts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotion_weights = route_weights[:, :self.num_emotion_experts]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;general_weights = route_weights[:, self.num_emotion_experts:]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Weighted combination of emotion experts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotion_output = torch.zeros_like(x_flat)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> i, expert <span style="color:#569cd6;">in</span> enumerate(self.emotion_experts):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotion_output += emotion_weights[:, i:i+1] * expert(x_flat)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Weighted combination of general experts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;general_output = torch.zeros_like(x_flat)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> i, expert <span style="color:#569cd6;">in</span> enumerate(self.general_experts):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;general_output += general_weights[:, i:i+1] * expert(x_flat)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Balance emotion and general processing based on emotion score</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined_output = emotion_score * emotion_output + (1 - emotion_score) * general_output<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> combined_output.view(batch_size, seq_len, self.hidden_dim)<br>
<br>
// Three-Stage Pre-training Pipeline<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">EmotionQwenPretrainer</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Three-stage pre-training for emotion and general capabilities"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, model, hybrid_compressor):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model = model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.compressor = hybrid_compressor<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.stage = 1<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">stage1_general_alignment</span>(self, general_datasets):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Stage 1: General multimodal alignment"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 1: Training on general VL datasets")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Train primarily on general experts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> batch <span style="color:#569cd6;">in</span> general_datasets:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = self._train_step(batch, emotion_weight=0.1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Update model</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">stage2_emotion_integration</span>(self, emotion_datasets):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Stage 2: Emotion-specific training"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 2: Integrating emotion understanding")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Train with balanced emotion/general processing</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> batch <span style="color:#569cd6;">in</span> emotion_datasets:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = self._train_step(batch, emotion_weight=0.5)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Update model</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">stage3_emotion_reasoning</span>(self, ver_dataset):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Stage 3: Fine-grained emotion reasoning with VER dataset"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 3: Fine-tuning on emotion reasoning")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Focus on emotion experts with high-quality data</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> batch <span style="color:#569cd6;">in</span> ver_dataset:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = self._train_step(batch, emotion_weight=0.9)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Update model</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">_train_step</span>(self, batch, emotion_weight):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Single training step with dynamic routing"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs, targets = batch<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = self.model(inputs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compressed = self.compressor(outputs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> F.cross_entropy(compressed, targets)<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
