<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ CVPR 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black <br>
        <span style="opacity:0.8">The University of Tokyo, Keio University, MPI-IS, JAIST, Tsinghua University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to generate holistic, full-body co-speech gestures (including face, hands, body) that are synchronized with audio and can be edited via masked hints, overcoming the limitations of previous methods that treat body parts in isolation?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„ã€æ•´ä½“çš„å…¨èº«ååŒè¯­éŸ³æ‰‹åŠ¿ï¼ˆåŒ…æ‹¬é¢éƒ¨ã€æ‰‹ã€èº«ä½“ï¼‰ï¼Œå¹¶å¯ä»¥é€šè¿‡æ©ç æç¤ºè¿›è¡Œç¼–è¾‘ï¼Œå…‹æœä»¥å‰å°†èº«ä½“éƒ¨ä½éš”ç¦»å¤„ç†çš„æ–¹æ³•çš„å±€é™æ€§ï¼Ÿ
        </div>
      </div>
    </div>
  
    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>BEAT2 Dataset:</b> Introduced a new mesh-level holistic co-speech dataset (BEAT2) combining MoShed SMPL-X body with FLAME head, offering high-quality 3D motion capture with refined fingers and neck.</div>
            <div class="lang-zh" style="display:none"><b>BEAT2 æ•°æ®é›†ï¼š</b>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç½‘æ ¼çº§æ•´ä½“ååŒè¯­éŸ³æ•°æ®é›† (BEAT2)ï¼Œç»“åˆäº† MoShed SMPL-X èº«ä½“å’Œ FLAME å¤´éƒ¨ï¼Œæä¾›äº†å…·æœ‰ç²¾ç»†æ‰‹æŒ‡å’Œé¢ˆéƒ¨çš„é«˜è´¨é‡ 3D åŠ¨ä½œæ•æ‰ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Masked Audio Gesture Transformer:</b> Proposed a framework that jointly learns audio-to-gesture generation and masked gesture reconstruction, allowing for effective encoding of audio and body gesture hints.</div>
            <div class="lang-zh" style="display:none"><b>Masked Audio Gesture Transformerï¼š</b>æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œè”åˆå­¦ä¹ éŸ³é¢‘åˆ°æ‰‹åŠ¿çš„ç”Ÿæˆå’Œæ©ç æ‰‹åŠ¿é‡å»ºï¼Œå…è®¸æœ‰æ•ˆç¼–ç éŸ³é¢‘å’Œèº«ä½“æ‰‹åŠ¿æç¤ºã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Holistic Generation:</b> Achieved state-of-the-art performance in generating unified full-body gestures (face, body, hands) and enabled flexible editing via masked inputs.</div>
            <div class="lang-zh" style="display:none"><b>æ•´ä½“ç”Ÿæˆï¼š</b>åœ¨ç”Ÿæˆç»Ÿä¸€çš„å…¨èº«æ‰‹åŠ¿ï¼ˆé¢éƒ¨ã€èº«ä½“ã€æ‰‹ï¼‰æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ©ç è¾“å…¥å®ç°äº†çµæ´»çš„ç¼–è¾‘ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/emage_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Fragmented Modeling:</b> Previous works often modeled body, hands, and face separately or ignored some parts, leading to incoherent holistic motion.</div>
            <div class="lang-zh" style="display:none"><b>ç¢ç‰‡åŒ–å»ºæ¨¡ï¼š</b>ä»¥å‰çš„å·¥ä½œé€šå¸¸åˆ†åˆ«å¯¹èº«ä½“ã€æ‰‹å’Œé¢éƒ¨è¿›è¡Œå»ºæ¨¡ï¼Œæˆ–è€…å¿½ç•¥æŸäº›éƒ¨åˆ†ï¼Œå¯¼è‡´æ•´ä½“è¿åŠ¨ä¸è¿è´¯ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Lack of Controllability:</b> Purely audio-driven methods often lack spatial control. Editing specific gestures while maintaining synchronization with audio is difficult.</div>
            <div class="lang-zh" style="display:none"><b>ç¼ºä¹å¯æ§æ€§ï¼š</b>çº¯éŸ³é¢‘é©±åŠ¨çš„æ–¹æ³•é€šå¸¸ç¼ºä¹ç©ºé—´æ§åˆ¶ã€‚åœ¨ä¿æŒä¸éŸ³é¢‘åŒæ­¥çš„åŒæ—¶ç¼–è¾‘ç‰¹å®šæ‰‹åŠ¿æ˜¯å›°éš¾çš„ã€‚</div>
          </li>
        </ul>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆè§£å†³æ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Masked Modeling:</b> Uses a transformer to reconstruct gestures from partial (masked) inputs during training. This teaches the model to infer full-body motion from audio and sparse spatial hints.</div>
            <div class="lang-zh" style="display:none"><b>æ©ç å»ºæ¨¡ï¼š</b>åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ Transformer ä»éƒ¨åˆ†ï¼ˆæ©ç ï¼‰è¾“å…¥é‡å»ºæ‰‹åŠ¿ã€‚è¿™æ•™ä¼šäº†æ¨¡å‹ä»éŸ³é¢‘å’Œç¨€ç–çš„ç©ºé—´æç¤ºä¸­æ¨æ–­å…¨èº«è¿åŠ¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Adaptive Feature Merging:</b> Adaptively merges speech features representing rhythm and content to drive the gesture generation, ensuring semantic alignment.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªé€‚åº”ç‰¹å¾åˆå¹¶ï¼š</b>è‡ªé€‚åº”åœ°åˆå¹¶ä»£è¡¨èŠ‚å¥å’Œå†…å®¹çš„è¯­éŸ³ç‰¹å¾ä»¥é©±åŠ¨æ‰‹åŠ¿ç”Ÿæˆï¼Œç¡®ä¿è¯­ä¹‰å¯¹é½ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Compositional VQ-VAEs:</b> Utilizes four separate VQ-VAEs for face, upper body, hands, and lower body to ensure high-fidelity and diverse generation for each part.</div>
            <div class="lang-zh" style="display:none"><b>ç»„åˆ VQ-VAEï¼š</b>ä¸ºé¢éƒ¨ã€ä¸ŠåŠèº«ã€æ‰‹å’Œä¸‹åŠèº«åˆ©ç”¨å››ä¸ªç‹¬ç«‹çš„ VQ-VAEï¼Œä»¥ç¡®ä¿æ¯ä¸ªéƒ¨åˆ†çš„é«˜ä¿çœŸåº¦å’Œå¤šæ ·åŒ–ç”Ÿæˆã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Method Details (Sec. 4)ï¼ˆæ–¹æ³•ç»†èŠ‚ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>4.1 Audio and Motion Encoders:</b> The model processes audio features (rhythm and content) and motion features separately. Motion is discretized into codebook indices using pre-trained VQ-VAEs for different body parts (Upper, Lower, Hands, Face), creating a compact representation space.
            </div>
            <div class="lang-zh" style="display:none">
                <b>4.1 éŸ³é¢‘ä¸åŠ¨ä½œç¼–ç å™¨ï¼š</b>æ¨¡å‹åˆ†åˆ«å¤„ç†éŸ³é¢‘ç‰¹å¾ï¼ˆèŠ‚å¥å’Œå†…å®¹ï¼‰å’ŒåŠ¨ä½œç‰¹å¾ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„ VQ-VAEï¼Œå°†ä¸åŒèº«ä½“éƒ¨ä½ï¼ˆä¸Šèº«ã€ä¸‹èº«ã€æ‰‹ã€è„¸ï¼‰çš„åŠ¨ä½œç¦»æ•£åŒ–ä¸ºç æœ¬ç´¢å¼•ï¼Œæ„å»ºç´§å‡‘çš„è¡¨ç¤ºç©ºé—´ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>4.2 Masked Audio Gesture Transformer:</b> This core component takes audio features and a masked sequence of gesture tokens. During training, random parts of the gesture are masked, and the transformer learns to reconstruct the full gesture. This joint training enables the model to effectively leverage both audio cues and partial body hints (e.g., "upper body visible, predict legs").
            </div>
            <div class="lang-zh" style="display:none">
                <b>4.2 æ©ç éŸ³é¢‘æ‰‹åŠ¿ Transformerï¼š</b>è¿™æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œæ¥æ”¶éŸ³é¢‘ç‰¹å¾å’Œè¢«æ©ç çš„æ‰‹åŠ¿ Token åºåˆ—ã€‚è®­ç»ƒæ—¶ï¼Œéšæœºæ©ç›–æ‰‹åŠ¿çš„éƒ¨åˆ†å†…å®¹ï¼ŒTransformer å­¦ä¹ é‡å»ºå®Œæ•´æ‰‹åŠ¿ã€‚è¿™ç§è”åˆè®­ç»ƒä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨éŸ³é¢‘çº¿ç´¢å’Œéƒ¨åˆ†èº«ä½“æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œä¸Šèº«å¯è§ï¼Œé¢„æµ‹è…¿éƒ¨â€ï¼‰ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>4.3 Adaptive Merging & Attention:</b> To fuse the rhythm and content audio features effectively, EMAGE employs an adaptive merging mechanism. It dynamically weights the importance of rhythmic vs. semantic cues based on the current context, ensuring the generated gestures are both synchronized (on beat) and meaningful (semantically relevant).
            </div>
            <div class="lang-zh" style="display:none">
                <b>4.3 è‡ªé€‚åº”åˆå¹¶ä¸æ³¨æ„åŠ›ï¼š</b>ä¸ºäº†æœ‰æ•ˆèåˆèŠ‚å¥å’Œå†…å®¹éŸ³é¢‘ç‰¹å¾ï¼ŒEMAGE é‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”åˆå¹¶æœºåˆ¶ã€‚å®ƒæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡åŠ¨æ€åŠ æƒèŠ‚å¥ä¸è¯­ä¹‰çº¿ç´¢çš„é‡è¦æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„æ‰‹åŠ¿æ—¢åŒæ­¥ï¼ˆåˆæ‹ï¼‰åˆæœ‰æ„ä¹‰ï¼ˆè¯­ä¹‰ç›¸å…³ï¼‰ã€‚
            </div>
          </li>
          <li>
            <div class="lang-en">
                <b>4.4 Holistic Generation:</b> Finally, the predicted discrete tokens are decoded by the compositional VQ-VAEs back into continuous 3D motion. The separate decoders for each body part allow for high-quality, articulate synthesis of the face, hands, and body simultaneously, resulting in a cohesive full-body animation.
            </div>
            <div class="lang-zh" style="display:none">
                <b>4.4 æ•´ä½“ç”Ÿæˆï¼š</b>æœ€åï¼Œé¢„æµ‹çš„ç¦»æ•£ Token ç”±ç»„åˆ VQ-VAE è§£ç å›è¿ç»­çš„ 3D åŠ¨ä½œã€‚é’ˆå¯¹æ¯ä¸ªèº«ä½“éƒ¨ä½çš„ç‹¬ç«‹è§£ç å™¨å…è®¸åŒæ—¶é«˜è´¨é‡ã€æ¸…æ™°åœ°åˆæˆé¢éƒ¨ã€æ‰‹éƒ¨å’Œèº«ä½“ï¼Œä»è€Œäº§ç”Ÿè¿è´¯çš„å…¨èº«åŠ¨ç”»ã€‚
            </div>
          </li>
        </ul>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/emage_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            
            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2401.00374.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2401.00374.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Experimentsï¼ˆå®éªŒï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Performance:</b> Outperformed SOTA baselines (like Habibie et al. and TalkShow) in terms of FGD (Frechet Gesture Distance) and diversity metrics on the BEAT2 dataset.
            </div>
            <div class="lang-zh" style="display:none">
                <b>æ€§èƒ½ï¼š</b>åœ¨ BEAT2 æ•°æ®é›†ä¸Šï¼Œåœ¨ FGD (Frechet Gesture Distance) å’Œå¤šæ ·æ€§æŒ‡æ ‡æ–¹é¢ä¼˜äº SOTA åŸºçº¿ï¼ˆå¦‚ Habibie ç­‰äººå’Œ TalkShowï¼‰ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>User Study:</b> Human evaluators preferred EMAGE results for their naturalness, synchronization, and holistic coherence.
            </div>
            <div class="lang-zh" style="display:none">
                <b>ç”¨æˆ·ç ”ç©¶ï¼š</b>äººå·¥è¯„ä¼°å‘˜å› å…¶è‡ªç„¶æ€§ã€åŒæ­¥æ€§å’Œæ•´ä½“è¿è´¯æ€§è€Œæ›´å–œæ¬¢ EMAGE ç»“æœã€‚
            </div>
          </li>
        </ul>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Flexibility:</b> EMAGE is not just a generator but an editor. It can take partial gestures (e.g., "raise right hand") and fill in the rest of the body motion plausibly while syncing with audio.
            </div>
            <div class="lang-zh" style="display:none">
                <b>çµæ´»æ€§ï¼š</b>EMAGE ä¸ä»…ä»…æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè¿˜æ˜¯ä¸€ä¸ªç¼–è¾‘å™¨ã€‚å®ƒå¯ä»¥æ¥å—éƒ¨åˆ†æ‰‹åŠ¿ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸¾èµ·å³æ‰‹â€ï¼‰å¹¶åˆç†åœ°å¡«å……å…¶ä½™çš„èº«ä½“åŠ¨ä½œï¼ŒåŒæ—¶ä¸éŸ³é¢‘åŒæ­¥ã€‚
            </div>
          </li>
        </ul>
        
        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">High-Level Insights: Why it Works?</h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                EMAGE works by treating gesture generation as a "completion" problem. During training, it learns to predict missing body parts from audio and visible parts. This allows it to learn strong priors about how body parts coordinate. By separately encoding "rhythm" and "content" from speech, it ensures that gestures beat in time with the audio while also reflecting the semantic meaning.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                EMAGE çš„å·¥ä½œåŸç†æ˜¯å°†æ‰‹åŠ¿ç”Ÿæˆè§†ä¸ºä¸€ä¸ªâ€œè¡¥å…¨â€é—®é¢˜ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œå®ƒå­¦ä¹ ä»éŸ³é¢‘å’Œå¯è§éƒ¨åˆ†é¢„æµ‹ç¼ºå¤±çš„èº«ä½“éƒ¨ä½ã€‚è¿™ä½¿å®ƒèƒ½å¤Ÿå­¦ä¹ å…³äºèº«ä½“éƒ¨ä½å¦‚ä½•åè°ƒçš„å¼ºå¤§å…ˆéªŒã€‚é€šè¿‡åˆ†åˆ«ä»è¯­éŸ³ä¸­ç¼–ç â€œèŠ‚å¥â€å’Œâ€œå†…å®¹â€ï¼Œå®ƒç¡®ä¿æ‰‹åŠ¿åœ¨æ—¶é—´ä¸Šä¸éŸ³é¢‘ä¸€è‡´ï¼ŒåŒæ—¶ä¹Ÿåæ˜ è¯­ä¹‰ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://github.com/PantoMatrix/PantoMatrix" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          <svg height="16" viewBox="0 0 16 16" width="16" style="fill:currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
          GitHub Repo
        </a>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="font-size:13px;line-height:1.6;color:rgba(232,236,255,.80);">
            <div class="lang-en">
                Conceptual masked gesture modeling.
            </div>
            <div class="lang-zh" style="display:none">
                æ¦‚å¿µæ©ç æ‰‹åŠ¿å»ºæ¨¡ã€‚
            </div>
        </div>
        
        <div style="position:relative; margin-top:12px; background:rgba(0,0,0,.3); border:1px solid rgba(255,255,255,.1); border-radius:8px; padding:12px; overflow-x:auto;">
<pre style="margin:0; font-family:Menlo,Consolas,monospace; font-size:12px; color:#d4d4d4;">
<span style="color:#6a9955;"># Masked Audio-Gesture Transformer</span>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(audio, partial_gesture, mask):
    <span style="color:#6a9955;"># 1. Encode Audio Features (Rhythm + Content)</span>
    audio_feat = audio_encoder(audio)
    
    <span style="color:#6a9955;"># 2. Apply Mask to Gesture</span>
    masked_input = partial_gesture * mask
    
    <span style="color:#6a9955;"># 3. Predict Full Gesture from Audio + Masked Input</span>
    <span style="color:#6a9955;"># The model learns to "inpaint" the missing motion based on audio context</span>
    pred_gesture = transformer(audio_feat, masked_input)
    
    <span style="color:#569cd6;">return</span> pred_gesture
</pre>
        </div>
      </div>
    </div>
  </section>
  <script>
    function toggleLang(btn) {
      const container = btn.closest('div');
      const enElements = container.querySelectorAll('.lang-en');
      const zhElements = container.querySelectorAll('.lang-zh');
      
      enElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
      
      zhElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
    }

    function openLocalPdf(filename) {
      window.open('pdfs/' + filename, '_blank');
    }

    function delLocalPdf(filename) {
      // User will implement this themselves
      alert("Please implement the delete logic or delete the file manually: pdfs/" + filename);
    }
  </script>
</body>
</html>