<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ Sensors Journal 2023
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">An Experimental Platform for Real-Time Students Engagement Measurements from Video in STEM Classrooms</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Islam Alkabbany, Asem M Ali, Chris Foreman, Thomas Tretter, Nicholas Hindy, Aly Farag<br>
        <span style="opacity:0.8">University of Louisville, USA</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can biometric sensor networks capture real-time behavioral and emotional engagement of students in STEM classrooms to enable timely educational interventions?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åˆ©ç”¨ç”Ÿç‰©è¯†åˆ«ä¼ æ„Ÿå™¨ç½‘ç»œæ•è·STEMè¯¾å ‚ä¸­å­¦ç”Ÿè¡Œä¸ºçš„å®æ—¶å‚ä¸åº¦å’Œæƒ…æ„Ÿå‚ä¸åº¦ï¼Œä»¥å®ç°åŠæ—¶çš„æ•™è‚²å¹²é¢„ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Real-Time Engagement Platform:</b> Developed a biometric sensor network (BSN) with web cameras, wall-mounted camera, and HPC for capturing head poses, eye gaze, body movements, and facial emotions in real-time STEM classroom environments.</div>
            <div class="lang-zh" style="display:none"><b>å®æ—¶å‚ä¸åº¦å¹³å°ï¼š</b>å¼€å‘äº†ç”Ÿç‰©è¯†åˆ«ä¼ æ„Ÿå™¨ç½‘ç»œï¼ˆBSNï¼‰ï¼Œé…å¤‡ç½‘ç»œæ‘„åƒå¤´ã€å¢™å£å®‰è£…æ‘„åƒå¤´å’Œé«˜æ€§èƒ½è®¡ç®—ï¼Œç”¨äºå®æ—¶æ•è·STEMè¯¾å ‚ç¯å¢ƒä¸­å­¦ç”Ÿçš„å¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€èº«ä½“è¿åŠ¨å’Œé¢éƒ¨æƒ…ç»ªã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Dual Engagement Components:</b> Comprehensive framework measuring both behavioral engagement (head pose, eye gaze, body movements) and emotional engagement (facial expressions) using AI-based models trained on low-level biometric features.</div>
            <div class="lang-zh" style="display:none"><b>åŒé‡å‚ä¸åº¦ç»„ä»¶ï¼š</b>å…¨é¢æ¡†æ¶æµ‹é‡è¡Œä¸ºå‚ä¸åº¦ï¼ˆå¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€èº«ä½“è¿åŠ¨ï¼‰å’Œæƒ…æ„Ÿå‚ä¸åº¦ï¼ˆé¢éƒ¨è¡¨æƒ…ï¼‰ï¼Œä½¿ç”¨åŸºäºAIçš„æ¨¡å‹å¯¹ä½çº§ç”Ÿç‰©è¯†åˆ«ç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Superior Performance:</b> Demonstrated better accuracy than state-of-the-art frameworks in estimating both behavioral and emotional engagement, with enhanced flexibility for deployment in any educational environment.</div>
            <div class="lang-zh" style="display:none"><b>å“è¶Šæ€§èƒ½ï¼š</b>åœ¨ä¼°è®¡è¡Œä¸ºå’Œæƒ…æ„Ÿå‚ä¸åº¦æ–¹é¢å±•ç¤ºäº†æ¯”ç°æœ‰æœ€å…ˆè¿›æ¡†æ¶æ›´å¥½çš„å‡†ç¡®æ€§ï¼Œå¹¶å…·æœ‰åœ¨ä»»ä½•æ•™è‚²ç¯å¢ƒä¸­éƒ¨ç½²çš„å¢å¼ºçµæ´»æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Quantitative Teaching Comparison:</b> Enabled quantitative comparison of different teaching methods through objective engagement measurements, facilitating evidence-based educational interventions.</div>
            <div class="lang-zh" style="display:none"><b>é‡åŒ–æ•™å­¦æ¯”è¾ƒï¼š</b>é€šè¿‡å®¢è§‚å‚ä¸åº¦æµ‹é‡å®ç°äº†ä¸åŒæ•™å­¦æ–¹æ³•çš„é‡åŒ–æ¯”è¾ƒï¼Œä¿ƒè¿›åŸºäºè¯æ®çš„æ•™è‚²å¹²é¢„ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Real-Time Processing:</b> Capturing and processing multiple biometric signals (head pose, eye gaze, facial expressions, body movements) simultaneously requires high-performance computing infrastructure for real-time analysis in classroom settings.</div>
            <div class="lang-zh" style="display:none"><b>å®æ—¶å¤„ç†ï¼š</b>åŒæ—¶æ•è·å’Œå¤„ç†å¤šä¸ªç”Ÿç‰©è¯†åˆ«ä¿¡å·ï¼ˆå¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€é¢éƒ¨è¡¨æƒ…ã€èº«ä½“è¿åŠ¨ï¼‰éœ€è¦åœ¨è¯¾å ‚ç¯å¢ƒä¸­è¿›è¡Œå®æ—¶åˆ†æçš„é«˜æ€§èƒ½è®¡ç®—åŸºç¡€è®¾æ–½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Modal Integration:</b> Combining behavioral (physical movements) and emotional (facial expressions) engagement components requires sophisticated fusion techniques to create unified engagement measurements.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€æ•´åˆï¼š</b>ç»“åˆè¡Œä¸ºï¼ˆèº«ä½“è¿åŠ¨ï¼‰å’Œæƒ…æ„Ÿï¼ˆé¢éƒ¨è¡¨æƒ…ï¼‰å‚ä¸åº¦ç»„ä»¶éœ€è¦å¤æ‚çš„èåˆæŠ€æœ¯æ¥åˆ›å»ºç»Ÿä¸€çš„å‚ä¸åº¦æµ‹é‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Privacy and Ethics:</b> Continuous video monitoring of students raises significant privacy concerns and requires careful consideration of data protection, consent, and ethical deployment in educational environments.</div>
            <div class="lang-zh" style="display:none"><b>éšç§å’Œä¼¦ç†ï¼š</b>å¯¹å­¦ç”Ÿçš„æŒç»­è§†é¢‘ç›‘æ§å¼•å‘äº†é‡å¤§éšç§é—®é¢˜ï¼Œéœ€è¦ä»”ç»†è€ƒè™‘æ•™è‚²ç¯å¢ƒä¸­çš„æ•°æ®ä¿æŠ¤ã€åŒæ„å’Œé“å¾·éƒ¨ç½²ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Environmental Adaptability:</b> Classroom environments vary significantly in lighting, camera angles, student positioning, and group sizes, requiring robust computer vision algorithms that work across diverse educational settings.</div>
            <div class="lang-zh" style="display:none"><b>ç¯å¢ƒé€‚åº”æ€§ï¼š</b>è¯¾å ‚ç¯å¢ƒåœ¨ç…§æ˜ã€æ‘„åƒå¤´è§’åº¦ã€å­¦ç”Ÿä½ç½®å’Œå°ç»„è§„æ¨¡æ–¹é¢å·®å¼‚å¾ˆå¤§ï¼Œéœ€è¦åœ¨ä¸åŒæ•™è‚²ç¯å¢ƒä¸­å·¥ä½œçš„å¼ºå¤§è®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The paper presents a comprehensive biometric sensor network approach for measuring student engagement:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Biometric Sensor Network:</b> Deployed web cameras for individual student monitoring, wall-mounted camera for group behavior capture, and HPC for real-time processing of head poses, eye gaze, body movements, and facial emotions.</li>
                <li style="margin-bottom:6px;"><b>Behavioral Engagement Features:</b> Extracted head pose angles (pitch, yaw, roll), eye gaze directions, and body movement patterns using computer vision algorithms to quantify physical engagement indicators.</li>
                <li style="margin-bottom:6px;"><b>Emotional Engagement Features:</b> Analyzed facial expressions through facial Action Units (AUs) and emotion recognition to measure affective states like happiness, sadness, anger, and engagement-related emotions.</li>
                <li style="margin-bottom:6px;"><b>AI-Based Engagement Model:</b> Trained machine learning models on low-level biometric features to estimate both behavioral and emotional engagement levels, enabling real-time classification and quantification.</li>
                <li style="margin-bottom:6px;"><b>Comparative Evaluation:</b> Conducted experiments comparing the proposed framework with state-of-the-art methods, demonstrating superior accuracy and flexibility across different educational environments.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„ç”Ÿç‰©è¯†åˆ«ä¼ æ„Ÿå™¨ç½‘ç»œæ–¹æ³•æ¥æµ‹é‡å­¦ç”Ÿå‚ä¸åº¦ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>ç”Ÿç‰©è¯†åˆ«ä¼ æ„Ÿå™¨ç½‘ç»œï¼š</b>éƒ¨ç½²ç½‘ç»œæ‘„åƒå¤´ç”¨äºä¸ªåˆ«å­¦ç”Ÿç›‘æ§ã€å¢™å£å®‰è£…æ‘„åƒå¤´ç”¨äºç¾¤ä½“è¡Œä¸ºæ•è·ï¼Œä»¥åŠHPCç”¨äºå®æ—¶å¤„ç†å¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€èº«ä½“è¿åŠ¨å’Œé¢éƒ¨æƒ…ç»ªã€‚</li>
                <li style="margin-bottom:6px;"><b>è¡Œä¸ºå‚ä¸åº¦ç‰¹å¾ï¼š</b>ä½¿ç”¨è®¡ç®—æœºè§†è§‰ç®—æ³•æå–å¤´éƒ¨å§¿åŠ¿è§’åº¦ï¼ˆä¿¯ä»°è§’ã€åèˆªè§’ã€æ»šè½¬è§’ï¼‰ã€çœ¼éƒ¨æ³¨è§†æ–¹å‘å’Œèº«ä½“è¿åŠ¨æ¨¡å¼ï¼Œä»¥é‡åŒ–èº«ä½“å‚ä¸åº¦æŒ‡æ ‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>æƒ…æ„Ÿå‚ä¸åº¦ç‰¹å¾ï¼š</b>é€šè¿‡é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰å’Œæƒ…ç»ªè¯†åˆ«åˆ†æé¢éƒ¨è¡¨æƒ…ï¼Œä»¥æµ‹é‡å¹¸ç¦ã€æ‚²ä¼¤ã€æ„¤æ€’ç­‰æƒ…æ„ŸçŠ¶æ€ä»¥åŠä¸å‚ä¸åº¦ç›¸å…³çš„å…¶ä»–æƒ…ç»ªã€‚</li>
                <li style="margin-bottom:6px;"><b>åŸºäºAIçš„å‚ä¸åº¦æ¨¡å‹ï¼š</b>åœ¨ä½çº§ç”Ÿç‰©è¯†åˆ«ç‰¹å¾ä¸Šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¼°è®¡è¡Œä¸ºå’Œæƒ…æ„Ÿå‚ä¸åº¦æ°´å¹³ï¼Œå®ç°å®æ—¶åˆ†ç±»å’Œé‡åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¯”è¾ƒè¯„ä¼°ï¼š</b>è¿›è¡Œå®éªŒæ¯”è¾ƒæ‰€æå‡ºçš„æ¡†æ¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨ä¸åŒæ•™è‚²ç¯å¢ƒä¸­çš„å“è¶Šå‡†ç¡®æ€§å’Œçµæ´»æ€§ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/stem_engagement_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <hr style="border-top: 1px dashed rgba(255,255,255,.1); margin: 20px 0;">
            <img src="Figures/stem_engagement_intro02.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

                 <button onclick="openLocalPdf('Real-Time_Students.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('Real-Time_Students.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>
    
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/stem_emgagement_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            
            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('Real-Time_Students.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('Real-Time_Students.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>


      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of real-time student engagement measurement in STEM classrooms stems from several key technological and methodological innovations:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Comprehensive Biometric Capture:</b> Multi-camera setup with web cameras for individual monitoring and wall-mounted cameras for contextual awareness enables holistic capture of both individual and group-level engagement signals.</li>
                <li style="margin-bottom:6px;"><b>Dual Engagement Framework:</b> Separating behavioral (observable actions) and emotional (internal states) engagement components provides richer understanding than single-measurement approaches, allowing nuanced assessment of student participation.</li>
                <li style="margin-bottom:6px;"><b>Real-Time Processing Capability:</b> HPC infrastructure enables immediate analysis and feedback, crucial for timely educational interventions during active learning sessions rather than post-hoc analysis.</li>
                <li style="margin-bottom:6px;"><b>AI-Driven Feature Integration:</b> Machine learning models effectively fuse low-level biometric signals (head pose, eye gaze, facial expressions) into high-level engagement estimates, learning complex patterns that manual observation cannot detect.</li>
                <li style="margin-bottom:6px;"><b>Educational Flexibility:</b> Platform adaptability to different classroom configurations and teaching methods enables comparative evaluation of pedagogical approaches, supporting evidence-based educational improvements.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>STEMè¯¾å ‚ä¸­å®æ—¶å­¦ç”Ÿå‚ä¸åº¦æµ‹é‡çš„æˆåŠŸæºäºå‡ ä¸ªå…³é”®çš„æŠ€æœ¯å’Œæ–¹æ³•åˆ›æ–°ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>å…¨é¢ç”Ÿç‰©è¯†åˆ«æ•è·ï¼š</b>å¤šæ‘„åƒå¤´è®¾ç½®é…å¤‡ç½‘ç»œæ‘„åƒå¤´ç”¨äºä¸ªäººç›‘æ§å’Œå¢™å£å®‰è£…æ‘„åƒå¤´ç”¨äºæƒ…å¢ƒæ„è¯†ï¼Œèƒ½å¤Ÿå…¨é¢æ•è·ä¸ªäººå’Œç¾¤ä½“æ°´å¹³çš„å‚ä¸åº¦ä¿¡å·ã€‚</li>
                <li style="margin-bottom:6px;"><b>åŒé‡å‚ä¸åº¦æ¡†æ¶ï¼š</b>åˆ†ç¦»è¡Œä¸ºï¼ˆå¯è§‚å¯ŸåŠ¨ä½œï¼‰å’Œæƒ…æ„Ÿï¼ˆå†…éƒ¨çŠ¶æ€ï¼‰å‚ä¸åº¦ç»„ä»¶æä¾›äº†æ¯”å•ä¸€æµ‹é‡æ–¹æ³•æ›´ä¸°å¯Œçš„ç†è§£ï¼Œå…è®¸å¯¹å­¦ç”Ÿå‚ä¸è¿›è¡Œç»†å¾®è¯„ä¼°ã€‚</li>
                <li style="margin-bottom:6px;"><b>å®æ—¶å¤„ç†èƒ½åŠ›ï¼š</b>HPCåŸºç¡€è®¾æ–½å®ç°äº†å³æ—¶åˆ†æå’Œåé¦ˆï¼Œå¯¹äºåœ¨ä¸»åŠ¨å­¦ä¹ è¯¾ç¨‹æœŸé—´è¿›è¡ŒåŠæ—¶æ•™è‚²å¹²é¢„è‡³å…³é‡è¦ï¼Œè€Œä¸æ˜¯äº‹ååˆ†æã€‚</li>
                <li style="margin-bottom:6px;"><b>AIé©±åŠ¨çš„ç‰¹å¾æ•´åˆï¼š</b>æœºå™¨å­¦ä¹ æ¨¡å‹æœ‰æ•ˆåœ°å°†ä½çº§ç”Ÿç‰©è¯†åˆ«ä¿¡å·ï¼ˆå¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€é¢éƒ¨è¡¨æƒ…ï¼‰èåˆåˆ°é«˜çº§å‚ä¸åº¦ä¼°è®¡ä¸­ï¼Œå­¦ä¹ æ‰‹åŠ¨è§‚å¯Ÿæ— æ³•æ£€æµ‹çš„å¤æ‚æ¨¡å¼ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ•™è‚²çµæ´»æ€§ï¼š</b>å¹³å°å¯¹ä¸åŒè¯¾å ‚é…ç½®å’Œæ•™å­¦æ–¹æ³•çš„é€‚åº”æ€§èƒ½å¤Ÿè¿›è¡Œæ•™å­¦æ–¹æ³•çš„æ¯”è¾ƒè¯„ä¼°ï¼Œæ”¯æŒåŸºäºè¯æ®çš„æ•™è‚²æ”¹è¿›ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This work establishes a groundbreaking real-time platform for measuring student engagement in STEM classrooms through comprehensive biometric sensor networks. By integrating behavioral and emotional engagement components with advanced AI processing, the system provides educators with objective, quantitative measures that were previously unavailable. The superior accuracy and flexibility demonstrated in comparative experiments validate the approach's potential for widespread adoption in educational settings. Beyond immediate engagement monitoring, the platform enables evidence-based comparison of teaching methods and facilitates timely interventions to improve learning outcomes. The integration of multiple biometric signals creates a holistic understanding of student participation that goes beyond traditional observation methods, opening new possibilities for personalized and adaptive educational environments. This research addresses critical challenges in STEM education retention and performance, particularly for at-risk student populations, by providing tools for early identification and intervention.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å·¥ä½œé€šè¿‡å…¨é¢çš„ç”Ÿç‰©è¯†åˆ«ä¼ æ„Ÿå™¨ç½‘ç»œå»ºç«‹äº†STEMè¯¾å ‚ä¸­æµ‹é‡å­¦ç”Ÿå‚ä¸åº¦çš„å¼€åˆ›æ€§å®æ—¶å¹³å°ã€‚é€šè¿‡å°†è¡Œä¸ºå’Œæƒ…æ„Ÿå‚ä¸åº¦ç»„ä»¶ä¸å…ˆè¿›çš„AIå¤„ç†ç›¸ç»“åˆï¼Œè¯¥ç³»ç»Ÿä¸ºæ•™è‚²å·¥ä½œè€…æä¾›äº†ä»¥å‰æ— æ³•è·å¾—çš„å®¢è§‚é‡åŒ–æµ‹é‡ã€‚åœ¨æ¯”è¾ƒå®éªŒä¸­å±•ç¤ºçš„å“è¶Šå‡†ç¡®æ€§å’Œçµæ´»æ€§éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ•™è‚²ç¯å¢ƒä¸­å¹¿æ³›åº”ç”¨çš„æ½œåŠ›ã€‚é™¤äº†ç›´æ¥å‚ä¸åº¦ç›‘æ§ï¼Œè¯¥å¹³å°è¿˜å®ç°äº†åŸºäºè¯æ®çš„æ•™å­¦æ–¹æ³•æ¯”è¾ƒï¼Œå¹¶ä¿ƒè¿›åŠæ—¶å¹²é¢„ä»¥æ”¹å–„å­¦ä¹ æˆæœã€‚å¤šä¸ªç”Ÿç‰©è¯†åˆ«ä¿¡å·çš„æ•´åˆåˆ›é€ äº†è¶…è¶Šä¼ ç»Ÿè§‚å¯Ÿæ–¹æ³•çš„å¯¹å­¦ç”Ÿå‚ä¸çš„æ•´ä½“ç†è§£ï¼Œä¸ºä¸ªæ€§åŒ–è‡ªé€‚åº”æ•™è‚²ç¯å¢ƒå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚è¿™é¡¹ç ”ç©¶è§£å†³äº†STEMæ•™è‚²ä¿ç•™å’Œè¡¨ç°çš„å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é«˜é£é™©å­¦ç”Ÿç¾¤ä½“ï¼Œé€šè¿‡æä¾›æ—©æœŸè¯†åˆ«å’Œå¹²é¢„çš„å·¥å…·ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>DOI:</b> <a href="https://doi.org/10.3390/s23031614" style="color:#8bffcf;">10.3390/s23031614</a></p>
          <p><b>PMC:</b> <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9919426/" style="color:#8bffcf;">PMC9919426</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import cv2<br>
import numpy as np<br>
import torch<br>
import torch.nn as nn<br>
from facenet_pytorch import MTCNN<br>
import mediapipe as mp<br>
<br>
// Biometric Sensor Network for Student Engagement<br>
class BiometricSensorNetwork:<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.face_detector = MTCNN()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.pose_estimator = mp.solutions.pose.Pose()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.face_mesh = mp.solutions.face_mesh.FaceMesh()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.engagement_model = EngagementClassifier()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def process_frame(self, frame):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Process single frame for engagement features"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;faces = self.face_detector.detect(frame)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pose_results = self.pose_estimator.process(frame)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;face_results = self.face_mesh.process(frame)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;behavioral_features = self._extract_behavioral_features(faces, pose_results)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotional_features = self._extract_emotional_features(face_results)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return behavioral_features, emotional_features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _extract_behavioral_features(self, faces, pose_results):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Extract head pose, eye gaze, body movements"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = {}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if faces[0] is not None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Head pose estimation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;landmarks = faces[1][0]  # Facial landmarks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features['head_pose'] = self._calculate_head_pose(landmarks)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features['eye_gaze'] = self._calculate_eye_gaze(landmarks)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if pose_results.pose_landmarks:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Body movement features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features['body_movement'] = self._calculate_body_movement(pose_results.pose_landmarks)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _extract_emotional_features(self, face_results):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Extract facial expression features"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if face_results.multi_face_landmarks:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;landmarks = face_results.multi_face_landmarks[0]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self._calculate_facial_action_units(landmarks)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return {}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _calculate_head_pose(self, landmarks):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Calculate head pose angles (pitch, yaw, roll)"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Simplified implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return {'pitch': 0.0, 'yaw': 0.0, 'roll': 0.0}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _calculate_eye_gaze(self, landmarks):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Calculate eye gaze direction"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return {'horizontal': 0.0, 'vertical': 0.0}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _calculate_body_movement(self, pose_landmarks):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Calculate body movement features"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return {'movement_intensity': 0.0}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _calculate_facial_action_units(self, landmarks):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Calculate Facial Action Units"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Simplified AU calculation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return {'AU1': 0.0, 'AU2': 0.0, 'AU4': 0.0, 'AU6': 0.0, 'AU12': 0.0}<br>
<br>
// Dual Engagement Classifier<br>
class EngagementClassifier(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, behavioral_dim=10, emotional_dim=15):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(EngagementClassifier, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Behavioral engagement network<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.behavioral_net = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(behavioral_dim, 64),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(64, 32),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(32, 1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sigmoid()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Emotional engagement network<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.emotional_net = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(emotional_dim, 64),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(64, 32),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(32, 1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sigmoid()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, behavioral_features, emotional_features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;behavioral_engagement = self.behavioral_net(behavioral_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotional_engagement = self.emotional_net(emotional_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return behavioral_engagement, emotional_engagement<br>
<br>
// Real-time Processing Pipeline<br>
class RealTimeEngagementMonitor:<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bsn = BiometricSensorNetwork()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.cap = None<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.is_running = False<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def start_monitoring(self, camera_id=0):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.cap = cv2.VideoCapture(camera_id)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.is_running = True<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while self.is_running:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ret, frame = self.cap.read()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if ret:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;behavioral, emotional = self.bsn.process_frame(frame)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;engagement_scores = self.bsn.engagement_model(behavioral, emotional)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Real-time feedback<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self._provide_feedback(engagement_scores)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if cv2.waitKey(1) & 0xFF == ord('q'):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.cap.release()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv2.destroyAllWindows()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _provide_feedback(self, engagement_scores):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Provide real-time feedback to educators"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;behavioral_score, emotional_score = engagement_scores<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;overall_engagement = (behavioral_score + emotional_score) / 2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Real-time Engagement - Behavioral: {behavioral_score:.3f}, Emotional: {emotional_score:.3f}, Overall: {overall_engagement:.3f}")<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
