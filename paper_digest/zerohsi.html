<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ 3DV 2026
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu<br>
        <span style="opacity:0.8">Stanford University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to synthesize realistic human motions that interact with 3D environments in unseen scenes without requiring paired motion-scene training data?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åœ¨æœªè§åœºæ™¯ä¸­åˆæˆä¸3Dç¯å¢ƒäº¤äº’çš„çœŸå®äººç±»è¿åŠ¨ï¼Œè€Œæ— éœ€é…å¯¹çš„è¿åŠ¨-åœºæ™¯è®­ç»ƒæ•°æ®ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>ZeroHSI Framework:</b> Pioneering zero-shot 4D human-scene interaction synthesis that distills interactions from video generation models and uses differentiable rendering for motion reconstruction, eliminating the need for MoCap training data.</div>
            <div class="lang-zh" style="display:none"><b>ZeroHSIæ¡†æ¶ï¼š</b>å¼€åˆ›æ€§çš„é›¶æ ·æœ¬4Däººç±»-åœºæ™¯äº¤äº’åˆæˆï¼Œä»è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­æå–äº¤äº’ï¼Œå¹¶ä½¿ç”¨å¯å¾®åˆ†æ¸²æŸ“è¿›è¡Œè¿åŠ¨é‡å»ºï¼Œæ¶ˆé™¤äº†å¯¹MoCapè®­ç»ƒæ•°æ®çš„éœ€è¦ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Video-to-4D Lifting:</b> Novel optimization-based approach integrating video generation with neural human rendering, enabling reconstruction of both human motions and object manipulations from generated 2D videos into coherent 4D interactions.</div>
            <div class="lang-zh" style="display:none"><b>è§†é¢‘åˆ°4Dæå‡ï¼š</b>æ–°é¢–çš„åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œå°†è§†é¢‘ç”Ÿæˆä¸ç¥ç»äººç±»æ¸²æŸ“é›†æˆï¼Œå®ç°ä»ç”Ÿæˆçš„2Dè§†é¢‘é‡å»ºäººç±»è¿åŠ¨å’Œç‰©ä½“æ“ä½œåˆ°è¿è´¯çš„4Däº¤äº’ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Robust Optimization Pipeline:</b> Comprehensive optimization strategy with photometric, center point, and depth regularization losses, handling low-quality generated videos through techniques for incorrect content removal, body part disappearance, and appearance changes.</div>
            <div class="lang-zh" style="display:none"><b>é²æ£’ä¼˜åŒ–ç®¡é“ï¼š</b>å…¨é¢çš„ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬å…‰åº¦å­¦ã€ä¸­å¿ƒç‚¹å’Œæ·±åº¦æ­£åˆ™åŒ–æŸå¤±ï¼Œé€šè¿‡æŠ€æœ¯å¤„ç†ä½è´¨é‡ç”Ÿæˆè§†é¢‘ï¼ŒåŒ…æ‹¬é”™è¯¯å†…å®¹ç§»é™¤ã€èº«ä½“éƒ¨ä½æ¶ˆå¤±å’Œå¤–è§‚å˜åŒ–ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Object Support:</b> Unified framework supporting both static environments and dynamic object manipulation, with dedicated optimization for object 6D poses and human-scene compatibility constraints.</div>
            <div class="lang-zh" style="display:none"><b>å¤šç‰©ä½“æ”¯æŒï¼š</b>ç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ”¯æŒé™æ€ç¯å¢ƒå’ŒåŠ¨æ€ç‰©ä½“æ“ä½œï¼Œå…·æœ‰é’ˆå¯¹ç‰©ä½“6Då§¿æ€å’Œäººç±»-åœºæ™¯å…¼å®¹æ€§çº¦æŸçš„ä¸“ç”¨ä¼˜åŒ–ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Extensive Evaluation:</b> Curated dataset with diverse indoor/outdoor scenes and interaction prompts, demonstrating contextual appropriateness across Mip-NeRF 360 and Tanks and Temples reconstructed scenes, with compatibility across multiple video generation models.</div>
            <div class="lang-zh" style="display:none"><b>å¹¿æ³›è¯„ä¼°ï¼š</b>ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å®¤å†…/å®¤å¤–åœºæ™¯å’Œäº¤äº’æç¤ºï¼Œå±•ç¤ºäº†åœ¨Mip-NeRF 360å’ŒTanks and Templesé‡å»ºåœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡é€‚å½“æ€§ï¼Œä»¥åŠè·¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å…¼å®¹æ€§ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/zerohsi_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2412.18600.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2412.18600.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/zerohsi_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2412.18600.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2412.18600.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Data Dependency:</b> Existing supervised approaches require paired 3D scenes and motion capture data for training, making them unable to synthesize interactions in unseen environments like in-the-wild or reconstructed scenes.</div>
            <div class="lang-zh" style="display:none"><b>æ•°æ®ä¾èµ–ï¼š</b>ç°æœ‰çš„ç›‘ç£æ–¹æ³•éœ€è¦é…å¯¹çš„3Dåœºæ™¯å’Œè¿åŠ¨æ•è·æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶æ— æ³•åœ¨æœªè§ç¯å¢ƒä¸­åˆæˆäº¤äº’ï¼Œå¦‚é‡å¤–æˆ–é‡å»ºåœºæ™¯ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Generalization Limitations:</b> Learning-based methods struggle with geometric variations and fail to handle diverse object categories and interaction types beyond their training distributions.</div>
            <div class="lang-zh" style="display:none"><b>æ³›åŒ–é™åˆ¶ï¼š</b>åŸºäºå­¦ä¹ çš„æ–¹æ³•éš¾ä»¥å¤„ç†å‡ ä½•å˜åŒ–ï¼Œæ— æ³•åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–å¤„ç†å¤šæ ·åŒ–çš„ç‰©ä½“ç±»åˆ«å’Œäº¤äº’ç±»å‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Dynamic Object Support:</b> Most methods focus on static environments and lack mechanisms for realistic manipulation of dynamic objects with proper physics and interaction constraints.</div>
            <div class="lang-zh" style="display:none"><b>åŠ¨æ€ç‰©ä½“æ”¯æŒï¼š</b>å¤§å¤šæ•°æ–¹æ³•ä¸“æ³¨äºé™æ€ç¯å¢ƒï¼Œç¼ºä¹å¯¹å…·æœ‰é€‚å½“ç‰©ç†å’Œäº¤äº’çº¦æŸçš„åŠ¨æ€ç‰©ä½“è¿›è¡ŒçœŸå®æ“ä½œçš„æœºåˆ¶ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Video-to-3D Reconstruction:</b> Lifting generated 2D videos to 3D motions requires handling camera pose estimation, object pose optimization, and maintaining temporal coherence under imperfect video generation quality.</div>
            <div class="lang-zh" style="display:none"><b>è§†é¢‘åˆ°3Dé‡å»ºï¼š</b>å°†ç”Ÿæˆçš„2Dè§†é¢‘æå‡åˆ°3Dè¿åŠ¨éœ€è¦å¤„ç†ç›¸æœºå§¿æ€ä¼°è®¡ã€ç‰©ä½“å§¿æ€ä¼˜åŒ–ï¼Œå¹¶åœ¨ä¸å®Œç¾çš„è§†é¢‘ç”Ÿæˆè´¨é‡ä¸‹ä¿æŒæ—¶é—´è¿è´¯æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Low-Quality Video Handling:</b> Video generation models produce artifacts like incorrect contents, body part disappearance, and appearance changes that require robust reconstruction techniques.</div>
            <div class="lang-zh" style="display:none"><b>ä½è´¨é‡è§†é¢‘å¤„ç†ï¼š</b>è§†é¢‘ç”Ÿæˆæ¨¡å‹äº§ç”Ÿé”™è¯¯å†…å®¹ã€èº«ä½“éƒ¨ä½æ¶ˆå¤±å’Œå¤–è§‚å˜åŒ–ç­‰ç‘•ç–µï¼Œéœ€è¦é²æ£’çš„é‡å»ºæŠ€æœ¯ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Video Generation Integration:</b> Leverages pretrained video generation models (KLING) conditioned on rendered initial frames and text prompts to generate contextually appropriate HSI videos with natural human-object interactions.</div>
            <div class="lang-zh" style="display:none"><b>è§†é¢‘ç”Ÿæˆé›†æˆï¼š</b>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆKLINGï¼‰ï¼Œä»¥æ¸²æŸ“çš„åˆå§‹å¸§å’Œæ–‡æœ¬æç¤ºä¸ºæ¡ä»¶ï¼Œç”Ÿæˆå…·æœ‰è‡ªç„¶äººç±»-ç‰©ä½“äº¤äº’çš„ä¸Šä¸‹æ–‡é€‚å½“çš„HSIè§†é¢‘ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Gaussian-based Representation:</b> Unified representation using 3D Gaussian Splatting for scenes, objects, and animatable humans, enabling differentiable rendering for optimization-based motion reconstruction.</div>
            <div class="lang-zh" style="display:none"><b>åŸºäºé«˜æ–¯è¡¨ç¤ºï¼š</b>ä½¿ç”¨3Dé«˜æ–¯æ³¼æº…å¯¹åœºæ™¯ã€ç‰©ä½“å’Œå¯åŠ¨ç”»äººç±»è¿›è¡Œç»Ÿä¸€è¡¨ç¤ºï¼Œå®ç°å¯å¾®åˆ†æ¸²æŸ“ä»¥è¿›è¡ŒåŸºäºä¼˜åŒ–çš„è¿åŠ¨é‡å»ºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Per-frame Optimization:</b> Sequential frame-by-frame optimization of camera poses, human SMPL-X parameters, and object 6D poses using photometric losses with center point and depth regularization for accurate 4D reconstruction.</div>
            <div class="lang-zh" style="display:none"><b>æ¯å¸§ä¼˜åŒ–ï¼š</b>ä½¿ç”¨å…‰åº¦æŸå¤±ä»¥åŠä¸­å¿ƒç‚¹å’Œæ·±åº¦æ­£åˆ™åŒ–ï¼Œå¯¹ç›¸æœºå§¿æ€ã€äººSMPL-Xå‚æ•°å’Œç‰©ä½“6Då§¿æ€è¿›è¡Œé¡ºåºçš„é€å¸§ä¼˜åŒ–ï¼Œä»¥å®ç°å‡†ç¡®çš„4Dé‡å»ºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Robust Video Processing:</b> Techniques for handling low-quality generated videos including dynamic foreground mask aggregation for incorrect content removal, visibility-based camera selection, and appearance change handling.</div>
            <div class="lang-zh" style="display:none"><b>é²æ£’è§†é¢‘å¤„ç†ï¼š</b>å¤„ç†ä½è´¨é‡ç”Ÿæˆè§†é¢‘çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬åŠ¨æ€å‰æ™¯æ©ç èšåˆä»¥ç§»é™¤é”™è¯¯å†…å®¹ã€åŸºäºå¯è§æ€§çš„ç›¸æœºé€‰æ‹©å’Œå¤–è§‚å˜åŒ–å¤„ç†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Refinement with Pose Priors:</b> Post-processing refinement in VPoser latent space with physics-based losses to enhance motion naturalness and physical plausibility, reducing artifacts from single-view reconstruction.</div>
            <div class="lang-zh" style="display:none"><b>ä½¿ç”¨å§¿æ€å…ˆéªŒçš„ä¼˜åŒ–ï¼š</b>åœ¨VPoseræ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåå¤„ç†ä¼˜åŒ–ï¼Œä½¿ç”¨åŸºäºç‰©ç†çš„æŸå¤±æ¥å¢å¼ºè¿åŠ¨è‡ªç„¶æ€§å’Œç‰©ç†åˆç†æ€§ï¼Œå‡å°‘å•è§†å›¾é‡å»ºçš„ç‘•ç–µã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Zero-Shot Paradigm Breakthrough:</b> ZeroHSI establishes a new paradigm for HSI synthesis by leveraging video generation models trained on vast internet data, enabling synthesis in arbitrary 3D environments without scene-specific training.
            </div>
            <div class="lang-zh" style="display:none">
                <b>é›¶æ ·æœ¬èŒƒå¼çªç ´ï¼š</b>ZeroHSIé€šè¿‡åˆ©ç”¨åœ¨å¤§é‡äº’è”ç½‘æ•°æ®ä¸Šè®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„HSIåˆæˆèŒƒå¼ï¼Œå®ç°æ— éœ€åœºæ™¯ç‰¹å®šè®­ç»ƒå³å¯åœ¨ä»»æ„3Dç¯å¢ƒä¸­è¿›è¡Œåˆæˆã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Video Generation as Knowledge Source:</b> Pretrained video generation models serve as rich knowledge repositories of human behaviors and interactions, providing a scalable alternative to expensive motion capture data collection.
            </div>
            <div class="lang-zh" style="display:none">
                <b>è§†é¢‘ç”Ÿæˆä¸ºçŸ¥è¯†æ¥æºï¼š</b>é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸ºäººç±»è¡Œä¸ºå’Œäº¤äº’çš„ä¸°å¯ŒçŸ¥è¯†åº“ï¼Œä¸ºæ˜‚è´µçš„è¿åŠ¨æ•è·æ•°æ®æ”¶é›†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Unified Optimization Framework:</b> The combination of differentiable rendering and multi-term optimization losses creates a general framework for lifting 2D video content to coherent 4D spatio-temporal interactions.
            </div>
            <div class="lang-zh" style="display:none">
                <b>ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶ï¼š</b>å¯å¾®åˆ†æ¸²æŸ“å’Œå¤šé¡¹ä¼˜åŒ–æŸå¤±çš„ç»“åˆä¸ºå°†2Dè§†é¢‘å†…å®¹æå‡åˆ°è¿è´¯çš„4Dæ—¶ç©ºäº¤äº’åˆ›å»ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ã€‚
            </div>
          </li>
        </ul>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because ZeroHSI bridges the gap between generative 2D video capabilities and 3D motion reconstruction through principled optimization. By leveraging the semantic understanding embedded in video generation models and using differentiable rendering as the connection mechanism, the approach transforms the challenging zero-shot problem into a tractable optimization task that respects both visual fidelity and physical constraints.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºZeroHSIé€šè¿‡åŸç†æ€§çš„ä¼˜åŒ–å¼¥åˆäº†ç”Ÿæˆ2Dè§†é¢‘èƒ½åŠ›å’Œ3Dè¿åŠ¨é‡å»ºä¹‹é—´çš„å·®è·ã€‚é€šè¿‡åˆ©ç”¨åµŒå…¥åœ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„è¯­ä¹‰ç†è§£ï¼Œå¹¶ä½¿ç”¨å¯å¾®åˆ†æ¸²æŸ“ä½œä¸ºè¿æ¥æœºåˆ¶ï¼Œè¯¥æ–¹æ³•å°†å…·æœ‰æŒ‘æˆ˜æ€§çš„é›¶æ ·æœ¬é—®é¢˜è½¬æ¢ä¸ºå°Šé‡è§†è§‰ä¿çœŸåº¦å’Œç‰©ç†çº¦æŸçš„æ˜“å¤„ç†ä¼˜åŒ–ä»»åŠ¡ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            This paper introduces ZeroHSI, a novel zero-shot approach for 4D human-scene interaction synthesis that eliminates the need for paired 3D scene and motion capture training data. By integrating video generation with differentiable neural rendering, ZeroHSI distills human-scene interactions from pretrained video models and reconstructs them into coherent 4D motions supporting both static environments and dynamic object manipulations. The framework demonstrates robust performance across diverse indoor and outdoor scenes from Mip-NeRF 360 and Tanks and Temples datasets, generating contextually appropriate interactions for activities ranging from walking and sitting to watering plants and playing instruments. Extensive ablation studies validate the effectiveness of the optimization components, while compatibility with multiple video generation models ensures adaptability to future advancements. ZeroHSI establishes a new paradigm for data-efficient HSI synthesis that leverages the rich behavioral knowledge embedded in large-scale video generation models.
          </div>
          <div class="lang-zh" style="display:none">
            æœ¬æ–‡ä»‹ç»äº†ZeroHSIï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œç”¨äº4Däººç±»-åœºæ™¯äº¤äº’åˆæˆï¼Œæ¶ˆé™¤äº†å¯¹é…å¯¹3Dåœºæ™¯å’Œè¿åŠ¨æ•è·è®­ç»ƒæ•°æ®çš„éœ€è¦ã€‚é€šè¿‡å°†è§†é¢‘ç”Ÿæˆä¸å¯å¾®åˆ†ç¥ç»æ¸²æŸ“é›†æˆï¼ŒZeroHSIä»é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ä¸­æå–äººç±»-åœºæ™¯äº¤äº’ï¼Œå¹¶å°†å®ƒä»¬é‡å»ºä¸ºæ”¯æŒé™æ€ç¯å¢ƒå’ŒåŠ¨æ€ç‰©ä½“æ“ä½œçš„è¿è´¯4Dè¿åŠ¨ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†åœ¨æ¥è‡ªMip-NeRF 360å’ŒTanks and Templesæ•°æ®é›†çš„å¤šæ ·åŒ–å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­çš„é²æ£’æ€§èƒ½ï¼Œä¸ºä»æ­¥è¡Œå’Œåä¸‹åˆ°æµ‡æ°´æ¤ç‰©å’Œæ¼”å¥ä¹å™¨çš„æ´»åŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡é€‚å½“çš„äº¤äº’ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†ä¼˜åŒ–ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å…¼å®¹æ€§ç¡®ä¿äº†å¯¹æœªæ¥è¿›å±•çš„é€‚åº”æ€§ã€‚ZeroHSIå»ºç«‹äº†ä¸€ä¸ªæ–°çš„èŒƒå¼ï¼Œç”¨äºæ•°æ®é«˜æ•ˆçš„HSIåˆæˆï¼Œåˆ©ç”¨åµŒå…¥åœ¨å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸°å¯Œè¡Œä¸ºçŸ¥è¯†ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://awfuact.github.io/zerohsi/" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Project Page
        </a>
      </div>
    </div>
</section>
</body>
</html>
