<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2018
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">World Models</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        David Ha, JÃ¼rgen Schmidhuber<br>
        <span style="opacity:0.8">Google Brain, IDSIA</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we build generative neural network models that learn compressed representations of reinforcement learning environments, enabling agents to train policies either from real environment interactions or entirely within hallucinated dreams generated by these world models?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•æ„å»ºç”Ÿæˆå¼ç¥ç»ç½‘ç»œæ¨¡å‹æ¥å­¦ä¹ å¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„å‹ç¼©è¡¨ç¤ºï¼Œä½¿ä»£ç†èƒ½å¤Ÿä»çœŸå®ç¯å¢ƒäº¤äº’ä¸­è®­ç»ƒç­–ç•¥ï¼Œæˆ–è€…å®Œå…¨åœ¨è¿™äº›ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„å¹»è§‰æ¢¦å¢ƒä¸­è®­ç»ƒï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>World Model Architecture:</b> Introduced a three-component agent architecture consisting of a VAE for visual compression (V), MDN-RNN for temporal prediction (M), and a simple linear controller (C) that can solve complex RL tasks using compressed representations.</div>
            <div class="lang-zh" style="display:none"><b>ä¸–ç•Œæ¨¡å‹æ¶æ„ï¼š</b>å¼•å…¥äº†ä¸€ä¸ªä¸‰ç»„ä»¶ä»£ç†æ¶æ„ï¼Œç”±ç”¨äºè§†è§‰å‹ç¼©çš„VAE (V)ã€ç”¨äºæ—¶é—´é¢„æµ‹çš„MDN-RNN (M)å’Œä¸€ä¸ªå¯ä»¥ä½¿ç”¨å‹ç¼©è¡¨ç¤ºè§£å†³å¤æ‚RLä»»åŠ¡çš„ç®€å•çº¿æ€§æ§åˆ¶å™¨ (C)ç»„æˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Dream-Based Training:</b> Demonstrated that agents can be trained entirely within hallucinated environments generated by their world models, achieving better-than-real performance by adjusting temperature parameters to make virtual environments more challenging.</div>
            <div class="lang-zh" style="display:none"><b>åŸºäºæ¢¦å¢ƒçš„è®­ç»ƒï¼š</b>è¯æ˜äº†ä»£ç†å¯ä»¥å®Œå…¨åœ¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„å¹»è§‰ç¯å¢ƒä¸­è®­ç»ƒï¼Œé€šè¿‡è°ƒæ•´æ¸©åº¦å‚æ•°ä½¿è™šæ‹Ÿç¯å¢ƒæ›´å…·æŒ‘æˆ˜æ€§ï¼Œä»è€Œå®ç°æ¯”çœŸå®ç¯å¢ƒæ›´å¥½çš„æ€§èƒ½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>State-of-the-Art Results:</b> Achieved 906Â±21 average score on CarRacing-v0 (solving the task) and 1092Â±556 on VizDoom Take Cover, significantly outperforming previous RL methods while using orders of magnitude fewer parameters.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›ç»“æœï¼š</b>åœ¨CarRacing-v0ä¸Šå®ç°äº†906Â±21çš„å¹³å‡åˆ†æ•°ï¼ˆè§£å†³äº†ä»»åŠ¡ï¼‰ï¼Œåœ¨VizDoom Take Coverä¸Šå®ç°äº†1092Â±556ï¼Œå¤§å¹…ä¼˜äºä»¥å‰çš„RLæ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ•°é‡å°‘å‡ ä¸ªæ•°é‡çº§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Efficient Learning:</b> Showed that separating world model training from policy optimization allows for more efficient credit assignment, with the large world model handling complex representations while evolution strategies optimize the small controller.</div>
            <div class="lang-zh" style="display:none"><b>é«˜æ•ˆå­¦ä¹ ï¼š</b>å±•ç¤ºäº†å°†ä¸–ç•Œæ¨¡å‹è®­ç»ƒä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»å…è®¸æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ï¼Œå¤§å‹ä¸–ç•Œæ¨¡å‹å¤„ç†å¤æ‚è¡¨ç¤ºï¼Œè€Œè¿›åŒ–ç­–ç•¥ä¼˜åŒ–å°å‹æ§åˆ¶å™¨ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Model-Based RL Complexity:</b> Traditional model-based RL approaches struggle with credit assignment problems when training large neural networks to predict both world dynamics and optimal policies simultaneously.</div>
            <div class="lang-zh" style="display:none"><b>åŸºäºæ¨¡å‹çš„RLå¤æ‚æ€§ï¼š</b>ä¼ ç»Ÿçš„åŸºäºæ¨¡å‹çš„RLæ–¹æ³•åœ¨åŒæ—¶è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œæ¥é¢„æµ‹ä¸–ç•ŒåŠ¨æ€å’Œæœ€ä¼˜ç­–ç•¥æ—¶ä¼šé‡åˆ°ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Representation Learning:</b> Learning compressed spatial and temporal representations of high-dimensional environments requires careful design of generative models that can capture essential features while discarding irrelevant details.</div>
            <div class="lang-zh" style="display:none"><b>è¡¨ç¤ºå­¦ä¹ ï¼š</b>å­¦ä¹ é«˜ç»´ç¯å¢ƒçš„å‹ç¼©ç©ºé—´å’Œæ—¶é—´è¡¨ç¤ºéœ€è¦ç²¾å¿ƒè®¾è®¡ç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•è·åŸºæœ¬ç‰¹å¾åŒæ—¶ä¸¢å¼ƒæ— å…³ç»†èŠ‚ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Training Instability:</b> Separating world model training from policy optimization introduces challenges in ensuring the learned representations are suitable for downstream policy learning tasks.</div>
            <div class="lang-zh" style="display:none"><b>è®­ç»ƒä¸ç¨³å®šæ€§ï¼š</b>å°†ä¸–ç•Œæ¨¡å‹è®­ç»ƒä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»ä¼šåœ¨ç¡®ä¿å­¦ä¹ åˆ°çš„è¡¨ç¤ºé€‚åˆä¸‹æ¸¸ç­–ç•¥å­¦ä¹ ä»»åŠ¡æ–¹é¢å¸¦æ¥æŒ‘æˆ˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Environment Hallucination:</b> Training policies in hallucinated environments risks learning strategies that exploit model imperfections rather than learning robust behaviors that transfer to the real environment.</div>
            <div class="lang-zh" style="display:none"><b>ç¯å¢ƒå¹»è§‰ï¼š</b>åœ¨å¹»è§‰ç¯å¢ƒä¸­è®­ç»ƒç­–ç•¥å­˜åœ¨å­¦ä¹ åˆ©ç”¨æ¨¡å‹ç¼ºé™·çš„ç­–ç•¥çš„é£é™©ï¼Œè€Œä¸æ˜¯å­¦ä¹ èƒ½å¤Ÿè½¬ç§»åˆ°çœŸå®ç¯å¢ƒçš„é²æ£’è¡Œä¸ºã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The World Model approach decomposes agent learning into separate components that work together:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Visual Compression (V):</b> A VAE compresses high-dimensional pixel observations into low-dimensional latent vectors z, learning to reconstruct images while capturing essential visual features.</li>
                <li style="margin-bottom:6px;"><b>Temporal Prediction (M):</b> An MDN-RNN predicts future latent vectors z given current state, action, and hidden state, modeling environment dynamics as a mixture of Gaussian distributions.</li>
                <li style="margin-bottom:6px;"><b>Policy Controller (C):</b> A simple linear network maps latent representations and RNN hidden states to actions, optimized via evolution strategies to avoid credit assignment issues.</li>
                <li style="margin-bottom:6px;"><b>Dream Training:</b> Agents can train entirely within hallucinated environments generated by the world model, with temperature parameters controlling environment difficulty.</li>
                <li style="margin-bottom:6px;"><b>Two-Phase Learning:</b> First train world model components unsupervised on environment data, then optimize controller using features extracted from the trained world model.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>ä¸–ç•Œæ¨¡å‹æ–¹æ³•å°†ä»£ç†å­¦ä¹ åˆ†è§£ä¸ºååŒå·¥ä½œçš„ç‹¬ç«‹ç»„ä»¶ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>è§†è§‰å‹ç¼© (V)ï¼š</b>VAEå°†é«˜ç»´åƒç´ è§‚æµ‹å‹ç¼©ä¸ºä½ç»´æ½œåœ¨å‘é‡zï¼Œå­¦ä¹ é‡å»ºå›¾åƒåŒæ—¶æ•è·åŸºæœ¬è§†è§‰ç‰¹å¾ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ—¶é—´é¢„æµ‹ (M)ï¼š</b>MDN-RNNæ ¹æ®å½“å‰çŠ¶æ€ã€åŠ¨ä½œå’Œéšè—çŠ¶æ€é¢„æµ‹æœªæ¥çš„æ½œåœ¨å‘é‡zï¼Œå°†ç¯å¢ƒåŠ¨æ€å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒçš„æ··åˆã€‚</li>
                <li style="margin-bottom:6px;"><b>ç­–ç•¥æ§åˆ¶å™¨ (C)ï¼š</b>ä¸€ä¸ªç®€å•çš„çº¿æ€§ç½‘ç»œå°†æ½œåœ¨è¡¨ç¤ºå’ŒRNNéšè—çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œï¼Œé€šè¿‡è¿›åŒ–ç­–ç•¥ä¼˜åŒ–ä»¥é¿å…ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¢¦å¢ƒè®­ç»ƒï¼š</b>ä»£ç†å¯ä»¥å®Œå…¨åœ¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„å¹»è§‰ç¯å¢ƒä¸­è®­ç»ƒï¼Œæ¸©åº¦å‚æ•°æ§åˆ¶ç¯å¢ƒéš¾åº¦ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä¸¤é˜¶æ®µå­¦ä¹ ï¼š</b>é¦–å…ˆåœ¨ç¯å¢ƒæ•°æ®ä¸Šæ— ç›‘ç£è®­ç»ƒä¸–ç•Œæ¨¡å‹ç»„ä»¶ï¼Œç„¶åä½¿ç”¨ä»è®­ç»ƒå¥½çš„ä¸–ç•Œæ¨¡å‹ä¸­æå–çš„ç‰¹å¾ä¼˜åŒ–æ§åˆ¶å™¨ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€å›¾ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
            <img src="Figures/world_models_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/world_models_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p>World Models decompose agents into VAE for vision, MDN-RNN for memory, and linear controller, enabling training in both real and hallucinated environments.</p>
            <p>Note: The approach achieved state-of-the-art performance on CarRacing-v0 (906Â±21 score) and VizDoom Take Cover (1092Â±556 survival time), demonstrating the power of compressed representations and dream-based learning.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_world_models.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_world_models.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>World Models succeed by decomposing complex RL problems into manageable, learnable components:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Compression for Efficiency:</b> VAE compression reduces high-dimensional visual inputs to compact latent representations, enabling efficient learning and prediction of environment dynamics.</li>
                <li style="margin-bottom:6px;"><b>Predictive Memory:</b> MDN-RNN provides temporal context and future prediction capabilities, allowing agents to plan ahead and make informed decisions based on anticipated outcomes.</li>
                <li style="margin-bottom:6px;"><b>Simplified Optimization:</b> Separating world model learning from policy optimization avoids the credit assignment bottleneck, allowing evolution strategies to efficiently optimize simple controllers.</li>
                <li style="margin-bottom:6px;"><b>Dream Enhancement:</b> Training in hallucinated environments with adjustable difficulty enables agents to learn robust policies that perform better than those trained solely in the real environment.</li>
                <li style="margin-bottom:6px;"><b>Scalable Architecture:</b> The modular design allows independent scaling of components - complex world models can be paired with simple controllers, enabling efficient learning across diverse environments.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>ä¸–ç•Œæ¨¡å‹é€šè¿‡å°†å¤æ‚çš„RLé—®é¢˜åˆ†è§£ä¸ºå¯ç®¡ç†çš„ã€å¯å­¦ä¹ çš„ç»„ä»¶è€ŒæˆåŠŸï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>å‹ç¼©ä»¥æé«˜æ•ˆç‡ï¼š</b>VAEå‹ç¼©å°†é«˜ç»´è§†è§‰è¾“å…¥å‡å°‘åˆ°ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œå®ç°ç¯å¢ƒåŠ¨æ€çš„é«˜æ•ˆå­¦ä¹ å’Œé¢„æµ‹ã€‚</li>
                <li style="margin-bottom:6px;"><b>é¢„æµ‹æ€§è®°å¿†ï¼š</b>MDN-RNNæä¾›æ—¶é—´ä¸Šä¸‹æ–‡å’Œæœªæ¥é¢„æµ‹èƒ½åŠ›ï¼Œå…è®¸ä»£ç†æå‰è§„åˆ’å¹¶åŸºäºé¢„æœŸç»“æœåšå‡ºæ˜æ™ºå†³ç­–ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç®€åŒ–ä¼˜åŒ–ï¼š</b>å°†ä¸–ç•Œæ¨¡å‹å­¦ä¹ ä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»é¿å…äº†ä¿¡ç”¨åˆ†é…ç“¶é¢ˆï¼Œå…è®¸è¿›åŒ–ç­–ç•¥é«˜æ•ˆä¼˜åŒ–ç®€å•æ§åˆ¶å™¨ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¢¦å¢ƒå¢å¼ºï¼š</b>åœ¨å…·æœ‰å¯è°ƒèŠ‚éš¾åº¦çš„å¹»è§‰ç¯å¢ƒä¸­è®­ç»ƒä½¿ä»£ç†èƒ½å¤Ÿå­¦ä¹ æ¯”ä»…åœ¨çœŸå®ç¯å¢ƒä¸­è®­ç»ƒçš„ç­–ç•¥æ›´é²æ£’çš„ç­–ç•¥ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯æ‰©å±•æ¶æ„ï¼š</b>æ¨¡å—åŒ–è®¾è®¡å…è®¸ç‹¬ç«‹æ‰©å±•ç»„ä»¶ - å¤æ‚çš„ä¸–ç•Œæ¨¡å‹å¯ä»¥ä¸ç®€å•æ§åˆ¶å™¨é…å¯¹ï¼Œå®ç°è·¨ä¸åŒç¯å¢ƒçš„æœ‰æ•ˆå­¦ä¹ ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This groundbreaking work introduces World Models, a paradigm-shifting approach to reinforcement learning that draws inspiration from human cognition. By decomposing agents into separate visual compression, temporal prediction, and decision-making components, the authors demonstrate how neural networks can learn to model and navigate complex environments with unprecedented efficiency. The key insightâ€”that separating world model learning from policy optimization enables more effective credit assignmentâ€”has profound implications for scaling RL to complex, high-dimensional environments. The ability to train agents entirely within hallucinated dreams generated by their own world models represents a fundamental breakthrough, allowing for safer, more efficient, and more scalable agent training. Experimental results on CarRacing-v0 and VizDoom Take Cover showcase the approach's power, achieving state-of-the-art performance while using orders of magnitude fewer parameters than traditional RL methods. The World Models framework opens new avenues for understanding intelligence through the lens of predictive coding and compressed representations, potentially bridging the gap between artificial and biological intelligence. This work establishes a new foundation for model-based RL, demonstrating that learning to predict and simulate the world can be more powerful than learning to act within it directly.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å¼€åˆ›æ€§å·¥ä½œå¼•å…¥äº†ä¸–ç•Œæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§èŒƒå¼è½¬å˜çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œçµæ„Ÿæ¥æºäºäººç±»è®¤çŸ¥ã€‚é€šè¿‡å°†ä»£ç†åˆ†è§£ä¸ºç‹¬ç«‹çš„è§†è§‰å‹ç¼©ã€æ—¶é—´é¢„æµ‹å’Œå†³ç­–åˆ¶å®šç»„ä»¶ï¼Œä½œè€…å±•ç¤ºäº†ç¥ç»ç½‘ç»œå¦‚ä½•ä»¥å‰æ‰€æœªæœ‰çš„æ•ˆç‡å­¦ä¹ å»ºæ¨¡å’Œå¯¼èˆªå¤æ‚ç¯å¢ƒã€‚å…³é”®æ´å¯Ÿâ€”â€”å°†ä¸–ç•Œæ¨¡å‹å­¦ä¹ ä¸ç­–ç•¥ä¼˜åŒ–åˆ†ç¦»å¯ä»¥å®ç°æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…â€”â€”å¯¹å°†RLæ‰©å±•åˆ°å¤æ‚çš„é«˜ç»´ç¯å¢ƒå…·æœ‰æ·±è¿œæ„ä¹‰ã€‚èƒ½å¤Ÿåœ¨ä»£ç†è‡ªå·±çš„ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„å¹»è§‰æ¢¦å¢ƒä¸­å®Œå…¨è®­ç»ƒä»£ç†çš„èƒ½åŠ›ä»£è¡¨äº†ä¸€ä¸ªæ ¹æœ¬æ€§çªç ´ï¼Œå…è®¸æ›´å®‰å…¨ã€æ›´é«˜æ•ˆå’Œæ›´å¯æ‰©å±•çš„ä»£ç†è®­ç»ƒã€‚åœ¨CarRacing-v0å’ŒVizDoom Take Coverä¸Šçš„å®éªŒç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•çš„å¼ºå¤§åŠŸèƒ½ï¼Œåœ¨ä½¿ç”¨æ¯”ä¼ ç»ŸRLæ–¹æ³•å°‘å‡ ä¸ªæ•°é‡çº§çš„å‚æ•°çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸–ç•Œæ¨¡å‹æ¡†æ¶é€šè¿‡é¢„æµ‹ç¼–ç å’Œå‹ç¼©è¡¨ç¤ºçš„è§†è§’å¼€è¾Ÿäº†ç†è§£æ™ºèƒ½çš„æ–°é€”å¾„ï¼Œå¯èƒ½å¼¥åˆäººå·¥æ™ºèƒ½ä¸ç”Ÿç‰©æ™ºèƒ½ä¹‹é—´çš„å·®è·ã€‚è¿™é¡¹å·¥ä½œä¸ºåŸºäºæ¨¡å‹çš„RLå»ºç«‹äº†æ–°çš„åŸºç¡€ï¼Œè¯æ˜äº†å­¦ä¹ é¢„æµ‹å’Œæ¨¡æ‹Ÿä¸–ç•Œå¯èƒ½æ¯”ç›´æ¥åœ¨å…¶ä¸­å­¦ä¹ è¡ŒåŠ¨æ›´å¼ºå¤§ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/1803.10122" target="_blank" style="color:#8bffcf;">1803.10122</a></p>
          <p><b>Project Site:</b> <a href="https://worldmodels.github.io" target="_blank" style="color:#8bffcf;">https://worldmodels.github.io</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/hardmaru/WorldModelsExperiments" target="_blank" style="color:#8bffcf;">https://github.com/hardmaru/WorldModelsExperiments</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# World Models Implementation<br>
<br>
// VAE (Vision) Model for image compression<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">VAE</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Variational Autoencoder for compressing visual observations"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, input_dim, latent_dim=32):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.encoder = self._build_encoder(input_dim, latent_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.decoder = self._build_decoder(latent_dim, input_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">encode</span>(self, x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mean, log_var = self.encoder(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;z = self._reparameterize(mean, log_var)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> z<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">decode</span>(self, z):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> self.decoder(z)<br>
<br>
// MDN-RNN (Memory) Model for temporal prediction<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">MDN_RNN</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Mixture Density Network RNN for predicting future latent states"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, latent_dim=32, action_dim=3, hidden_dim=256, num_mixtures=5):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.rnn = self._build_rnn(latent_dim + action_dim, hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.mdn = self._build_mdn(hidden_dim, latent_dim, num_mixtures)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.done_predictor = self._build_done_predictor(hidden_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, action, z, h):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input_tensor = torch.cat([action, z], dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;h_next = self.rnn(input_tensor, h)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;z_next = self._sample_from_mdn(self.mdn(h_next))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;done_prob = torch.sigmoid(self.done_predictor(h_next))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> z_next, h_next, done_prob<br>
<br>
// Linear Controller (C) Model<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">Controller</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Simple linear controller mapping latent state + hidden state to actions"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, latent_dim=32, hidden_dim=256, action_dim=3):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input_dim = latent_dim + hidden_dim<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.linear = nn.Linear(input_dim, action_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, z, h):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined = torch.cat([z, h], dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;action = torch.tanh(self.linear(combined))  <span style="color:#6a9955;"># Bound actions to [-1, 1]</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> action<br>
<br>
// Dream Environment for training in hallucinated worlds<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">DreamEnvironment</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Environment wrapper that uses world model to generate dreams"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, vae, mdn_rnn, temperature=1.0):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.vae = vae<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.mdn_rnn = mdn_rnn<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.temperature = temperature<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.reset()<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">reset</span>(self):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.h = self.mdn_rnn.initial_hidden()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.z = torch.randn_like(torch.zeros(1, 32))  <span style="color:#6a9955;"># Random initial latent</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> self.vae.decode(self.z)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">step</span>(self, action):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.z, self.h, done_prob = self.mdn_rnn(action, self.z, self.h)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Apply temperature for controlled difficulty</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.z = self.z / self.temperature<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;observation = self.vae.decode(self.z)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;done = done_prob > 0.5<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reward = 1.0  <span style="color:#6a9955;"># Simple survival reward</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> observation, reward, done, {}<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
