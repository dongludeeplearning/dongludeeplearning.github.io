<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ Neural Computing and Applications 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Optimizing Student Engagement Detection Using Facial and Behavioral Features</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Riju Das, Soumyabrata Dev<br>
        <span style="opacity:0.8">University College Dublin, Ireland</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can facial action units and behavioral features be directly mapped to student engagement labels for accurate real-time detection in educational environments?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•å°†é¢éƒ¨åŠ¨ä½œå•å…ƒå’Œè¡Œä¸ºç‰¹å¾ç›´æ¥æ˜ å°„åˆ°å­¦ç”Ÿå‚ä¸åº¦æ ‡ç­¾ï¼Œä»¥å®ç°æ•™è‚²ç¯å¢ƒä¸­å‡†ç¡®çš„å®æ—¶æ£€æµ‹ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>AU-Engagement Mapping:</b> First direct mapping of facial action units (AUs) to engagement labels using conditional probability, relative activation ratio, and statistical discriminative coefficient (SDC) metrics.</div>
            <div class="lang-zh" style="display:none"><b>AU-å‚ä¸åº¦æ˜ å°„ï¼š</b>é¦–æ¬¡ä½¿ç”¨æ¡ä»¶æ¦‚ç‡ã€ç›¸å¯¹æ¿€æ´»æ¯”ç‡å’Œç»Ÿè®¡åˆ¤åˆ«ç³»æ•°ï¼ˆSDCï¼‰æŒ‡æ ‡å°†é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ç›´æ¥æ˜ å°„åˆ°å‚ä¸åº¦æ ‡ç­¾ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Comprehensive ML/DL Analysis:</b> Systematic comparison of traditional machine learning (SVM, RF, XGBoost) and deep learning approaches (ResNet, EfficientNet) for engagement detection using facial and behavioral features.</div>
            <div class="lang-zh" style="display:none"><b>å…¨é¢ML/DLåˆ†æï¼š</b>ç³»ç»Ÿæ¯”è¾ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆSVMã€RFã€XGBoostï¼‰å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆResNetã€EfficientNetï¼‰ï¼Œç”¨äºä½¿ç”¨é¢éƒ¨å’Œè¡Œä¸ºç‰¹å¾çš„å‚ä¸åº¦æ£€æµ‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multimodal Integration:</b> Dual-path architecture combining image features with OpenFace behavioral features (AUs, eye gaze, head pose) for enhanced engagement classification performance.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€é›†æˆï¼š</b>åŒè·¯å¾„æ¶æ„å°†å›¾åƒç‰¹å¾ä¸OpenFaceè¡Œä¸ºç‰¹å¾ï¼ˆAUã€çœ¼éƒ¨æ³¨è§†ã€å¤´éƒ¨å§¿åŠ¿ï¼‰ç›¸ç»“åˆï¼Œä»¥å¢å¼ºå‚ä¸åº¦åˆ†ç±»æ€§èƒ½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Reproducible Framework:</b> Open-source code repository with comprehensive evaluation on WACV and DAiSEE datasets, achieving 82.9% accuracy with XGBoost and 47.2% with EfficientNet.</div>
            <div class="lang-zh" style="display:none"><b>å¯é‡ç°æ¡†æ¶ï¼š</b>å¼€æºä»£ç åº“ï¼Œåœ¨WACVå’ŒDAiSEEæ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒXGBoostè¾¾åˆ°82.9%çš„å‡†ç¡®ç‡ï¼ŒEfficientNetè¾¾åˆ°47.2%ã€‚</div>
          </li>
        </ul>
      </div>



      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Direct AU-Engagement Mapping:</b> Bridging the gap between low-level facial action units and high-level engagement states without relying on intermediate emotion classification steps.</div>
            <div class="lang-zh" style="display:none"><b>ç›´æ¥AU-å‚ä¸åº¦æ˜ å°„ï¼š</b>å¼¥åˆä½çº§é¢éƒ¨åŠ¨ä½œå•å…ƒå’Œé«˜æ°´å¹³å‚ä¸åº¦çŠ¶æ€ä¹‹é—´çš„å·®è·ï¼Œè€Œä¸ä¾èµ–äºä¸­é—´æƒ…ç»ªåˆ†ç±»æ­¥éª¤ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Dataset Imbalance:</b> Handling significantly imbalanced engagement classes in educational datasets (412 disengaged vs. 2247 partially engaged vs. 1765 engaged images) using weighted sampling and loss functions.</div>
            <div class="lang-zh" style="display:none"><b>æ•°æ®é›†ä¸å¹³è¡¡ï¼š</b>ä½¿ç”¨åŠ æƒé‡‡æ ·å’ŒæŸå¤±å‡½æ•°å¤„ç†æ•™è‚²æ•°æ®é›†ä¸­ä¸¥é‡ä¸å¹³è¡¡çš„å‚ä¸åº¦ç±»åˆ«ï¼ˆ412ä¸ªä¸å‚ä¸ã€2247ä¸ªéƒ¨åˆ†å‚ä¸ã€1765ä¸ªå‚ä¸å›¾åƒï¼‰ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Real-Time Processing:</b> Extracting and processing multimodal facial features (AUs, gaze, pose, landmarks) in real-time for continuous engagement monitoring in educational settings.</div>
            <div class="lang-zh" style="display:none"><b>å®æ—¶å¤„ç†ï¼š</b>å®æ—¶æå–å’Œå¤„ç†å¤šæ¨¡æ€é¢éƒ¨ç‰¹å¾ï¼ˆAUã€æ³¨è§†ã€å§¿åŠ¿ã€åœ°æ ‡ï¼‰ï¼Œç”¨äºæ•™è‚²ç¯å¢ƒä¸­çš„è¿ç»­å‚ä¸åº¦ç›‘æ§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Static vs. Dynamic Analysis:</b> Extending AU-engagement mappings from static images to dynamic video sequences, accounting for temporal variations in facial expressions across engagement states.</div>
            <div class="lang-zh" style="display:none"><b>é™æ€vsåŠ¨æ€åˆ†æï¼š</b>å°†AU-å‚ä¸åº¦æ˜ å°„ä»é™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€è§†é¢‘åºåˆ—ï¼Œè€ƒè™‘å‚ä¸åº¦çŠ¶æ€ä¸‹é¢éƒ¨è¡¨æƒ…çš„æ—¶é—´å˜åŒ–ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The paper presents a comprehensive framework for optimizing student engagement detection:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Facial Feature Extraction:</b> Utilized OpenFace for AU detection, eye gaze, and head pose estimation, and MediaPipe for 468 facial landmarks to capture comprehensive behavioral features.</li>
                <li style="margin-bottom:6px;"><b>AU-Engagement Statistical Analysis:</b> Computed conditional probability, relative activation ratio (RAR), and statistical discriminative coefficient (SDC) to map 17 AUs directly to engagement labels across WACV and DAiSEE datasets.</li>
                <li style="margin-bottom:6px;"><b>Machine Learning Pipeline:</b> Trained traditional ML models (SVM, Random Forest, XGBoost) on concatenated behavioral features, employing class weights and WeightedRandomSampler to handle dataset imbalance.</li>
                <li style="margin-bottom:6px;"><b>Deep Learning Architectures:</b> Implemented ResNet and EfficientNet with dual-path approach combining image features with OpenFace behavioral features, using Optuna for hyperparameter optimization.</li>
                <li style="margin-bottom:6px;"><b>Interpretability Analysis:</b> Applied Grad-CAM for model visualization and ablation studies to understand feature importance, with comprehensive evaluation on accuracy, precision, recall, and F1-score metrics.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–å­¦ç”Ÿå‚ä¸åº¦æ£€æµ‹ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>é¢éƒ¨ç‰¹å¾æå–ï¼š</b>ä½¿ç”¨OpenFaceè¿›è¡ŒAUæ£€æµ‹ã€çœ¼éƒ¨æ³¨è§†å’Œå¤´éƒ¨å§¿åŠ¿ä¼°è®¡ï¼Œä½¿ç”¨MediaPipeè¿›è¡Œ468ä¸ªé¢éƒ¨åœ°æ ‡ï¼Œä»¥æ•è·å…¨é¢çš„è¡Œä¸ºç‰¹å¾ã€‚</li>
                <li style="margin-bottom:6px;"><b>AU-å‚ä¸åº¦ç»Ÿè®¡åˆ†æï¼š</b>è®¡ç®—æ¡ä»¶æ¦‚ç‡ã€ç›¸å¯¹æ¿€æ´»æ¯”ç‡ï¼ˆRARï¼‰å’Œç»Ÿè®¡åˆ¤åˆ«ç³»æ•°ï¼ˆSDCï¼‰ï¼Œå°†17ä¸ªAUç›´æ¥æ˜ å°„åˆ°WACVå’ŒDAiSEEæ•°æ®é›†çš„å‚ä¸åº¦æ ‡ç­¾ã€‚</li>
                <li style="margin-bottom:6px;"><b>æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼š</b>åœ¨ä¸²è”çš„è¡Œä¸ºç‰¹å¾ä¸Šè®­ç»ƒä¼ ç»ŸMLæ¨¡å‹ï¼ˆSVMã€éšæœºæ£®æ—ã€XGBoostï¼‰ï¼Œä½¿ç”¨ç±»åˆ«æƒé‡å’ŒWeightedRandomSampleræ¥å¤„ç†æ•°æ®é›†ä¸å¹³è¡¡ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ·±åº¦å­¦ä¹ æ¶æ„ï¼š</b>ä½¿ç”¨åŒè·¯å¾„æ–¹æ³•å®ç°ResNetå’ŒEfficientNetï¼Œå°†å›¾åƒç‰¹å¾ä¸OpenFaceè¡Œä¸ºç‰¹å¾ç›¸ç»“åˆï¼Œä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯è§£é‡Šæ€§åˆ†æï¼š</b>åº”ç”¨Grad-CAMè¿›è¡Œæ¨¡å‹å¯è§†åŒ–å’Œæ¶ˆèç ”ç©¶ä»¥äº†è§£ç‰¹å¾é‡è¦æ€§ï¼Œä½¿ç”¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°æŒ‡æ ‡è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/optimizing_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
          <img src="Figures/optimizing_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('Optimizing student engagement.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('Optimizing student engagement.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of direct AU-engagement mapping for student engagement detection stems from several methodological innovations:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Statistical AU Characterization:</b> The three statistical metrics (conditional probability, RAR, SDC) provide robust characterization of AU-engagement relationships, identifying discriminative AUs like AU45/AU20 for disengagement and AU14/AU23 for engagement without intermediate emotion classification.</li>
                <li style="margin-bottom:6px;"><b>Multimodal Feature Integration:</b> Combining behavioral features (AUs, gaze, pose) with image features through dual-path architectures captures complementary information that single-modality approaches miss, leading to superior classification performance.</li>
                <li style="margin-bottom:6px;"><b>Imbalance-Aware Training:</b> Class weights and weighted sampling ensure minority classes (disengaged students) receive appropriate attention during training, preventing model bias toward majority partially engaged samples.</li>
                <li style="margin-bottom:6px;"><b>Hyperparameter Optimization:</b> Optuna-driven tuning identifies optimal configurations for different model architectures and feature combinations, maximizing performance across diverse educational scenarios.</li>
                <li style="margin-bottom:6px;"><b>Cross-Dataset Validation:</b> Evaluation on both static (WACV) and dynamic (DAiSEE) datasets ensures robustness across different data modalities and temporal characteristics, supporting real-world deployment in various educational settings.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>ç›´æ¥AU-å‚ä¸åº¦æ˜ å°„åœ¨å­¦ç”Ÿå‚ä¸åº¦æ£€æµ‹æ–¹é¢çš„æˆåŠŸæºäºå‡ ä¸ªæ–¹æ³•åˆ›æ–°ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>ç»Ÿè®¡AUè¡¨å¾ï¼š</b>ä¸‰ä¸ªç»Ÿè®¡æŒ‡æ ‡ï¼ˆæ¡ä»¶æ¦‚ç‡ã€RARã€SDCï¼‰æä¾›äº†AU-å‚ä¸åº¦å…³ç³»çš„ç¨³å¥è¡¨å¾ï¼Œè¯†åˆ«å‡ºåˆ¤åˆ«æ€§AUï¼Œå¦‚ä¸å‚ä¸çš„AU45/AU20å’Œå‚ä¸çš„AU14/AU23ï¼Œè€Œæ— éœ€ä¸­é—´æƒ…ç»ªåˆ†ç±»ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šæ¨¡æ€ç‰¹å¾é›†æˆï¼š</b>é€šè¿‡åŒè·¯å¾„æ¶æ„å°†è¡Œä¸ºç‰¹å¾ï¼ˆAUã€æ³¨è§†ã€å§¿åŠ¿ï¼‰ä¸å›¾åƒç‰¹å¾ç›¸ç»“åˆï¼Œæ•è·å•æ¨¡æ€æ–¹æ³•é—æ¼çš„äº’è¡¥ä¿¡æ¯ï¼Œä»è€Œå®ç°å“è¶Šçš„åˆ†ç±»æ€§èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä¸å¹³è¡¡æ„ŸçŸ¥è®­ç»ƒï¼š</b>ç±»åˆ«æƒé‡å’ŒåŠ æƒé‡‡æ ·ç¡®ä¿å°‘æ•°ç±»åˆ«ï¼ˆä¸å‚ä¸çš„å­¦ç”Ÿï¼‰åœ¨è®­ç»ƒæœŸé—´è·å¾—é€‚å½“å…³æ³¨ï¼Œé˜²æ­¢æ¨¡å‹åå‘å¤šæ•°éƒ¨åˆ†å‚ä¸æ ·æœ¬ã€‚</li>
                <li style="margin-bottom:6px;"><b>è¶…å‚æ•°ä¼˜åŒ–ï¼š</b>Optunaé©±åŠ¨çš„è°ƒä¼˜ä¸ºä¸åŒæ¨¡å‹æ¶æ„å’Œç‰¹å¾ç»„åˆè¯†åˆ«å‡ºæœ€ä½³é…ç½®ï¼Œåœ¨ä¸åŒæ•™è‚²åœºæ™¯ä¸­æœ€å¤§åŒ–æ€§èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>è·¨æ•°æ®é›†éªŒè¯ï¼š</b>åœ¨é™æ€ï¼ˆWACVï¼‰å’ŒåŠ¨æ€ï¼ˆDAiSEEï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç¡®ä¿åœ¨ä¸åŒæ•°æ®æ¨¡æ€å’Œæ—¶é—´ç‰¹å¾ä¸Šçš„é²æ£’æ€§ï¼Œæ”¯æŒåœ¨å„ç§æ•™è‚²ç¯å¢ƒä¸­å®é™…éƒ¨ç½²ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This comprehensive study establishes a novel framework for optimizing student engagement detection by directly mapping facial action units to engagement labels, bypassing traditional emotion classification intermediaries. The systematic evaluation of machine learning and deep learning approaches on WACV and DAiSEE datasets demonstrates superior performance with XGBoost achieving 82.9% accuracy and EfficientNet reaching 47.2% in deep learning experiments. The statistical analysis using conditional probability, relative activation ratio, and statistical discriminative coefficient provides interpretable insights into AU-engagement relationships, identifying key discriminative units for each engagement state. The multimodal integration of facial features with advanced handling of dataset imbalance and hyperparameter optimization creates a robust, reproducible framework suitable for real-time deployment in educational environments. This work advances the field of affective computing in education by providing educators with objective, automated tools for engagement assessment, facilitating timely interventions to enhance learning outcomes and student satisfaction.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å…¨é¢ç ”ç©¶é€šè¿‡å°†é¢éƒ¨åŠ¨ä½œå•å…ƒç›´æ¥æ˜ å°„åˆ°å‚ä¸åº¦æ ‡ç­¾ï¼Œå»ºç«‹äº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç»•è¿‡äº†ä¼ ç»Ÿçš„æƒ…ç»ªåˆ†ç±»ä¸­ä»‹ã€‚åœ¨WACVå’ŒDAiSEEæ•°æ®é›†ä¸Šå¯¹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç³»ç»Ÿè¯„ä¼°å±•ç¤ºäº†å“è¶Šæ€§èƒ½ï¼ŒXGBoostè¾¾åˆ°82.9%çš„å‡†ç¡®ç‡ï¼ŒEfficientNetåœ¨æ·±åº¦å­¦ä¹ å®éªŒä¸­è¾¾åˆ°47.2%ã€‚ä½¿ç”¨æ¡ä»¶æ¦‚ç‡ã€ç›¸å¯¹æ¿€æ´»æ¯”ç‡å’Œç»Ÿè®¡åˆ¤åˆ«ç³»æ•°çš„ç»Ÿè®¡åˆ†ææä¾›äº†å¯¹AU-å‚ä¸åº¦å…³ç³»çš„å¯è§£é‡Šè§è§£ï¼Œè¯†åˆ«å‡ºæ¯ä¸ªå‚ä¸åº¦çŠ¶æ€çš„å…³é”®åˆ¤åˆ«å•å…ƒã€‚é¢éƒ¨ç‰¹å¾çš„å¤šæ¨¡æ€é›†æˆä»¥åŠå¯¹æ•°æ®é›†ä¸å¹³è¡¡çš„é«˜çº§å¤„ç†å’Œè¶…å‚æ•°ä¼˜åŒ–åˆ›å»ºäº†ä¸€ä¸ªç¨³å¥ã€å¯é‡ç°çš„æ¡†æ¶ï¼Œé€‚ç”¨äºæ•™è‚²ç¯å¢ƒä¸­çš„å®æ—¶éƒ¨ç½²ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä¸ºæ•™è‚²å·¥ä½œè€…æä¾›å®¢è§‚çš„è‡ªåŠ¨åŒ–å‚ä¸åº¦è¯„ä¼°å·¥å…·ï¼Œæ¨åŠ¨äº†æ•™è‚²æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„å‘å±•ï¼Œä¿ƒè¿›åŠæ—¶å¹²é¢„ä»¥å¢å¼ºå­¦ä¹ æˆæœå’Œå­¦ç”Ÿæ»¡æ„åº¦ã€‚</p>
          </div>
        </div>
      </div>



      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import pandas as pd<br>
import numpy as np<br>
from sklearn.ensemble import RandomForestClassifier<br>
from sklearn.svm import SVC<br>
import xgboost as xgb<br>
from torch.utils.data import WeightedRandomSampler<br>
<br>
// Statistical AU-Engagement Mapping<br>
def compute_au_engagement_metrics(au_data, engagement_labels):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;Compute conditional probability, RAR, and SDC for AU-engagement mapping<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;au_columns = [f'AU{i}' for i in range(1, 18)]  # AU1 to AU17<br>
&nbsp;&nbsp;&nbsp;&nbsp;engagement_classes = np.unique(engagement_labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;metrics = {}<br>
&nbsp;&nbsp;&nbsp;&nbsp;for au in au_columns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conditional_probs = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rar_values = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sdc_values = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;overall_activation = au_data[au].mean()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for cls in engagement_classes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class_mask = engagement_labels == cls<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conditional_prob = au_data.loc[class_mask, au].mean()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rar = conditional_prob / overall_activation if overall_activation > 0 else 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conditional_probs.append(conditional_prob)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rar_values.append(rar)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Compute SDC<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class_prior = np.mean(class_mask)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sdc_value = class_prior * np.log(conditional_prob / overall_activation) if conditional_prob > 0 and overall_activation > 0 else 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sdc_values.append(sdc_value)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics[au] = {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'conditional_probs': conditional_probs,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'rar_values': rar_values,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'sdc_values': sdc_values<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return metrics<br>
<br>
// Multimodal Engagement Classifier with Imbalance Handling<br>
class EngagementClassifier:<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, model_type='xgboost'):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model_type = model_type<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if model_type == 'xgboost':<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model = xgb.XGBClassifier(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;objective='multi:softprob',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_class=3,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eval_metric='mlogloss'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif model_type == 'svm':<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model = SVC(kernel='rbf', probability=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif model_type == 'rf':<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model = RandomForestClassifier(n_estimators=100)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def compute_class_weights(self, labels):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Compute class weights for imbalanced dataset"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unique_labels, counts = np.unique(labels, return_counts=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;total_samples = len(labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights = {}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for label, count in zip(unique_labels, counts):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights[label] = total_samples / (len(unique_labels) * count)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return weights<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def create_weighted_sampler(self, labels):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Create WeightedRandomSampler for imbalanced data"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class_weights = self.compute_class_weights(labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sample_weights = np.array([class_weights[label] for label in labels])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return WeightedRandomSampler(sample_weights, len(sample_weights))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def fit(self, X, y):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.model_type == 'xgboost':<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class_weights = self.compute_class_weights(y)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model.fit(X, y, sample_weight=[class_weights[label] for label in y])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model.fit(X, y)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def predict(self, X):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.model.predict(X)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def predict_proba(self, X):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.model.predict_proba(X)<br>
<br>
// Dual-Path Deep Learning Model<br>
import torch<br>
import torch.nn as nn<br>
from torchvision.models import resnet50, efficientnet_b0<br>
<br>
class DualPathEngagementModel(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, num_classes=3, behavioral_dim=709):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(DualPathEngagementModel, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Image feature extractor (ResNet or EfficientNet)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.image_backbone = resnet50(pretrained=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.image_backbone.fc = nn.Identity()  # Remove classification head<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Behavioral feature processor<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.behavioral_processor = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(behavioral_dim, 128),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Dropout(0.5),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(128, 64)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Fusion and classification<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.fusion = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(2048 + 64, 512),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Dropout(0.3),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(512, num_classes)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, image, behavioral_features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract image features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_features = self.image_backbone(image)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Process behavioral features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;behavioral_processed = self.behavioral_processor(behavioral_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Concatenate features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined = torch.cat([image_features, behavioral_processed], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Classification<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output = self.fusion(combined)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return output<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
