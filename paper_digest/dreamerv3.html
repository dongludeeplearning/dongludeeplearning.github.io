<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2023 â€¢ Nature 2025 April
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Mastering Diverse Domains through World Models</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap<br>
        <span style="opacity:0.8">Google DeepMind, University of Toronto</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we develop a single reinforcement learning algorithm that learns to solve diverse tasks across domains ranging from video games to robotics, without requiring domain-specific hyperparameter tuning or human expertise?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•å¼€å‘ä¸€ä¸ªå•ä¸€çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿå­¦ä¹ è§£å†³ä»è§†é¢‘æ¸¸æˆåˆ°æœºå™¨äººæŠ€æœ¯çš„å„ç§é¢†åŸŸä¸­çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼Œè€Œæ— éœ€é¢†åŸŸç‰¹å®šçš„è¶…å‚æ•°è°ƒæ•´æˆ–äººå·¥ä¸“ä¸šçŸ¥è¯†ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>DreamerV3 Algorithm:</b> Introduced the third generation of Dreamer, a general RL algorithm that learns world models and uses imagination for planning, achieving state-of-the-art performance across 150+ diverse tasks with fixed hyperparameters.</div>
            <div class="lang-zh" style="display:none"><b>DreamerV3ç®—æ³•ï¼š</b>å¼•å…¥äº†Dreamerçš„ç¬¬ä¸‰ä»£ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„RLç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ ä¸–ç•Œæ¨¡å‹å¹¶ä½¿ç”¨æƒ³è±¡è¿›è¡Œè§„åˆ’ï¼Œä»¥å›ºå®šè¶…å‚æ•°åœ¨150å¤šç§å¤šæ ·åŒ–ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Minecraft Diamond Collection:</b> First algorithm to collect diamonds in Minecraft from scratch without human data or curricula, solving a long-standing challenge in AI that requires farsighted strategies, exploration, and sparse rewards in an open world.</div>
            <div class="lang-zh" style="display:none"><b>Minecrafté’»çŸ³æ”¶é›†ï¼š</b>ç¬¬ä¸€ä¸ªä»é›¶å¼€å§‹åœ¨Minecraftä¸­æ”¶é›†é’»çŸ³çš„ç®—æ³•ï¼Œæ— éœ€äººç±»æ•°æ®æˆ–è¯¾ç¨‹ï¼Œè§£å†³äº†AIä¸­çš„ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œè¿œè§ç­–ç•¥ã€æ¢ç´¢å’Œç¨€ç–å¥–åŠ±ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Robustness Techniques:</b> Developed normalization, balancing, and transformation techniques that enable stable learning across domains with different signal magnitudes, reward scales, and data distributions.</div>
            <div class="lang-zh" style="display:none"><b>é²æ£’æ€§æŠ€æœ¯ï¼š</b>å¼€å‘äº†å½’ä¸€åŒ–ã€å¹³è¡¡å’Œå˜æ¢æŠ€æœ¯ï¼Œä½¿ç®—æ³•èƒ½å¤Ÿåœ¨å…·æœ‰ä¸åŒä¿¡å·å¹…åº¦ã€å¥–åŠ±è§„æ¨¡å’Œæ•°æ®åˆ†å¸ƒçš„é¢†åŸŸä¸­è¿›è¡Œç¨³å®šçš„å­¦ä¹ ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scalable Architecture:</b> Demonstrated that Dreamer scales predictably with model size and computational resources, enabling practitioners to improve performance by simply increasing compute without algorithm redesign.</div>
            <div class="lang-zh" style="display:none"><b>å¯æ‰©å±•æ¶æ„ï¼š</b>è¯æ˜äº†Dreamerå¯ä»¥ä¸æ¨¡å‹å¤§å°å’Œè®¡ç®—èµ„æºè¿›è¡Œå¯é¢„æµ‹çš„æ‰©å±•ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿé€šè¿‡ç®€å•å¢åŠ è®¡ç®—èµ„æºæ¥æé«˜æ€§èƒ½ï¼Œè€Œæ— éœ€é‡æ–°è®¾è®¡ç®—æ³•ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Domain Generalization:</b> Traditional RL algorithms require extensive hyperparameter tuning and domain expertise for each new application, making them brittle and expensive to apply broadly.</div>
            <div class="lang-zh" style="display:none"><b>é¢†åŸŸæ³›åŒ–ï¼š</b>ä¼ ç»Ÿçš„RLç®—æ³•éœ€è¦ä¸ºæ¯ä¸ªæ–°åº”ç”¨è¿›è¡Œå¤§é‡çš„è¶…å‚æ•°è°ƒæ•´å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œä½¿å…¶è„†å¼±ä¸”éš¾ä»¥å¹¿æ³›åº”ç”¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>World Model Learning:</b> Robustly learning and leveraging world models for planning requires overcoming challenges in prediction accuracy, representation quality, and balancing reconstruction vs. control objectives.</div>
            <div class="lang-zh" style="display:none"><b>ä¸–ç•Œæ¨¡å‹å­¦ä¹ ï¼š</b>ä¸ºäº†è§„åˆ’è€Œç¨³å¥åœ°å­¦ä¹ å’Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹éœ€è¦å…‹æœé¢„æµ‹å‡†ç¡®æ€§ã€è¡¨ç¤ºè´¨é‡ä»¥åŠå¹³è¡¡é‡å»ºä¸æ§åˆ¶ç›®æ ‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scale and Stability:</b> Ensuring stable learning across domains with vastly different reward scales, observation types, and computational requirements while maintaining efficiency.</div>
            <div class="lang-zh" style="display:none"><b>è§„æ¨¡å’Œç¨³å®šæ€§ï¼š</b>ç¡®ä¿åœ¨å…·æœ‰æˆªç„¶ä¸åŒçš„å¥–åŠ±è§„æ¨¡ã€è§‚æµ‹ç±»å‹å’Œè®¡ç®—è¦æ±‚çš„é¢†åŸŸä¸­è¿›è¡Œç¨³å®šçš„å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒæ•ˆç‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Open-World Exploration:</b> Developing agents that can explore and learn long-term strategies in open-ended environments with sparse rewards and procedural generation.</div>
            <div class="lang-zh" style="display:none"><b>å¼€æ”¾ä¸–ç•Œæ¢ç´¢ï¼š</b>å¼€å‘èƒ½å¤Ÿåœ¨å…·æœ‰ç¨€ç–å¥–åŠ±å’Œç¨‹åºç”Ÿæˆçš„å¼€æ”¾å¼ç¯å¢ƒä¸­æ¢ç´¢å’Œå­¦ä¹ é•¿æœŸç­–ç•¥çš„ä»£ç†ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>DreamerV3 learns a world model and uses imagination for planning, with robustness techniques enabling generalization across domains:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>World Model Architecture:</b> Recurrent State-Space Model (RSSM) that encodes observations, predicts future states via sequence modeling, and reconstructs inputs to ensure informative representations.</li>
                <li style="margin-bottom:6px;"><b>Actor-Critic Learning:</b> Actor selects actions while critic predicts value distributions using imagined trajectories generated by the world model, with concurrent training of all components.</li>
                <li style="margin-bottom:6px;"><b>Robustness Techniques:</b> Symlog transformations for handling large/small values, KL balancing with free bits for stable representation learning, return normalization for scale invariance, and twohot regression for multi-modal distributions.</li>
                <li style="margin-bottom:6px;"><b>Scalable Implementation:</b> Fixed hyperparameters across domains, predictable scaling with model size and compute, enabling practitioners to improve performance by simply increasing resources.</li>
                <li style="margin-bottom:6px;"><b>Imagination-Based Planning:</b> Generates imagined trajectories by sampling from the world model to learn policies without requiring additional environment interactions beyond training data.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>DreamerV3å­¦ä¹ ä¸–ç•Œæ¨¡å‹å¹¶ä½¿ç”¨æƒ³è±¡è¿›è¡Œè§„åˆ’ï¼Œå…·æœ‰é²æ£’æ€§æŠ€æœ¯å®ç°è·¨é¢†åŸŸæ³›åŒ–ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>ä¸–ç•Œæ¨¡å‹æ¶æ„ï¼š</b>å¾ªç¯çŠ¶æ€-ç©ºé—´æ¨¡å‹(RSSM)ï¼Œå¯¹è§‚æµ‹è¿›è¡Œç¼–ç ï¼Œé€šè¿‡åºåˆ—å»ºæ¨¡é¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œå¹¶é‡å»ºè¾“å…¥ä»¥ç¡®ä¿ä¿¡æ¯ä¸°å¯Œçš„è¡¨ç¤ºã€‚</li>
                <li style="margin-bottom:6px;"><b>Actor-Criticå­¦ä¹ ï¼š</b>Actoré€‰æ‹©åŠ¨ä½œè€ŒCriticé¢„æµ‹ä»·å€¼åˆ†å¸ƒï¼Œä½¿ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„æƒ³è±¡è½¨è¿¹ï¼ŒåŒæ—¶è®­ç»ƒæ‰€æœ‰ç»„ä»¶ã€‚</li>
                <li style="margin-bottom:6px;"><b>é²æ£’æ€§æŠ€æœ¯ï¼š</b>Symlogå˜æ¢ç”¨äºå¤„ç†å¤§/å°å€¼ï¼ŒKLå¹³è¡¡ä¸è‡ªç”±bitsç”¨äºç¨³å®šçš„è¡¨ç¤ºå­¦ä¹ ï¼Œè¿”å›å½’ä¸€åŒ–ç”¨äºè§„æ¨¡ä¸å˜æ€§ï¼Œä»¥åŠtwohotå›å½’ç”¨äºå¤šæ¨¡æ€åˆ†å¸ƒã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯æ‰©å±•å®ç°ï¼š</b>è·¨é¢†åŸŸå›ºå®šè¶…å‚æ•°ï¼Œå¯é¢„æµ‹åœ°éšæ¨¡å‹å¤§å°å’Œè®¡ç®—æ‰©å±•ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿé€šè¿‡ç®€å•å¢åŠ èµ„æºæ¥æé«˜æ€§èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>åŸºäºæƒ³è±¡çš„è§„åˆ’ï¼š</b>é€šè¿‡ä»ä¸–ç•Œæ¨¡å‹é‡‡æ ·ç”Ÿæˆæƒ³è±¡è½¨è¿¹ï¼Œåœ¨æ— éœ€è®­ç»ƒæ•°æ®ä¹‹å¤–é¢å¤–ç¯å¢ƒäº¤äº’çš„æƒ…å†µä¸‹å­¦ä¹ ç­–ç•¥ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€å›¾ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
            <img src="Figures/dreamerv3_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/dreamerv3_overview_00.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <div style="height:1px; background:rgba(255,255,255,.10); margin:12px 0;"></div>
            <img src="Figures/dreamerv3_overview_01.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            
            <p>DreamerV3's world model learns to predict future observations and rewards, enabling imagination-based planning that generalizes across diverse domains with fixed hyperparameters.</p>
            <p>Note: DreamerV3 achieves state-of-the-art performance on Atari, ProcGen, DMLab, and Minecraft, including the first solution to collect diamonds in Minecraft from scratch without human data.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_dreamerv3.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_dreamerv3.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>DreamerV3's success stems from principled design choices that enable robust learning across diverse domains:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>World Model as Abstraction:</b> The RSSM provides a compressed, Markovian representation that filters out irrelevant details while preserving task-relevant information, enabling efficient planning in imagination.</li>
                <li style="margin-bottom:6px;"><b>Concurrent Learning:</b> Training world model, actor, and critic simultaneously allows each component to benefit from the others' improvements, creating a virtuous cycle of better representations and policies.</li>
                <li style="margin-bottom:6px;"><b>Robust Loss Functions:</b> Symlog transformations and twohot regression handle the wide range of reward scales and distributions across domains, preventing gradient instability and ensuring stable optimization.</li>
                <li style="margin-bottom:6px;"><b>KL Balancing:</b> Free bits and KL coefficient clipping prevent the world model from becoming overly conservative or unstable, enabling consistent representation learning across different data distributions.</li>
                <li style="margin-bottom:6px;"><b>Scale Invariance:</b> Return normalization and exploration entropy scaling ensure that the algorithm explores appropriately regardless of reward magnitude or density, enabling consistent performance across sparse and dense reward settings.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>DreamerV3çš„æˆåŠŸæºäºåŸåˆ™æ€§è®¾è®¡é€‰æ‹©ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–é¢†åŸŸä¸­è¿›è¡Œé²æ£’å­¦ä¹ ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>ä¸–ç•Œæ¨¡å‹ä½œä¸ºæŠ½è±¡ï¼š</b>RSSMæä¾›å‹ç¼©çš„é©¬å°”å¯å¤«è¡¨ç¤ºï¼Œè¿‡æ»¤æ‰æ— å…³ç»†èŠ‚åŒæ—¶ä¿ç•™ä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œå®ç°æƒ³è±¡ä¸­çš„é«˜æ•ˆè§„åˆ’ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¹¶å‘å­¦ä¹ ï¼š</b>åŒæ—¶è®­ç»ƒä¸–ç•Œæ¨¡å‹ã€Actorå’ŒCriticï¼Œä½¿æ¯ä¸ªç»„ä»¶éƒ½èƒ½ä»å…¶ä»–ç»„ä»¶çš„æ”¹è¿›ä¸­å—ç›Šï¼Œåˆ›é€ æ›´å¥½çš„è¡¨ç¤ºå’Œç­–ç•¥çš„è‰¯æ€§å¾ªç¯ã€‚</li>
                <li style="margin-bottom:6px;"><b>é²æ£’æŸå¤±å‡½æ•°ï¼š</b>Symlogå˜æ¢å’Œtwohotå›å½’å¤„ç†è·¨é¢†åŸŸçš„å¹¿æ³›å¥–åŠ±è§„æ¨¡å’Œåˆ†å¸ƒï¼Œé˜²æ­¢æ¢¯åº¦ä¸ç¨³å®šå¹¶ç¡®ä¿ç¨³å®šçš„ä¼˜åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>KLå¹³è¡¡ï¼š</b>è‡ªç”±bitså’ŒKLç³»æ•°è£å‰ªé˜²æ­¢ä¸–ç•Œæ¨¡å‹å˜å¾—è¿‡äºä¿å®ˆæˆ–ä¸ç¨³å®šï¼Œå®ç°è·¨ä¸åŒæ•°æ®åˆ†å¸ƒçš„ä¸€è‡´è¡¨ç¤ºå­¦ä¹ ã€‚</li>
                <li style="margin-bottom:6px;"><b>è§„æ¨¡ä¸å˜æ€§ï¼š</b>è¿”å›å½’ä¸€åŒ–å’Œæ¢ç´¢ç†µç¼©æ”¾ç¡®ä¿ç®—æ³•æ— è®ºå¥–åŠ±å¤§å°æˆ–å¯†åº¦å¦‚ä½•éƒ½è¿›è¡Œé€‚å½“æ¢ç´¢ï¼Œå®ç°è·¨ç¨€ç–å’Œå¯†é›†å¥–åŠ±è®¾ç½®çš„ä¸€è‡´æ€§èƒ½ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This landmark work presents DreamerV3, a transformative reinforcement learning algorithm that achieves unprecedented generalization across diverse domains with fixed hyperparameters. By learning world models that enable imagination-based planning, DreamerV3 overcomes the fundamental brittleness of traditional RL approaches, eliminating the need for domain-specific tuning and human expertise. The algorithm's remarkable achievement of collecting diamonds in Minecraft from scratchâ€”without human data, curricula, or domain knowledgeâ€”demonstrates the power of world model-based learning to tackle complex, open-ended problems that have long challenged the field of artificial intelligence. Through carefully engineered robustness techniques including symlog transformations, KL balancing, and return normalization, DreamerV3 provides a stable and predictable path to scaling RL performance simply by increasing computational resources. The work establishes world models as a cornerstone of general-purpose reinforcement learning, showing that learning to simulate and plan in imagination can be more powerful than direct interaction with the environment. DreamerV3 represents a significant step toward making reinforcement learning a truly general tool for solving real-world problems, with the potential to accelerate progress in robotics, game AI, and beyond. The algorithm's success across 150+ diverse tasksâ€”from Atari games and robotic control to the complex open world of Minecraftâ€”validates the world model approach and opens new avenues for developing more capable and adaptable artificial agents.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹é‡Œç¨‹ç¢‘å¼å·¥ä½œæå‡ºäº†DreamerV3ï¼Œè¿™æ˜¯ä¸€ç§å˜é©æ€§çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥å›ºå®šè¶…å‚æ•°å®ç°äº†è·¨å¤šæ ·åŒ–é¢†åŸŸå‰æ‰€æœªæœ‰çš„æ³›åŒ–ã€‚é€šè¿‡å­¦ä¹ èƒ½å¤Ÿå®ç°åŸºäºæƒ³è±¡è§„åˆ’çš„ä¸–ç•Œæ¨¡å‹ï¼ŒDreamerV3å…‹æœäº†ä¼ ç»ŸRLæ–¹æ³•çš„æ ¹æœ¬è„†å¼±æ€§ï¼Œæ¶ˆé™¤äº†å¯¹é¢†åŸŸç‰¹å®šè°ƒæ•´å’Œäººå·¥ä¸“ä¸šçŸ¥è¯†çš„éœ€æ±‚ã€‚è¯¥ç®—æ³•åœ¨Minecraftä¸­ä»é›¶å¼€å§‹æ”¶é›†é’»çŸ³çš„æ˜¾è‘—æˆå°±â€”â€”æ— éœ€äººç±»æ•°æ®ã€è¯¾ç¨‹æˆ–é¢†åŸŸçŸ¥è¯†â€”â€”è¯æ˜äº†åŸºäºä¸–ç•Œæ¨¡å‹çš„å­¦ä¹ è§£å†³å¤æ‚ã€å¼€æ”¾å¼é—®é¢˜çš„å¼ºå¤§èƒ½åŠ›ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é²æ£’æ€§æŠ€æœ¯ï¼ŒåŒ…æ‹¬symlogå˜æ¢ã€KLå¹³è¡¡å’Œè¿”å›å½’ä¸€åŒ–ï¼ŒDreamerV3æä¾›äº†ä¸€æ¡ç¨³å®šä¸”å¯é¢„æµ‹çš„è·¯å¾„ï¼Œé€šè¿‡ç®€å•å¢åŠ è®¡ç®—èµ„æºæ¥æ‰©å±•RLæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå°†ä¸–ç•Œæ¨¡å‹ç¡®ç«‹ä¸ºé€šç”¨å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€ï¼Œè¡¨æ˜å­¦ä¹ åœ¨æƒ³è±¡ä¸­æ¨¡æ‹Ÿå’Œè§„åˆ’å¯èƒ½æ¯”ç›´æ¥ä¸ç¯å¢ƒäº¤äº’æ›´å¼ºå¤§ã€‚DreamerV3ä»£è¡¨äº†ä½¿å¼ºåŒ–å­¦ä¹ æˆä¸ºè§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„çœŸæ­£é€šç”¨å·¥å…·çš„é‡è¦ä¸€æ­¥ï¼Œæœ‰æ½œåŠ›åŠ é€Ÿæœºå™¨äººã€æ¸¸æˆAIç­‰é¢†åŸŸçš„å‘å±•ã€‚è¯¥ç®—æ³•åœ¨150å¤šç§å¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„æˆåŠŸâ€”â€”ä»Atariæ¸¸æˆå’Œæœºå™¨äººæ§åˆ¶åˆ°Minecraftçš„å¤æ‚å¼€æ”¾ä¸–ç•Œâ€”â€”éªŒè¯äº†ä¸–ç•Œæ¨¡å‹æ–¹æ³•ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§å’Œæ›´å…·é€‚åº”æ€§çš„AIä»£ç†å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2301.04104" target="_blank" style="color:#8bffcf;">2301.04104</a></p>
          <p><b>Project Site:</b> <a href="https://danijar.com/dreamerv3" target="_blank" style="color:#8bffcf;">https://danijar.com/dreamerv3</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# DreamerV3 World Model Implementation<br>
<br>
// Recurrent State-Space Model (RSSM)<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">RSSM</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""World model that learns to predict future states and rewards"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, hidden_dim, stochastic_dim, deterministic_dim):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.encoder = Encoder()  <span style="color:#6a9955;"># CNN for image inputs</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.decoder = Decoder()  <span style="color:#6a9955;"># Decoder for reconstruction</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.sequence_model = GRUCell(hidden_dim, deterministic_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.dynamics_predictor = Linear(deterministic_dim, stochastic_dim * 2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.reward_predictor = Linear(deterministic_dim + stochastic_dim, reward_bins)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.continue_predictor = Linear(deterministic_dim + stochastic_dim, 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, obs, action, prev_state):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Predict next state, reward, and continuation"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stochastic_prev, deterministic_prev = prev_state<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Encode observation</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;encoded_obs = self.encoder(obs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stochastic_posterior = self._sample_posterior(encoded_obs, deterministic_prev)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Predict next deterministic state</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deterministic_next = self.sequence_model(stochastic_prev, action, deterministic_prev)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Predict next stochastic state (dynamics)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stochastic_prior = self._sample_prior(deterministic_next)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Predict reward and continuation</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;state_combined = torch.cat([deterministic_next, stochastic_prior], dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reward_logits = self.reward_predictor(state_combined)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue_logits = self.continue_predictor(state_combined)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> (stochastic_posterior, deterministic_next), reward_logits, continue_logits<br>
<br>
// Symlog Transformation for Robust Predictions<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">symlog</span>(x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Symmetric logarithmic transformation for handling large/small values"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> torch.sign(x) * torch.log(torch.abs(x) + 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">symexp</span>(x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Inverse of symlog transformation"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> torch.sign(x) * (torch.exp(torch.abs(x)) - 1)<br>
<br>
// Twohot Regression for Multi-modal Value Distributions<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">TwohotPredictor</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Predicts value distributions using twohot encoding over exponentially spaced bins"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, bins=255):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bins = bins<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.bin_centers = symexp(torch.linspace(-20, 20, bins))<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Predict logits over bins"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logits = self.network(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probs = F.softmax(logits, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value = torch.sum(probs * self.bin_centers, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> value, logits<br>
<br>
// Imagination-based Actor-Critic Training<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">DreamerV3</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Complete DreamerV3 algorithm with world model and actor-critic"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__self, world_model, actor, critic, imagination_horizon=15):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.world_model = world_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.actor = actor<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.critic = critic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.horizon = imagination_horizon<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">imagine_trajectory</span>(self, initial_state, actions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Generate imagined trajectory using world model"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;states = [initial_state]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continues = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> action <span style="color:#569cd6;">in</span> actions:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next_state, reward_logits, continue_logits = self.world_model(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;states[-1][0], action, states[-1])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;states.append(next_state)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards.append(reward_logits)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continues.append(continue_logits)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> states, rewards, continues<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">compute_lambda_returns</span>(self, rewards, continues, values):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Compute Î»-returns with discount Î³ and Î» parameters"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returns = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gamma, lam = 0.997, 0.95<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> t <span style="color:#569cd6;">in</span> reversed(range(len(rewards))):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">if</span> t == len(rewards) - 1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next_return = values[t+1]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">else</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next_return = returns[0]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return_t = rewards[t] + gamma * continues[t] * (<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1 - lam) * values[t] + lam * next_return)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;returns.insert(0, return_t)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> returns<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
