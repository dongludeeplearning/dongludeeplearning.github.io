<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ CVPR 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">HSI-GPT: A General-Purpose Large Scene-Motion-Language Model for Human Scene Interaction</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Yuan Wang, Yali Li, Xiang Li, Shengjin Wang<br>
        <span style="opacity:0.8">Tsinghua University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to create a general-purpose model that can handle diverse human-scene interaction tasks with multiple control modalities using large language models?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åˆ›å»ºä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å…·æœ‰å¤šç§æ§åˆ¶æ¨¡æ€çš„å¤šæ ·åŒ–äººç±»-åœºæ™¯äº¤äº’ä»»åŠ¡ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>General-Purpose HSI Model:</b> First Large Scene-Motion-Language Model that applies LLM's next-token prediction paradigm to HSI domain, unifying diverse control signals and supporting multiple HSI-related tasks.</div>
            <div class="lang-zh" style="display:none"><b>é€šç”¨HSIæ¨¡å‹ï¼š</b>é¦–ä¸ªå¤§å‹åœºæ™¯-è¿åŠ¨-è¯­è¨€æ¨¡å‹ï¼Œå°†LLMçš„next-tokené¢„æµ‹èŒƒå¼åº”ç”¨åˆ°HSIé¢†åŸŸï¼Œç»Ÿä¸€å¤šç§æ§åˆ¶ä¿¡å·å¹¶æ”¯æŒå¤šç§HSIç›¸å…³ä»»åŠ¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Modal Tokenization:</b> Motion-aware VQ-VAE for discretizing human motions into LLM-interpretable tokens, combined with 3D scene and affordance encoding for comprehensive HSI representation.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€tokenizationï¼š</b>è¿åŠ¨æ„ŸçŸ¥VQ-VAEå°†äººç±»è¿åŠ¨ç¦»æ•£åŒ–ä¸ºLLMå¯è§£é‡Šçš„tokenï¼Œç»“åˆ3Dåœºæ™¯å’Œå¯åŠæ€§ç¼–ç ï¼Œå®ç°å…¨é¢çš„HSIè¡¨ç¤ºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Modal Alignment:</b> Multi-modal Interaction Aggregator (MIA) and extended motion-language vocabulary to align 3D scenes, human motions, and language into shared LLM embedding space.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€å¯¹é½ï¼š</b>å¤šæ¨¡æ€äº¤äº’èšåˆå™¨(MIA)å’Œæ‰©å±•çš„è¿åŠ¨-è¯­è¨€è¯æ±‡è¡¨ï¼Œå°†3Dåœºæ™¯ã€äººç±»è¿åŠ¨å’Œè¯­è¨€å¯¹é½åˆ°å…±äº«çš„LLMåµŒå…¥ç©ºé—´ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Three-Stage Training Pipeline:</b> Progressive training strategy including multimodal tokenizers training, scene-motion-language alignment, and instruction tuning for comprehensive HSI capabilities.</div>
            <div class="lang-zh" style="display:none"><b>ä¸‰é˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼š</b>æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€tokenizersè®­ç»ƒã€åœºæ™¯-è¿åŠ¨-è¯­è¨€å¯¹é½å’ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å®ç°å…¨é¢çš„HSIèƒ½åŠ›ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Unified Instruction Framework:</b> Task-agnostic instruction templates that enable flexible control with diverse modalities (scenes, text, key-frames, affordances) and support various HSI tasks uniformly.</div>
            <div class="lang-zh" style="display:none"><b>ç»Ÿä¸€æŒ‡ä»¤æ¡†æ¶ï¼š</b>ä¸ä»»åŠ¡æ— å…³çš„æŒ‡ä»¤æ¨¡æ¿ï¼Œèƒ½å¤Ÿä½¿ç”¨å¤šç§æ¨¡æ€ï¼ˆåœºæ™¯ã€æ–‡æœ¬ã€å…³é”®å¸§ã€å¯åŠæ€§ï¼‰è¿›è¡Œçµæ´»æ§åˆ¶ï¼Œå¹¶ç»Ÿä¸€æ”¯æŒå„ç§HSIä»»åŠ¡ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/hsi_gpt_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Limited Control Modalities:</b> Existing HSI methods constrained to single control conditions (e.g., only text), lacking flexibility to integrate multiple modalities like 3D scenes, key-frame poses, and affordances.</div>
            <div class="lang-zh" style="display:none"><b>æœ‰é™çš„æ§åˆ¶æ¨¡æ€ï¼š</b>ç°æœ‰çš„HSIæ–¹æ³•ä»…é™äºå•ä¸ªæ§åˆ¶æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä»…æ–‡æœ¬ï¼‰ï¼Œç¼ºä¹é›†æˆå¤šç§æ¨¡æ€ï¼ˆå¦‚3Dåœºæ™¯ã€å…³é”®å¸§å§¿åŠ¿å’Œå¯åŠæ€§ï¼‰çš„çµæ´»æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Task-Specific Frameworks:</b> Current approaches design separate models for different HSI tasks, lacking cross-task generalization and scalability across captioning, generation, and completion tasks.</div>
            <div class="lang-zh" style="display:none"><b>ç‰¹å®šä»»åŠ¡æ¡†æ¶ï¼š</b>å½“å‰æ–¹æ³•ä¸ºä¸åŒçš„HSIä»»åŠ¡è®¾è®¡å•ç‹¬çš„æ¨¡å‹ï¼Œç¼ºä¹è·¨ä»»åŠ¡æ³›åŒ–å’Œåœ¨å­—å¹•ç”Ÿæˆã€ç”Ÿæˆå’Œè¡¥å…¨ä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Isolated Modality Processing:</b> Previous methods treat 3D scenes, human motion, and language as separate modalities, failing to capture their complex interplay and resulting in physically inconsistent motions.</div>
            <div class="lang-zh" style="display:none"><b>å­¤ç«‹çš„æ¨¡æ€å¤„ç†ï¼š</b>ä»¥å‰çš„æ–¹æ³•å°†3Dåœºæ™¯ã€äººç±»è¿åŠ¨å’Œè¯­è¨€è§†ä¸ºåˆ†ç¦»çš„æ¨¡æ€ï¼Œæ— æ³•æ•æ‰å®ƒä»¬ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´ç‰©ç†ä¸Šä¸ä¸€è‡´çš„è¿åŠ¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Lack of LLM Integration:</b> Existing HSI models fail to leverage general world knowledge from LLMs, limiting their ability to understand semantic relationships and perform reasoning in 3D environments.</div>
            <div class="lang-zh" style="display:none"><b>ç¼ºä¹LLMé›†æˆï¼š</b>ç°æœ‰çš„HSIæ¨¡å‹æ— æ³•åˆ©ç”¨LLMçš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†ï¼Œé™åˆ¶äº†å®ƒä»¬ç†è§£è¯­ä¹‰å…³ç³»å’Œåœ¨3Dç¯å¢ƒä¸­æ‰§è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Descriptive Unfaithfulness:</b> Generated motions are not faithful to textual descriptions and physically implausible in diverse 3D environments due to inadequate exploration of scene-motion-language relationships.</div>
            <div class="lang-zh" style="display:none"><b>æè¿°ä¸å¿ å®ï¼š</b>ç”±äºå¯¹åœºæ™¯-è¿åŠ¨-è¯­è¨€å…³ç³»çš„æ¢ç´¢ä¸è¶³ï¼Œç”Ÿæˆçš„è¿åŠ¨ä¸æ–‡æœ¬æè¿°ä¸ç¬¦ï¼Œåœ¨å¤šæ ·åŒ–çš„3Dç¯å¢ƒä¸­ç‰©ç†ä¸Šä¸åˆç†ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Modal Tokenization:</b> Motion-aware VQ-VAE quantizes continuous human motions into discrete LLM-interpretable tokens, while 3D scenes and affordance maps are encoded into visual tokens.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€Tokenizationï¼š</b>è¿åŠ¨æ„ŸçŸ¥VQ-VAEå°†è¿ç»­çš„äººç±»è¿åŠ¨é‡åŒ–ä¸ºç¦»æ•£çš„LLMå¯è§£é‡Štokenï¼ŒåŒæ—¶3Dåœºæ™¯å’Œå¯åŠæ€§åœ°å›¾è¢«ç¼–ç ä¸ºè§†è§‰tokenã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scene-Language Interaction:</b> Multi-modal Interaction Aggregator (MIA) aligns permutation-invariant 3D scene features with LLM text embeddings using learnable query tokens and cross-attention mechanisms.</div>
            <div class="lang-zh" style="display:none"><b>åœºæ™¯-è¯­è¨€äº¤äº’ï¼š</b>å¤šæ¨¡æ€äº¤äº’èšåˆå™¨(MIA)ä½¿ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢tokenå’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ’åˆ—ä¸å˜çš„3Dåœºæ™¯ç‰¹å¾ä¸LLMæ–‡æœ¬åµŒå…¥å¯¹é½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Extended Motion-Language Vocabulary:</b> Joint vocabulary combining text and motion tokens with special markers, enabling seamless "any-to-any" translation between motion and language modalities.</div>
            <div class="lang-zh" style="display:none"><b>æ‰©å±•çš„è¿åŠ¨-è¯­è¨€è¯æ±‡è¡¨ï¼š</b>ç»“åˆæ–‡æœ¬å’Œè¿åŠ¨tokençš„è”åˆè¯æ±‡è¡¨å¸¦æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œå®ç°è¿åŠ¨å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„æ— ç¼"any-to-any"è½¬æ¢ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Three-Stage Training:</b> Progressive pipeline including multimodal tokenizers training for motion discretization, scene-motion-language alignment for modality fusion, and instruction tuning for task generalization.</div>
            <div class="lang-zh" style="display:none"><b>ä¸‰é˜¶æ®µè®­ç»ƒï¼š</b>æ¸è¿›å¼æµæ°´çº¿åŒ…æ‹¬ç”¨äºè¿åŠ¨ç¦»æ•£åŒ–çš„å¤šæ¨¡æ€tokenizersè®­ç»ƒã€ç”¨äºæ¨¡æ€èåˆçš„åœºæ™¯-è¿åŠ¨-è¯­è¨€å¯¹é½ï¼Œä»¥åŠç”¨äºä»»åŠ¡æ³›åŒ–çš„æŒ‡ä»¤å¾®è°ƒã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Unified Instruction Framework:</b> Task-agnostic templates that integrate diverse control signals into coherent prompts, enabling flexible multi-modal HSI generation and understanding.</div>
            <div class="lang-zh" style="display:none"><b>ç»Ÿä¸€æŒ‡ä»¤æ¡†æ¶ï¼š</b>ä¸ä»»åŠ¡æ— å…³çš„æ¨¡æ¿ï¼Œå°†å¤šç§æ§åˆ¶ä¿¡å·é›†æˆåˆ°è¿è´¯çš„æç¤ºä¸­ï¼Œå®ç°çµæ´»çš„å¤šæ¨¡æ€HSIç”Ÿæˆå’Œç†è§£ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/hsi_gpt_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('cvpr2025_hsi_gpt.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('cvpr2025_hsi_gpt.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <b>Key Advantages:</b> HSI-GPT pioneers the integration of LLMs with 3D scene understanding and human motion generation, achieving unprecedented flexibility across multiple HSI tasks. By unifying diverse control modalities into a single framework, it overcomes the limitations of task-specific approaches and enables seamless transitions between different interaction scenarios. The three-stage training pipeline ensures robust alignment of multimodal representations, leading to physically realistic and semantically coherent motion generation.

            <br><br>
            <b>Technical Innovations:</b> The motion-aware VQ-VAE and Multi-modal Interaction Aggregator represent significant advances in multimodal learning for embodied AI. The extended motion-language vocabulary enables novel capabilities like motion captioning and completion, while the instruction tuning approach allows for zero-shot generalization to unseen scenarios.

            <br><br>
            <b>Limitations:</b> While highly flexible, the model's performance may be constrained by the quality of underlying 3D scene representations and motion capture data. The computational overhead of processing point clouds and affordance maps could be optimized for real-time applications.

            <br><br>
            <b>Future Impact:</b> HSI-GPT establishes a new paradigm for human-scene interaction modeling, with applications in robotics, virtual reality, and content creation. The unified framework could inspire similar multimodal integrations in other domains requiring complex scene understanding and physical reasoning.
          </div>
          <div class="lang-zh" style="display:none">
            <b>å…³é”®ä¼˜åŠ¿ï¼š</b>HSI-GPTå¼€åˆ›äº†LLMä¸3Dåœºæ™¯ç†è§£å’Œäººç±»è¿åŠ¨ç”Ÿæˆçš„é›†æˆï¼Œåœ¨å¤šä¸ªHSIä»»åŠ¡ä¸­å®ç°äº†å‰æ‰€æœªæœ‰çš„çµæ´»æ€§ã€‚é€šè¿‡å°†å¤šç§æ§åˆ¶æ¨¡æ€ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ï¼Œå®ƒå…‹æœäº†ç‰¹å®šä»»åŠ¡æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å®ç°äº†ä¸åŒäº¤äº’åœºæ™¯ä¹‹é—´çš„æ— ç¼è½¬æ¢ã€‚ä¸‰é˜¶æ®µè®­ç»ƒæµæ°´çº¿ç¡®ä¿äº†å¤šæ¨¡æ€è¡¨ç¤ºçš„ç¨³å¥å¯¹é½ï¼Œå¯¼è‡´ç‰©ç†ä¸ŠçœŸå®å’Œè¯­ä¹‰ä¸Šä¸€è‡´çš„è¿åŠ¨ç”Ÿæˆã€‚

            <br><br>
            <b>æŠ€æœ¯åˆ›æ–°ï¼š</b>è¿åŠ¨æ„ŸçŸ¥VQ-VAEå’Œå¤šæ¨¡æ€äº¤äº’èšåˆå™¨ä»£è¡¨äº†å…·èº«AIå¤šæ¨¡æ€å­¦ä¹ çš„é‡è¦è¿›æ­¥ã€‚æ‰©å±•çš„è¿åŠ¨-è¯­è¨€è¯æ±‡è¡¨å®ç°äº†è¿åŠ¨å­—å¹•å’Œè¡¥å…¨ç­‰æ–°é¢–èƒ½åŠ›ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•å…è®¸å¯¹æœªè§åœºæ™¯çš„é›¶-shotæ³›åŒ–ã€‚

            <br><br>
            <b>å±€é™æ€§ï¼š</b>è™½ç„¶é«˜åº¦çµæ´»ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯èƒ½å—åˆ°åº•å±‚3Dåœºæ™¯è¡¨ç¤ºå’Œè¿åŠ¨æ•æ‰æ•°æ®è´¨é‡çš„é™åˆ¶ã€‚å¤„ç†ç‚¹äº‘å’Œå¯åŠæ€§åœ°å›¾çš„è®¡ç®—å¼€é”€å¯ä»¥é’ˆå¯¹å®æ—¶åº”ç”¨è¿›è¡Œä¼˜åŒ–ã€‚

            <br><br>
            <b>æœªæ¥å½±å“ï¼š</b>HSI-GPTä¸ºäººç±»-åœºæ™¯äº¤äº’å»ºæ¨¡å»ºç«‹äº†æ–°èŒƒå¼ï¼Œåœ¨æœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå†…å®¹åˆ›ä½œæ–¹é¢å…·æœ‰åº”ç”¨å‰æ™¯ã€‚ç»Ÿä¸€æ¡†æ¶å¯èƒ½å¯å‘å…¶ä»–é¢†åŸŸç±»ä¼¼çš„å¤šæ¨¡æ€é›†æˆï¼Œè¿™äº›é¢†åŸŸéœ€è¦å¤æ‚çš„åœºæ™¯ç†è§£å’Œç‰©ç†æ¨ç†ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            HSI-GPT represents a transformative approach to human-scene interaction modeling by applying large language model principles to the synthesis of physically realistic, controllable motions in 3D environments. The innovative integration of motion tokenization, multimodal alignment, and instruction tuning enables unprecedented flexibility across diverse HSI tasks, from text-conditioned generation to motion completion and understanding. By establishing a unified framework that accommodates multiple control modalitiesâ€”3D scenes, textual commands, key-frame poses, and scene affordancesâ€”HSI-GPT overcomes fundamental limitations of previous approaches that relied on task-specific models with constrained control signals. Extensive experiments validate the model's exceptional performance across multiple HSI-related tasks, demonstrating superior physical plausibility and semantic coherence compared to existing methods. The work establishes HSI-GPT as the first general-purpose model for human-scene interaction, opening new possibilities for embodied AI applications and setting a foundation for future research in multimodal scene understanding and interaction synthesis. The open-source nature of the implementation will facilitate further advancements in this rapidly evolving field.
          </div>
          <div class="lang-zh" style="display:none">
            HSI-GPTé€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹åŸç†åº”ç”¨äº3Dç¯å¢ƒä¸­ç‰©ç†ä¸ŠçœŸå®ã€å¯æ§è¿åŠ¨çš„åˆæˆï¼Œä»£è¡¨äº†äººç±»-åœºæ™¯äº¤äº’å»ºæ¨¡çš„å˜é©æ€§æ–¹æ³•ã€‚è¿åŠ¨tokenizationã€å¤šæ¨¡æ€å¯¹é½å’ŒæŒ‡ä»¤å¾®è°ƒçš„åˆ›æ–°é›†æˆå®ç°äº†åœ¨å¤šæ ·åŒ–HSIä»»åŠ¡ä¸­çš„å‰æ‰€æœªæœ‰çµæ´»æ€§ï¼Œä»æ–‡æœ¬æ¡ä»¶ç”Ÿæˆåˆ°è¿åŠ¨è¡¥å…¨å’Œç†è§£ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå®¹çº³å¤šç§æ§åˆ¶æ¨¡æ€â€”â€”3Dåœºæ™¯ã€æ–‡æœ¬å‘½ä»¤ã€å…³é”®å¸§å§¿åŠ¿å’Œåœºæ™¯å¯åŠæ€§â€”â€”HSI-GPTå…‹æœäº†ä»¥å‰æ–¹æ³•çš„æ ¹æœ¬å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå…·æœ‰å—é™æ§åˆ¶ä¿¡å·çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ã€‚å¤§é‡å®éªŒéªŒè¯äº†æ¨¡å‹åœ¨å¤šä¸ªHSIç›¸å…³ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æ˜¾ç¤ºå‡ºæ›´å¥½çš„ç‰©ç†åˆç†æ€§å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚è¯¥å·¥ä½œå°†HSI-GPTç¡®ç«‹ä¸ºé¦–ä¸ªç”¨äºäººç±»-åœºæ™¯äº¤äº’çš„é€šç”¨æ¨¡å‹ï¼Œä¸ºå…·èº«AIåº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€åœºæ™¯ç†è§£å’Œäº¤äº’åˆæˆé¢†åŸŸçš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚å®ç°çš„å¼€æºæ€§è´¨å°†ä¿ƒè¿›è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„è¿›ä¸€æ­¥è¿›æ­¥ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>LLM as Universal Interface:</b> The core insight is that LLMs excel at unifying diverse modalities through text tokens. By converting 3D scenes, human motions, and affordances into a shared token space, HSI-GPT transforms complex multimodal reasoning into next-token prediction, enabling unprecedented flexibility across HSI tasks.
            </div>
            <div class="lang-zh" style="display:none">
                <b>LLMä½œä¸ºé€šç”¨æ¥å£ï¼š</b>æ ¸å¿ƒæ´å¯Ÿæ˜¯LLMæ“…é•¿é€šè¿‡æ–‡æœ¬tokenç»Ÿä¸€ä¸åŒæ¨¡æ€ã€‚é€šè¿‡å°†3Dåœºæ™¯ã€äººç±»è¿åŠ¨å’Œå¯åŠæ€§è½¬æ¢ä¸ºå…±äº«tokenç©ºé—´ï¼ŒHSI-GPTå°†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†è½¬æ¢ä¸ºnext-tokené¢„æµ‹ï¼Œä»è€Œåœ¨HSIä»»åŠ¡ä¸­å®ç°å‰æ‰€æœªæœ‰çš„çµæ´»æ€§ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Discrete Motion Representation:</b> Motion tokenization via VQ-VAE bridges the continuous motion domain with discrete language tokens, enabling motion generation, completion, and understanding within a unified autoregressive framework.
            </div>
            <div class="lang-zh" style="display:none">
                <b>ç¦»æ•£è¿åŠ¨è¡¨ç¤ºï¼š</b>é€šè¿‡VQ-VAEè¿›è¡Œè¿åŠ¨tokenizationå°†è¿ç»­è¿åŠ¨åŸŸä¸ç¦»æ•£è¯­è¨€tokenè¿æ¥èµ·æ¥ï¼Œä»è€Œåœ¨ç»Ÿä¸€çš„autoregressiveæ¡†æ¶ä¸­å®ç°è¿åŠ¨ç”Ÿæˆã€è¡¥å…¨å’Œç†è§£ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Progressive Training Strategy:</b> The three-stage pipeline (tokenization â†’ alignment â†’ instruction tuning) ensures stable learning of complex multimodal interactions, with each stage building upon the previous one's representations.
            </div>
            <div class="lang-zh" style="display:none">
                <b>æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼š</b>ä¸‰é˜¶æ®µæµæ°´çº¿ï¼ˆtokenization â†’ å¯¹é½ â†’ æŒ‡ä»¤å¾®è°ƒï¼‰ç¡®ä¿å¤æ‚å¤šæ¨¡æ€äº¤äº’çš„ç¨³å®šå­¦ä¹ ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½å»ºç«‹åœ¨å‰ä¸€é˜¶æ®µçš„è¡¨ç¤ºä¹‹ä¸Šã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Scene Affordance Integration:</b> By explicitly modeling scene affordances, the model understands physical constraints and interaction possibilities, leading to more realistic and contextually appropriate motion generation.
            </div>
            <div class="lang-zh" style="display:none">
                <b>åœºæ™¯å¯åŠæ€§é›†æˆï¼š</b>é€šè¿‡æ˜¾å¼å»ºæ¨¡åœºæ™¯å¯åŠæ€§ï¼Œæ¨¡å‹ç†è§£ç‰©ç†çº¦æŸå’Œäº¤äº’å¯èƒ½æ€§ï¼Œä»è€Œå®ç°æ›´çœŸå®å’Œä¸Šä¸‹æ–‡é€‚å½“çš„è¿åŠ¨ç”Ÿæˆã€‚
            </div>
          </li>
        </ul>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because HSI-GPT leverages the LLM's proven ability to model complex relationships and generate coherent sequences. By discretizing motions and projecting all modalities into the LLM's token space, it transforms the notoriously difficult problem of multimodal scene-motion reasoning into the well-solved domain of language modeling. The key is treating "human motion in 3D scenes" as a form of "non-verbal language" that LLMs can learn to generate, complete, and understand through unified token prediction.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºHSI-GPTåˆ©ç”¨äº†LLMå»ºæ¨¡å¤æ‚å…³ç³»å’Œç”Ÿæˆè¿è´¯åºåˆ—çš„æˆç†Ÿèƒ½åŠ›ã€‚é€šè¿‡ç¦»æ•£åŒ–è¿åŠ¨å¹¶å°†æ‰€æœ‰æ¨¡æ€æŠ•å½±åˆ°LLMçš„tokenç©ºé—´ï¼Œå®ƒå°†å¤šæ¨¡æ€åœºæ™¯-è¿åŠ¨æ¨ç†è¿™ä¸€å…¬è®¤çš„éš¾é¢˜è½¬æ¢ä¸ºè¯­è¨€å»ºæ¨¡è¿™ä¸€å·²è§£å†³é¢†åŸŸã€‚å…³é”®æ˜¯å°†"3Dåœºæ™¯ä¸­çš„äººç±»è¿åŠ¨"è§†ä¸ºLLMå¯ä»¥é€šè¿‡ç»Ÿä¸€tokené¢„æµ‹æ¥å­¦ä¹ ç”Ÿæˆã€è¡¥å…¨å’Œç†è§£çš„ä¸€ç§"éè¯­è¨€å½¢å¼"ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="font-family:ui-monospace,monospace;font-size:13px;color:rgba(232,236,255,.8);background:rgba(0,0,0,.3);padding:16px;border-radius:8px;border:1px solid rgba(255,255,255,.05);">
          <div class="lang-en">
            <pre><code>import torch
import torch.nn as nn
from transformers import LLaMAForCausalLM

class HSI_GPT(nn.Module):
    """General-Purpose Large Scene-Motion-Language Model"""

    def __init__(self, motion_vocab_size=1024, scene_dim=768):
        super().__init__()
        # Multimodal encoders
        self.scene_encoder = PointCloudEncoder()          # 3D scene encoding
        self.motion_tokenizer = MotionVQVAE(vocab_size=motion_vocab_size)  # Motion tokenization
        self.affordance_encoder = AffordanceEncoder()     # Scene affordance encoding
        self.language_model = LLaMAForCausalLM.from_pretrained('llama')

        # Alignment modules
        self.scene_adapter = SceneAdapter(scene_dim, 4096)
        self.motion_adapter = MotionAdapter(motion_vocab_size, 4096)

        # Interaction aggregator
        self.interaction_aggregator = InteractionAggregator()

    def forward(self, scene, motion, text, affordance=None):
        # 1. Multimodal encoding
        scene_features = self.scene_encoder(scene)                    # [B, D_scene]
        motion_tokens = self.motion_tokenizer.tokenize(motion)       # [B, seq_len]
        text_tokens = self.language_model.tokenize(text)             # [B, L]

        # 2. Affordance integration (optional)
        if affordance is not None:
            affordance_features = self.affordance_encoder(affordance)
            scene_features = scene_features + affordance_features

        # 3. Cross-modal alignment
        aligned_scene = self.scene_adapter(scene_features, text_tokens)
        aligned_motion = self.motion_adapter(motion_tokens, text_tokens)

        # 4. Multimodal interaction
        interaction_features = self.interaction_aggregator(
            aligned_scene, aligned_motion, text_tokens
        )

        # 5. Language model generation
        output = self.language_model.generate(
            inputs=interaction_features,
            max_length=interaction_features.size(1) + 100
        )

        return output

class MotionVQVAE(nn.Module):
    """Vector Quantized Variational Autoencoder for Motion"""

    def __init__(self, vocab_size=1024, hidden_dim=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(135, hidden_dim, 3, padding=1),  # SMPL joints
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, 3, padding=1),
        )
        self.codebook = nn.Embedding(vocab_size, hidden_dim)
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(hidden_dim, hidden_dim, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(hidden_dim, 135, 3, padding=1),
        )

    def tokenize(self, motion):
        """Convert continuous motion to discrete tokens"""
        # motion: [B, T, 135]
        features = self.encoder(motion.transpose(1, 2))  # [B, hidden_dim, T]
        features = features.transpose(1, 2)  # [B, T, hidden_dim]

        # Vector quantization
        distances = torch.cdist(features, self.codebook.weight.unsqueeze(0))
        tokens = torch.argmin(distances, dim=-1)  # [B, T]

        return tokens

    def reconstruct(self, tokens):
        """Reconstruct motion from tokens"""
        embeddings = self.codebook(tokens)  # [B, T, hidden_dim]
        embeddings = embeddings.transpose(1, 2)  # [B, hidden_dim, T]

        motion = self.decoder(embeddings)  # [B, 135, T]
        motion = motion.transpose(1, 2)  # [B, T, 135]

        return motion

class InteractionAggregator(nn.Module):
    """Multimodal Interaction Aggregator"""

    def __init__(self, embed_dim=4096, num_heads=8):
        super().__init__()
        self.scene_proj = nn.Linear(768, embed_dim)
        self.motion_proj = nn.Linear(embed_dim, embed_dim)

        # Cross-modal attention
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            batch_first=True
        )

        self.output_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, scene_features, motion_features, text_features):
        # Project to common dimension
        scene_emb = self.scene_proj(scene_features)      # [B, seq_len, embed_dim]
        motion_emb = self.motion_proj(motion_features)   # [B, seq_len, embed_dim]

        # Cross-modal interaction via attention
        attended_scene, _ = self.cross_attention(
            query=scene_emb,
            key=motion_emb,
            value=motion_emb
        )

        # Combine with text features
        combined = attended_scene + text_features

        # Final projection
        output = self.output_proj(combined)

        return output

class SceneAdapter(nn.Module):
    """Scene-to-Language Adapter"""

    def __init__(self, scene_dim=768, llm_dim=4096):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(scene_dim, llm_dim),
            nn.LayerNorm(llm_dim),
            nn.ReLU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, scene_features, text_tokens):
        adapted_scene = self.adapter(scene_features)
        return adapted_scene

class MotionAdapter(nn.Module):
    """Motion-to-Language Adapter"""

    def __init__(self, vocab_size=1024, llm_dim=4096):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, llm_dim)
        self.adapter = nn.Sequential(
            nn.LayerNorm(llm_dim),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, motion_tokens, text_tokens):
        motion_emb = self.embedding(motion_tokens)
        adapted_motion = self.adapter(motion_emb)
        return adapted_motion

# Three-stage training pipeline
def train_hsi_gpt():
    """HSI-GPT training pipeline"""

    model = HSI_GPT()

    # Stage 1: Pre-train tokenizers
    print("Stage 1: Training motion tokenizer...")
    motion_optimizer = torch.optim.Adam(model.motion_tokenizer.parameters())

    for epoch in range(100):
        for batch in motion_dataset:
            tokens = model.motion_tokenizer.tokenize(batch['motion'])
            reconstructed = model.motion_tokenizer.reconstruct(tokens)

            # VQ-VAE loss
            recon_loss = F.mse_loss(reconstructed, batch['motion'])
            vq_loss = compute_vq_loss(tokens, model.motion_tokenizer.codebook)

            loss = recon_loss + 0.25 * vq_loss
            motion_optimizer.zero_grad()
            loss.backward()
            motion_optimizer.step()

    # Stage 2: Multimodal alignment
    print("Stage 2: Aligning modalities...")
    full_optimizer = torch.optim.Adam([
        {'params': model.scene_encoder.parameters()},
        {'params': model.scene_adapter.parameters()},
        {'params': model.motion_adapter.parameters()},
        {'params': model.interaction_aggregator.parameters()},
        {'params': model.language_model.parameters(), 'lr': 1e-5}
    ])

    for epoch in range(50):
        for batch in multimodal_dataset:
            scene, motion, text = batch['scene'], batch['motion'], batch['text']

            output = model(scene, motion, text)
            target = batch['target']

            # Language modeling loss
            loss = F.cross_entropy(output.view(-1, output.size(-1)), target.view(-1))

            full_optimizer.zero_grad()
            loss.backward()
            full_optimizer.step()

    # Stage 3: Instruction tuning
    print("Stage 3: Instruction tuning with LoRA...")
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=16, lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05
    )
    model = get_peft_model(model, lora_config)

    instruction_templates = {
        'generation': "Generate motion for '{}' in scene: {}",
        'completion': "Complete motion {} in scene: {}",
        'captioning': "Describe motion {} in scene: {}",
        'control': "Generate motion with {} control: {}"
    }

    lora_optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

    for epoch in range(10):
        for batch in instruction_dataset:
            task = batch['task']
            template = instruction_templates[task]

            instruction = template.format(batch['condition'], batch['context'])
            response = model.generate(instruction, scene=batch.get('scene'))

            loss = instruction_following_loss(response, batch['target'])
            lora_optimizer.zero_grad()
            loss.backward()
            lora_optimizer.step()

    return model

# Usage example
if __name__ == "__main__":
    model = train_hsi_gpt()

    # Text-conditioned HSI generation
    scene = load_3d_scene("living_room.obj")
    instruction = "A person walks to the chair and sits down"
    motion = model.generate_motion(scene, instruction)

    # Multi-modal controlled generation
    keyframe_poses = load_keyframes("sit_sequence.npy")
    affordance_map = compute_affordance_map(scene)
    motion = model.generate_with_controls(scene, instruction,
                                        keyframes=keyframe_poses,
                                        affordance=affordance_map)

    print(f"Generated motion with {motion.shape[0]} frames")</code></pre>
          </div>
          <div class="lang-zh" style="display:none">
            <pre><code>import torch
import torch.nn as nn
from transformers import LLaMAForCausalLM

class HSI_GPT(nn.Module):
    """é€šç”¨åœºæ™¯-è¿åŠ¨-è¯­è¨€å¤§å‹æ¨¡å‹"""

    def __init__(self, motion_vocab_size=1024, scene_dim=768):
        super().__init__()
        # å¤šæ¨¡æ€ç¼–ç å™¨
        self.scene_encoder = PointCloudEncoder()          # 3Dåœºæ™¯ç¼–ç 
        self.motion_tokenizer = MotionVQVAE(vocab_size=motion_vocab_size)  # è¿åŠ¨tokenization
        self.affordance_encoder = AffordanceEncoder()     # åœºæ™¯å¯åŠæ€§ç¼–ç 
        self.language_model = LLaMAForCausalLM.from_pretrained('llama')

        # å¯¹é½æ¨¡å—
        self.scene_adapter = SceneAdapter(scene_dim, 4096)
        self.motion_adapter = MotionAdapter(motion_vocab_size, 4096)

        # äº¤äº’èšåˆå™¨
        self.interaction_aggregator = InteractionAggregator()

    def forward(self, scene, motion, text, affordance=None):
        # 1. å¤šæ¨¡æ€ç¼–ç 
        scene_features = self.scene_encoder(scene)                    # [B, D_scene]
        motion_tokens = self.motion_tokenizer.tokenize(motion)       # [B, seq_len]
        text_tokens = self.language_model.tokenize(text)             # [B, L]

        # 2. å¯åŠæ€§é›†æˆï¼ˆå¯é€‰ï¼‰
        if affordance is not None:
            affordance_features = self.affordance_encoder(affordance)
            scene_features = scene_features + affordance_features

        # 3. è·¨æ¨¡æ€å¯¹é½
        aligned_scene = self.scene_adapter(scene_features, text_tokens)
        aligned_motion = self.motion_adapter(motion_tokens, text_tokens)

        # 4. å¤šæ¨¡æ€äº¤äº’
        interaction_features = self.interaction_aggregator(
            aligned_scene, aligned_motion, text_tokens
        )

        # 5. è¯­è¨€æ¨¡å‹ç”Ÿæˆ
        output = self.language_model.generate(
            inputs=interaction_features,
            max_length=interaction_features.size(1) + 100
        )

        return output

class MotionVQVAE(nn.Module):
    """è¿åŠ¨å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨"""

    def __init__(self, vocab_size=1024, hidden_dim=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(135, hidden_dim, 3, padding=1),  # SMPLå…³èŠ‚
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, 3, padding=1),
        )
        self.codebook = nn.Embedding(vocab_size, hidden_dim)
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(hidden_dim, hidden_dim, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(hidden_dim, 135, 3, padding=1),
        )

    def tokenize(self, motion):
        """å°†è¿ç»­è¿åŠ¨è½¬æ¢ä¸ºç¦»æ•£token"""
        # motion: [B, T, 135]
        features = self.encoder(motion.transpose(1, 2))  # [B, hidden_dim, T]
        features = features.transpose(1, 2)  # [B, T, hidden_dim]

        # å‘é‡é‡åŒ–
        distances = torch.cdist(features, self.codebook.weight.unsqueeze(0))
        tokens = torch.argmin(distances, dim=-1)  # [B, T]

        return tokens

    def reconstruct(self, tokens):
        """ä»tokené‡å»ºè¿åŠ¨"""
        embeddings = self.codebook(tokens)  # [B, T, hidden_dim]
        embeddings = embeddings.transpose(1, 2)  # [B, hidden_dim, T]

        motion = self.decoder(embeddings)  # [B, 135, T]
        motion = motion.transpose(1, 2)  # [B, T, 135]

        return motion

class InteractionAggregator(nn.Module):
    """å¤šæ¨¡æ€äº¤äº’èšåˆå™¨"""

    def __init__(self, embed_dim=4096, num_heads=8):
        super().__init__()
        self.scene_proj = nn.Linear(768, embed_dim)
        self.motion_proj = nn.Linear(embed_dim, embed_dim)

        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            batch_first=True
        )

        self.output_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, scene_features, motion_features, text_features):
        # æŠ•å½±åˆ°å…±åŒç»´åº¦
        scene_emb = self.scene_proj(scene_features)      # [B, seq_len, embed_dim]
        motion_emb = self.motion_proj(motion_features)   # [B, seq_len, embed_dim]

        # é€šè¿‡æ³¨æ„åŠ›è¿›è¡Œè·¨æ¨¡æ€äº¤äº’
        attended_scene, _ = self.cross_attention(
            query=scene_emb,
            key=motion_emb,
            value=motion_emb
        )

        # ä¸æ–‡æœ¬ç‰¹å¾ç»“åˆ
        combined = attended_scene + text_features

        # æœ€ç»ˆæŠ•å½±
        output = self.output_proj(combined)

        return output

class SceneAdapter(nn.Module):
    """åœºæ™¯-è¯­è¨€é€‚é…å™¨"""

    def __init__(self, scene_dim=768, llm_dim=4096):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(scene_dim, llm_dim),
            nn.LayerNorm(llm_dim),
            nn.ReLU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, scene_features, text_tokens):
        adapted_scene = self.adapter(scene_features)
        return adapted_scene

class MotionAdapter(nn.Module):
    """è¿åŠ¨-è¯­è¨€é€‚é…å™¨"""

    def __init__(self, vocab_size=1024, llm_dim=4096):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, llm_dim)
        self.adapter = nn.Sequential(
            nn.LayerNorm(llm_dim),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, motion_tokens, text_tokens):
        motion_emb = self.embedding(motion_tokens)
        adapted_motion = self.adapter(motion_emb)
        return adapted_motion

# ä¸‰é˜¶æ®µè®­ç»ƒæµæ°´çº¿
def train_hsi_gpt():
    """HSI-GPTè®­ç»ƒæµæ°´çº¿"""

    model = HSI_GPT()

    # Stage 1: é¢„è®­ç»ƒtokenizer
    print("Stage 1: è®­ç»ƒè¿åŠ¨tokenizer...")
    motion_optimizer = torch.optim.Adam(model.motion_tokenizer.parameters())

    for epoch in range(100):
        for batch in motion_dataset:
            tokens = model.motion_tokenizer.tokenize(batch['motion'])
            reconstructed = model.motion_tokenizer.reconstruct(tokens)

            # VQ-VAEæŸå¤±
            recon_loss = F.mse_loss(reconstructed, batch['motion'])
            vq_loss = compute_vq_loss(tokens, model.motion_tokenizer.codebook)

            loss = recon_loss + 0.25 * vq_loss
            motion_optimizer.zero_grad()
            loss.backward()
            motion_optimizer.step()

    # Stage 2: å¤šæ¨¡æ€å¯¹é½
    print("Stage 2: å¯¹é½æ¨¡æ€...")
    full_optimizer = torch.optim.Adam([
        {'params': model.scene_encoder.parameters()},
        {'params': model.scene_adapter.parameters()},
        {'params': model.motion_adapter.parameters()},
        {'params': model.interaction_aggregator.parameters()},
        {'params': model.language_model.parameters(), 'lr': 1e-5}
    ])

    for epoch in range(50):
        for batch in multimodal_dataset:
            scene, motion, text = batch['scene'], batch['motion'], batch['text']

            output = model(scene, motion, text)
            target = batch['target']

            # è¯­è¨€å»ºæ¨¡æŸå¤±
            loss = F.cross_entropy(output.view(-1, output.size(-1)), target.view(-1))

            full_optimizer.zero_grad()
            loss.backward()
            full_optimizer.step()

    # Stage 3: æŒ‡ä»¤å¾®è°ƒ
    print("Stage 3: ä½¿ç”¨LoRAè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ...")
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=16, lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05
    )
    model = get_peft_model(model, lora_config)

    instruction_templates = {
        'generation': "ä¸ºåœºæ™¯ç”Ÿæˆè¿åŠ¨ '{}': {}",
        'completion': "åœ¨åœºæ™¯ä¸­è¡¥å…¨è¿åŠ¨ {}: {}",
        'captioning': "æè¿°åœºæ™¯ä¸­çš„è¿åŠ¨ {}: {}",
        'control': "ä½¿ç”¨ {} æ§åˆ¶ç”Ÿæˆè¿åŠ¨: {}"
    }

    lora_optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

    for epoch in range(10):
        for batch in instruction_dataset:
            task = batch['task']
            template = instruction_templates[task]

            instruction = template.format(batch['condition'], batch['context'])
            response = model.generate(instruction, scene=batch.get('scene'))

            loss = instruction_following_loss(response, batch['target'])
            lora_optimizer.zero_grad()
            loss.backward()
            lora_optimizer.step()

    return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = train_hsi_gpt()

    # æ–‡æœ¬æ¡ä»¶HSIç”Ÿæˆ
    scene = load_3d_scene("living_room.obj")
    instruction = "ä¸€ä¸ªäººèµ°åˆ°æ¤…å­æ—è¾¹åä¸‹æ¥"
    motion = model.generate_motion(scene, instruction)

    # å¤šæ¨¡æ€æ§åˆ¶ç”Ÿæˆ
    keyframe_poses = load_keyframes("sit_sequence.npy")
    affordance_map = compute_affordance_map(scene)
    motion = model.generate_with_controls(scene, instruction,
                                        keyframes=keyframe_poses,
                                        affordance=affordance_map)

    print(f"ç”Ÿæˆäº† {motion.shape[0]} å¸§çš„è¿åŠ¨åºåˆ—")</code></pre>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="#" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Code Not Available (æš‚æ— ä»£ç )
        </a>
      </div>
    </div>
</section>
</body>
</html>
