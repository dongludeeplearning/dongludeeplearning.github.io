<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ Computers and Education: Artificial Intelligence 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Predicting learners' engagement and help-seeking behaviors in an e-learning environment by using facial and head pose features</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Guan-Yun Wang*, Yasuhiro Hatori, Yoshiyuki Sato, Chia-Huei Tseng, Satoshi Shioiri<br>
        <span style="opacity:0.8">Tohoku University, Fu Jen Catholic University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we automatically predict learners' mental states of engagement and help-seeking in e-learning environments using only webcam facial videos and machine learning techniques?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>æˆ‘ä»¬å¦‚ä½•ä»…ä½¿ç”¨ç½‘ç»œæ‘„åƒå¤´é¢éƒ¨è§†é¢‘å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯è‡ªåŠ¨é¢„æµ‹åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸­å­¦ä¹ è€…çš„å‚ä¸åº¦å’Œæ±‚åŠ©å¿ƒç†çŠ¶æ€ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Comprehensive Feature Extraction:</b> Utilized OpenFace 2.2.0 to extract Action Units (AUs) and head pose parameters, creating three feature sets (Basic AUs, Head Pose, Co-occurring AUs) with statistical measures over 0.5s time windows for robust facial behavior analysis.</div>
            <div class="lang-zh" style="display:none"><b>å…¨é¢ç‰¹å¾æå–ï¼š</b>ä½¿ç”¨OpenFace 2.2.0æå–åŠ¨ä½œå•å…ƒ(AUs)å’Œå¤´éƒ¨å§¿åŠ¿å‚æ•°ï¼Œåœ¨0.5ç§’æ—¶é—´çª—å£ä¸Šåˆ›å»ºä¸‰ä¸ªç‰¹å¾é›†ï¼ˆåŸºæœ¬AUsã€å¤´éƒ¨å§¿åŠ¿ã€å…±ç°AUsï¼‰ï¼Œå…·æœ‰ç»Ÿè®¡åº¦é‡ï¼Œç”¨äºå¼ºå¤§çš„é¢éƒ¨è¡Œä¸ºåˆ†æã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Machine Learning Pipeline:</b> Implemented LightGBM and SVM classifiers achieving 0.69-0.93 accuracy in predicting engagement and help-seeking states, with LightGBM outperforming SVM using 5-fold cross-validation on 9 participants' data.</div>
            <div class="lang-zh" style="display:none"><b>æœºå™¨å­¦ä¹ ç®¡é“ï¼š</b>å®ç°LightGBMå’ŒSVMåˆ†ç±»å™¨ï¼Œåœ¨é¢„æµ‹å‚ä¸åº¦å’Œæ±‚åŠ©çŠ¶æ€æ—¶è¾¾åˆ°0.69-0.93çš„å‡†ç¡®ç‡ï¼Œä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯åœ¨9åå‚ä¸è€…çš„æ•°æ®ä¸ŠLightGBMä¼˜äºSVMã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Feature Importance Analysis:</b> Applied SHAP analysis to identify key facial features - AU02 (outer brow raiser), AU23 (lip tightener), and AU04 (brow lowerer) for engagement prediction; AU04, AU23, and AU14 (dimpler) for help-seeking prediction.</div>
            <div class="lang-zh" style="display:none"><b>ç‰¹å¾é‡è¦æ€§åˆ†æï¼š</b>åº”ç”¨SHAPåˆ†ææ¥è¯†åˆ«å…³é”®é¢éƒ¨ç‰¹å¾ - AU02ï¼ˆå¤–çœ‰ä¸Šæ‰¬ï¼‰ã€AU23ï¼ˆå”‡ç´§ç¼©ï¼‰å’ŒAU04ï¼ˆçœ‰ä¸‹å‚ï¼‰ç”¨äºå‚ä¸åº¦é¢„æµ‹ï¼›AU04ã€AU23å’ŒAU14ï¼ˆé…’çªï¼‰ç”¨äºæ±‚åŠ©é¢„æµ‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>ITS Integration Framework:</b> Developed a web-based intelligent tutoring system prototype that can detect mental states and provide timely hints without verbal requests, using only webcam input for non-intrusive monitoring.</div>
            <div class="lang-zh" style="display:none"><b>ITSé›†æˆæ¡†æ¶ï¼š</b>å¼€å‘äº†ä¸€ä¸ªåŸºäºç½‘ç»œçš„æ™ºèƒ½è¾…å¯¼ç³»ç»ŸåŸå‹ï¼Œå¯ä»¥æ£€æµ‹å¿ƒç†çŠ¶æ€å¹¶åœ¨æ²¡æœ‰å£å¤´è¯·æ±‚çš„æƒ…å†µä¸‹æä¾›åŠæ—¶æç¤ºï¼Œä»…ä½¿ç”¨ç½‘ç»œæ‘„åƒå¤´è¾“å…¥è¿›è¡Œéä¾µå…¥å¼ç›‘æ§ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Individual Variability:</b> Significant individual differences in facial expressions and head movements across participants, requiring personalized model adaptation rather than pooled training data.</div>
            <div class="lang-zh" style="display:none"><b>ä¸ªä½“å·®å¼‚ï¼š</b>å‚ä¸è€…ä¹‹é—´é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨è¿åŠ¨çš„æ˜¾è‘—ä¸ªä½“å·®å¼‚ï¼Œéœ€è¦ä¸ªæ€§åŒ–æ¨¡å‹é€‚åº”è€Œä¸æ˜¯æ± åŒ–è®­ç»ƒæ•°æ®ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Annotation Reliability:</b> Low inter-rater agreement (Kendall's W = 0.04) among human annotators for engagement labeling, necessitating careful annotation protocols and rater selection.</div>
            <div class="lang-zh" style="display:none"><b>æ³¨é‡Šå¯é æ€§ï¼š</b>äººç±»æ³¨é‡Šè€…å¯¹å‚ä¸åº¦æ ‡æ³¨çš„ä½ä¸€è‡´æ€§ï¼ˆKendall's W = 0.04ï¼‰ï¼Œéœ€è¦ä»”ç»†çš„æ³¨é‡Šåè®®å’Œè¯„åˆ†è€…é€‰æ‹©ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Data Imbalance:</b> Limited dataset size (9 participants) and imbalanced samples between engagement states (65.6% high, 34.4% low), affecting model generalization.</div>
            <div class="lang-zh" style="display:none"><b>æ•°æ®ä¸å¹³è¡¡ï¼š</b>æœ‰é™çš„æ•°æ®é›†å¤§å°ï¼ˆ9åå‚ä¸è€…ï¼‰å’Œå‚ä¸åº¦çŠ¶æ€ä¹‹é—´çš„æ ·æœ¬ä¸å¹³è¡¡ï¼ˆ65.6%é«˜ï¼Œ34.4%ä½ï¼‰ï¼Œå½±å“æ¨¡å‹æ³›åŒ–ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Real-time Processing:</b> Need for lightweight, fast models suitable for real-time ITS applications, balancing accuracy with computational efficiency for webcam-based monitoring.</div>
            <div class="lang-zh" style="display:none"><b>å®æ—¶å¤„ç†ï¼š</b>éœ€è¦è½»é‡çº§ã€å¿«é€Ÿçš„æ¨¡å‹é€‚ç”¨äºå®æ—¶ITSåº”ç”¨ï¼Œåœ¨ç½‘ç»œæ‘„åƒå¤´ç›‘æ§ä¸­å¹³è¡¡å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The study employed a comprehensive methodology combining computer vision and machine learning techniques:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Data Collection:</b> Recruited 9 university students to solve linguistic problems on a custom ITS webpage with hint functionality, recording facial videos, clicks, and answers.</li>
                <li style="margin-bottom:6px;"><b>Facial Feature Extraction:</b> Used OpenFace 2.2.0 to extract 18 AUs and head pose parameters (pitch, yaw, roll, x/y/z coordinates) from each video frame.</li>
                <li style="margin-bottom:6px;"><b>Feature Engineering:</b> Created three feature sets with statistical measures (mean, median, std, min, max, range) over 0.5s time windows, resulting in 210 AU features and 36 head pose features.</li>
                <li style="margin-bottom:6px;"><b>Classification Models:</b> Trained LightGBM and SVM models using 5-fold cross-validation for predicting engagement and help-seeking states separately.</li>
                <li style="margin-bottom:6px;"><b>Interpretability:</b> Applied SHAP (Shapley Additive exPlanations) analysis to evaluate the importance of Basic AUs and Head Pose features in model predictions.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>æœ¬ç ”ç©¶ç»“åˆè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯é‡‡ç”¨äº†å…¨é¢çš„æ–¹æ³•ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ•°æ®æ”¶é›†ï¼š</b>æ‹›å‹Ÿ9åå¤§å­¦ç”Ÿåœ¨è‡ªå®šä¹‰çš„ITSç½‘é¡µä¸Šè§£å†³è¯­è¨€é—®é¢˜ï¼Œç½‘é¡µå…·æœ‰æç¤ºåŠŸèƒ½ï¼Œè®°å½•é¢éƒ¨è§†é¢‘ã€ç‚¹å‡»å’Œç­”æ¡ˆã€‚</li>
                <li style="margin-bottom:6px;"><b>é¢éƒ¨ç‰¹å¾æå–ï¼š</b>ä½¿ç”¨OpenFace 2.2.0ä»æ¯ä¸ªè§†é¢‘å¸§æå–18ä¸ªAUå’Œå¤´éƒ¨å§¿åŠ¿å‚æ•°ï¼ˆä¿¯ä»°è§’ã€åèˆªè§’ã€æ»šè½¬è§’ã€x/y/zåæ ‡ï¼‰ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç‰¹å¾å·¥ç¨‹ï¼š</b>åœ¨0.5ç§’æ—¶é—´çª—å£ä¸Šåˆ›å»ºä¸‰ä¸ªç‰¹å¾é›†ï¼Œå…·æœ‰ç»Ÿè®¡åº¦é‡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€èŒƒå›´ï¼‰ï¼Œäº§ç”Ÿ210ä¸ªAUç‰¹å¾å’Œ36ä¸ªå¤´éƒ¨å§¿åŠ¿ç‰¹å¾ã€‚</li>
                <li style="margin-bottom:6px;"><b>åˆ†ç±»æ¨¡å‹ï¼š</b>ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒLightGBMå’ŒSVMæ¨¡å‹ï¼Œåˆ†åˆ«é¢„æµ‹å‚ä¸åº¦å’Œæ±‚åŠ©çŠ¶æ€ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯è§£é‡Šæ€§ï¼š</b>åº”ç”¨SHAPï¼ˆShapley Additive exPlanationsï¼‰åˆ†ææ¥è¯„ä¼°åŸºæœ¬AUså’Œå¤´éƒ¨å§¿åŠ¿ç‰¹å¾åœ¨æ¨¡å‹é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/engagement_prediction_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('1-s2.0-S2666920X2500027X-main.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('1-s2.0-S2666920X2500027X-main.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/engagement_prediction_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('1-s2.0-S2666920X2500027X-main.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('1-s2.0-S2666920X2500027X-main.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of this approach lies in several key factors:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Facial Expression Reliability:</b> Action Units provide standardized, anatomically-based measurements of facial muscle activity that correlate with cognitive and emotional states during learning.</li>
                <li style="margin-bottom:6px;"><b>Head Pose Complementarity:</b> Head movements add contextual information about attention and concentration levels, with pitch movements indicating engagement and yaw movements signaling help-seeking.</li>
                <li style="margin-bottom:6px;"><b>Machine Learning Effectiveness:</b> LightGBM's ability to handle complex feature interactions combined with SHAP's interpretability ensures both high accuracy and model transparency.</li>
                <li style="margin-bottom:6px;"><b>Non-intrusive Monitoring:</b> Webcam-based approach requires no specialized equipment, enabling practical deployment in existing e-learning platforms.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™ç§æ–¹æ³•æˆåŠŸçš„å…³é”®åœ¨äºå‡ ä¸ªå› ç´ ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>é¢éƒ¨è¡¨æƒ…å¯é æ€§ï¼š</b>åŠ¨ä½œå•å…ƒæä¾›åŸºäºè§£å‰–å­¦çš„æ ‡å‡†åŒ–é¢éƒ¨è‚Œè‚‰æ´»åŠ¨æµ‹é‡ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¸è®¤çŸ¥å’Œæƒ…ç»ªçŠ¶æ€ç›¸å…³ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤´éƒ¨å§¿åŠ¿äº’è¡¥æ€§ï¼š</b>å¤´éƒ¨è¿åŠ¨å¢åŠ äº†æ³¨æ„åŠ›æ°´å¹³å’Œä¸“æ³¨åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¿¯ä»°è¿åŠ¨è¡¨ç¤ºå‚ä¸åº¦ï¼Œåèˆªè¿åŠ¨è¡¨ç¤ºæ±‚åŠ©ã€‚</li>
                <li style="margin-bottom:6px;"><b>æœºå™¨å­¦ä¹ æœ‰æ•ˆæ€§ï¼š</b>LightGBMå¤„ç†å¤æ‚ç‰¹å¾äº¤äº’çš„èƒ½åŠ›ç»“åˆSHAPçš„å¯è§£é‡Šæ€§ç¡®ä¿äº†é«˜å‡†ç¡®ç‡å’Œæ¨¡å‹é€æ˜åº¦ã€‚</li>
                <li style="margin-bottom:6px;"><b>éä¾µå…¥å¼ç›‘æ§ï¼š</b>åŸºäºç½‘ç»œæ‘„åƒå¤´çš„æ–¹æ¡ˆä¸éœ€è¦ä¸“é—¨è®¾å¤‡ï¼Œèƒ½å¤Ÿåœ¨ç°æœ‰åœ¨çº¿å­¦ä¹ å¹³å°ä¸­å®é™…éƒ¨ç½²ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This research demonstrates the feasibility of using facial expressions and head poses to automatically predict learners' engagement and help-seeking behaviors in e-learning environments. The study achieved high accuracy (0.69-0.93) using LightGBM classifiers with OpenFace-extracted features. Key findings include the importance of eye-related AUs (AU02, AU04) for engagement detection and mouth-related AUs (AU14, AU23) for help-seeking prediction. The approach provides a foundation for developing more responsive and personalized intelligent tutoring systems that can intervene before students become frustrated or disengaged, ultimately improving learning outcomes in online education.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹ç ”ç©¶è¯æ˜äº†ä½¿ç”¨é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿æ¥è‡ªåŠ¨é¢„æµ‹å­¦ä¹ è€…åœ¨åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸­çš„å‚ä¸åº¦å’Œæ±‚åŠ©è¡Œä¸ºçš„å¯èƒ½æ€§ã€‚ç ”ç©¶ä½¿ç”¨LightGBMåˆ†ç±»å™¨å’ŒOpenFaceæå–çš„ç‰¹å¾å®ç°äº†é«˜å‡†ç¡®ç‡ï¼ˆ0.69-0.93ï¼‰ã€‚å…³é”®å‘ç°åŒ…æ‹¬çœ¼ç›ç›¸å…³AUï¼ˆAU02ã€AU04ï¼‰å¯¹å‚ä¸åº¦æ£€æµ‹çš„é‡è¦æ€§ä»¥åŠå˜´å·´ç›¸å…³AUï¼ˆAU14ã€AU23ï¼‰å¯¹æ±‚åŠ©é¢„æµ‹çš„é‡è¦æ€§ã€‚è¯¥æ–¹æ³•ä¸ºå¼€å‘æ›´å“åº”æ€§å’Œä¸ªæ€§åŒ–çš„æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥åœ¨å­¦ç”Ÿæ„Ÿåˆ°æ²®ä¸§æˆ–è„±ç¦»æ¥è§¦ä¹‹å‰è¿›è¡Œå¹²é¢„ï¼Œæœ€ç»ˆæ”¹å–„åœ¨çº¿æ•™è‚²çš„å­¦ä¹ æˆæœã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
          <p><b>DOI:</b> <a href="https://doi.org/10.1016/j.caeai.2025.100387" style="color:#8bffcf;">10.1016/j.caeai.2025.100387</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import openface<br>
import lightgbm as lgb<br>
import numpy as np<br>
from sklearn.model_selection import cross_val_score<br>
import shap<br>
<br>
// Feature extraction using OpenFace<br>
def extract_facial_features(video_path):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Extract AUs and head pose features from video"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;face_analyzer = openface.OpenFace()<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Process video frame by frame<br>
&nbsp;&nbsp;&nbsp;&nbsp;features = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;for frame in video_frames:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract Action Units (0-17 AUs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aus = face_analyzer.get_action_units(frame)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract head pose (pitch, yaw, roll, x, y, z)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;head_pose = face_analyzer.get_head_pose(frame)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Combine features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;frame_features = aus + head_pose<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features.append(frame_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return np.array(features)<br>
<br>
// LightGBM classification for engagement prediction<br>
def train_engagement_model(features, labels):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Train LightGBM model for engagement classification"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Prepare dataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;X = features  # Shape: (n_samples, n_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;y = labels    # Binary: 0=low engagement, 1=high engagement<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# LightGBM parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;params = {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'objective': 'binary',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'metric': 'binary_logloss',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'boosting_type': 'gbdt',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'num_leaves': 31,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'learning_rate': 0.05,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'feature_fraction': 0.9<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# 5-fold cross-validation<br>
&nbsp;&nbsp;&nbsp;&nbsp;cv_scores = cross_val_score(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lgb.LGBMClassifier(**params),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X, y,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv=5,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scoring='accuracy'<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return cv_scores.mean()<br>
<br>
// SHAP analysis for feature importance<br>
def explain_model_predictions(model, X_test):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Use SHAP to explain model predictions"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Create SHAP explainer<br>
&nbsp;&nbsp;&nbsp;&nbsp;explainer = shap.TreeExplainer(model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Calculate SHAP values<br>
&nbsp;&nbsp;&nbsp;&nbsp;shap_values = explainer.shap_values(X_test)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return shap_values
        </div>
      </div>
    </div>
</section>
</body>
</html>