<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ACM MM '24
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Towards End-to-End Explainable Facial Action Unit Recognition via Vision-Language Joint Learning</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Xuri Ge*, Junchen Fu*, Fuhai Chenâ€ , Shan An, Nicu Sebe, Joemon M. Jose<br>
        <span style="opacity:0.8">University of Glasgow, Fuzhou University, Tianjin University, University of Trento</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we achieve end-to-end explainable facial action unit recognition by integrating language generation capabilities that provide both accurate AU predictions and interpretable natural language descriptions of facial muscle states?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>æˆ‘ä»¬å¦‚ä½•é€šè¿‡æ•´åˆè¯­è¨€ç”Ÿæˆèƒ½åŠ›æ¥å®ç°ç«¯åˆ°ç«¯çš„å¯è§£é‡Šé¢éƒ¨åŠ¨ä½œå•å…ƒè¯†åˆ«ï¼Œæ—¢æä¾›å‡†ç¡®çš„AUé¢„æµ‹ï¼Œåˆæä¾›é¢éƒ¨è‚Œè‚‰çŠ¶æ€çš„å¯è§£é‡Šè‡ªç„¶è¯­è¨€æè¿°ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>End-to-End Vision-Language Joint Learning:</b> Pioneered VL-FAU, a novel framework that integrates language generation supervisors within facial AU recognition network for simultaneous AU prediction and explainable language descriptions.</div>
            <div class="lang-zh" style="display:none"><b>ç«¯åˆ°ç«¯è§†è§‰-è¯­è¨€è”åˆå­¦ä¹ ï¼š</b>å¼€åˆ›äº†VL-FAUï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå°†è¯­è¨€ç”Ÿæˆç›‘ç£å™¨æ•´åˆåˆ°é¢éƒ¨AUè¯†åˆ«ç½‘ç»œä¸­ï¼Œå®ç°AUé¢„æµ‹å’Œå¯è§£é‡Šè¯­è¨€æè¿°çš„åŒæ—¶è¿›è¡Œã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Dual-Level AU Representation Learning:</b> Introduced dual-level attention refinement (DAIR) based on multi-scale combined facial features, providing attention-aware AU representation capabilities through channel and spatial attention mining.</div>
            <div class="lang-zh" style="display:none"><b>åŒå±‚AUè¡¨ç¤ºå­¦ä¹ ï¼š</b>å¼•å…¥äº†åŸºäºå¤šå°ºåº¦ç»„åˆé¢éƒ¨ç‰¹å¾çš„åŒå±‚æ³¨æ„åŠ›ç²¾ç‚¼ï¼ˆDAIRï¼‰ï¼Œé€šè¿‡é€šé“å’Œç©ºé—´æ³¨æ„åŠ›æŒ–æ˜æä¾›æ³¨æ„åŠ›æ„ŸçŸ¥çš„AUè¡¨ç¤ºèƒ½åŠ›ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Local and Global Language Generation:</b> Designed local AU language generation for fine-grained semantic supervision of individual AU branches and global facial language generation for whole-face semantic supervision, enhancing inter-AU discriminability and intra-subject consistency.</div>
            <div class="lang-zh" style="display:none"><b>å±€éƒ¨å’Œå…¨å±€è¯­è¨€ç”Ÿæˆï¼š</b>è®¾è®¡äº†å±€éƒ¨AUè¯­è¨€ç”Ÿæˆç”¨äºå•ä¸ªAUåˆ†æ”¯çš„ç»†ç²’åº¦è¯­ä¹‰ç›‘ç£ï¼Œä»¥åŠå…¨å±€é¢éƒ¨è¯­è¨€ç”Ÿæˆç”¨äºæ•´ä¸ªé¢éƒ¨çš„è¯­ä¹‰ç›‘ç£ï¼Œå¢å¼ºäº†AUé—´å¯åŒºåˆ†æ€§å’Œä¸»é¢˜å†…ä¸€è‡´æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>State-of-the-Art Performance:</b> Achieved superior performance on DISFA and BP4D benchmarks, outperforming existing methods with interpretable language descriptions that provide linguistic semantics beyond classification results.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›æ€§èƒ½ï¼š</b>åœ¨DISFAå’ŒBP4DåŸºå‡†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›è¶…è¶Šåˆ†ç±»ç»“æœçš„è¯­è¨€è¯­ä¹‰çš„å¯è§£é‡Šè¯­è¨€æè¿°ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Limited AU Representation Discriminability:</b> Traditional FAU recognition methods struggle to maintain discriminative features among different AUs while preserving rich semantic representations within individual AUs.</div>
            <div class="lang-zh" style="display:none"><b>æœ‰é™çš„AUè¡¨ç¤ºå¯åŒºåˆ†æ€§ï¼š</b>ä¼ ç»Ÿçš„FAUè¯†åˆ«æ–¹æ³•éš¾ä»¥åœ¨ä¿æŒä¸åŒAUä¹‹é—´çš„åˆ¤åˆ«ç‰¹å¾çš„åŒæ—¶ä¿ç•™å•ä¸ªAUå†…çš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Lack of Explainability:</b> Most FAU recognition approaches only provide classification results without explanations for AU decisions, making it difficult to understand model reasoning and trust predictions.</div>
            <div class="lang-zh" style="display:none"><b>ç¼ºä¹å¯è§£é‡Šæ€§ï¼š</b>å¤§å¤šæ•°FAUè¯†åˆ«æ–¹æ³•åªæä¾›åˆ†ç±»ç»“æœï¼Œè€Œä¸æä¾›AUå†³ç­–çš„è§£é‡Šï¼Œä½¿å¾—ç†è§£æ¨¡å‹æ¨ç†å’Œä¿¡ä»»é¢„æµ‹å˜å¾—å›°éš¾ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Intra- and Inter-Subject Variability:</b> Facial expressions vary significantly between subjects and even within the same subject under different emotional states, requiring robust representation learning across diverse facial appearances.</div>
            <div class="lang-zh" style="display:none"><b>ä¸»é¢˜å†…å’Œä¸»é¢˜é—´å˜å¼‚æ€§ï¼š</b>é¢éƒ¨è¡¨æƒ…åœ¨ä¸åŒä¸»é¢˜ä¹‹é—´ä»¥åŠåŒä¸€ä¸»é¢˜åœ¨ä¸åŒæƒ…ç»ªçŠ¶æ€ä¸‹å˜åŒ–æ˜¾è‘—ï¼Œéœ€è¦è·¨ä¸åŒé¢éƒ¨å¤–è§‚çš„é²æ£’è¡¨ç¤ºå­¦ä¹ ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Computational Efficiency:</b> Integrating language generation capabilities into FAU recognition while maintaining real-time performance and reasonable computational resource requirements.</div>
            <div class="lang-zh" style="display:none"><b>è®¡ç®—æ•ˆç‡ï¼š</b>åœ¨å°†è¯­è¨€ç”Ÿæˆèƒ½åŠ›æ•´åˆåˆ°FAUè¯†åˆ«ä¸­çš„åŒæ—¶ä¿æŒå®æ—¶æ€§èƒ½å’Œåˆç†çš„è®¡ç®—èµ„æºéœ€æ±‚ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>VL-FAU employs a sophisticated vision-language joint learning framework:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Multi-Scale Facial Feature Extraction:</b> Utilizes Swin-Transformer to extract hierarchical facial representations from different stages, combining them via learnable linear layers for comprehensive face understanding.</li>
                <li style="margin-bottom:6px;"><b>Dual-Level AU Refinement (DAIR):</b> Implements channel-level and spatial-level attention mechanisms within independent AU branches to enhance attention-aware AU representation mining.</li>
                <li style="margin-bottom:6px;"><b>Local AU Language Generation:</b> Each AU branch includes an LSTM-based language decoder that generates fine-grained muscle descriptions, providing semantic supervision for individual AU classification.</li>
                <li style="margin-bottom:6px;"><b>Global Facial Language Generation:</b> Leverages multi-scale facial features to generate descriptions of activated AUs across the entire face, ensuring discriminability between different facial states.</li>
                <li style="margin-bottom:6px;"><b>Joint Optimization:</b> Trains vision AU recognition and language generation simultaneously with multi-label classification loss, local language generation loss, and global language generation loss.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>VL-FAUé‡‡ç”¨äº†ä¸€ç§å¤æ‚çš„è§†è§‰-è¯­è¨€è”åˆå­¦ä¹ æ¡†æ¶ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>å¤šå°ºåº¦é¢éƒ¨ç‰¹å¾æå–ï¼š</b>åˆ©ç”¨Swin-Transformerä»ä¸åŒé˜¶æ®µæå–å±‚æ¬¡é¢éƒ¨è¡¨ç¤ºï¼Œé€šè¿‡å¯å­¦ä¹ çº¿æ€§å±‚ç»„åˆå®ƒä»¬ä»¥å®ç°å…¨é¢çš„é¢éƒ¨ç†è§£ã€‚</li>
                <li style="margin-bottom:6px;"><b>åŒå±‚AUç²¾ç‚¼ï¼ˆDAIRï¼‰ï¼š</b>åœ¨ç‹¬ç«‹AUåˆ†æ”¯å†…å®ç°é€šé“çº§å’Œç©ºé—´çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºæ³¨æ„åŠ›æ„ŸçŸ¥çš„AUè¡¨ç¤ºæŒ–æ˜ã€‚</li>
                <li style="margin-bottom:6px;"><b>å±€éƒ¨AUè¯­è¨€ç”Ÿæˆï¼š</b>æ¯ä¸ªAUåˆ†æ”¯åŒ…æ‹¬ä¸€ä¸ªåŸºäºLSTMçš„è¯­è¨€è§£ç å™¨ï¼Œç”Ÿæˆç»†ç²’åº¦çš„è‚Œè‚‰æè¿°ï¼Œä¸ºå•ä¸ªAUåˆ†ç±»æä¾›è¯­ä¹‰ç›‘ç£ã€‚</li>
                <li style="margin-bottom:6px;"><b>å…¨å±€é¢éƒ¨è¯­è¨€ç”Ÿæˆï¼š</b>åˆ©ç”¨å¤šå°ºåº¦é¢éƒ¨ç‰¹å¾ç”Ÿæˆæ•´ä¸ªé¢éƒ¨æ¿€æ´»AUçš„æè¿°ï¼Œç¡®ä¿ä¸åŒé¢éƒ¨çŠ¶æ€ä¹‹é—´çš„å¯åŒºåˆ†æ€§ã€‚</li>
                <li style="margin-bottom:6px;"><b>è”åˆä¼˜åŒ–ï¼š</b>ä½¿ç”¨å¤šæ ‡ç­¾åˆ†ç±»æŸå¤±ã€å±€éƒ¨è¯­è¨€ç”ŸæˆæŸå¤±å’Œå…¨å±€è¯­è¨€ç”ŸæˆæŸå¤±åŒæ—¶è®­ç»ƒè§†è§‰AUè¯†åˆ«å’Œè¯­è¨€ç”Ÿæˆã€‚</li>
            </ol>
          </div>
        </div>
      </div>
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/explainable_fau_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('3664647.3681443.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('3664647.3681443.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/explainable_fau_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('3664647.3681443.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('3664647.3681443.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of VL-FAU stems from its innovative multimodal integration:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Semantic Enhancement through Language:</b> Language generation provides explicit semantic supervision that enriches AU representations beyond visual features alone, enabling better discrimination between similar facial expressions.</li>
                <li style="margin-bottom:6px;"><b>Multi-Level Attention Mining:</b> The dual-level refinement captures both channel-wise and spatial-wise attention patterns, ensuring comprehensive AU feature extraction from multi-scale facial representations.</li>
                <li style="margin-bottom:6px;"><b>Joint Learning Synergy:</b> Vision and language tasks reinforce each other during training, where AU classification benefits from language semantics and language generation improves through visual grounding.</li>
                <li style="margin-bottom:6px;"><b>Interpretable by Design:</b> Unlike black-box classifiers, VL-FAU provides natural language explanations that align with FACS definitions, making AU predictions trustworthy and clinically useful.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>VL-FAUçš„æˆåŠŸæºäºå…¶åˆ›æ–°çš„å¤šæ¨¡æ€æ•´åˆï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>é€šè¿‡è¯­è¨€çš„è¯­ä¹‰å¢å¼ºï¼š</b>è¯­è¨€ç”Ÿæˆæä¾›äº†è¶…è¶Šå•ç‹¬è§†è§‰ç‰¹å¾çš„æ˜ç¡®è¯­ä¹‰ç›‘ç£ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç›¸ä¼¼çš„é¢éƒ¨è¡¨æƒ…ä¹‹é—´å®ç°æ›´å¥½çš„åŒºåˆ†ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šå±‚æ³¨æ„åŠ›æŒ–æ˜ï¼š</b>åŒå±‚ç²¾ç‚¼æ•è·äº†é€šé“çº§å’Œç©ºé—´çº§çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œç¡®ä¿ä»å¤šå°ºåº¦é¢éƒ¨è¡¨ç¤ºä¸­è¿›è¡Œå…¨é¢çš„AUç‰¹å¾æå–ã€‚</li>
                <li style="margin-bottom:6px;"><b>è”åˆå­¦ä¹ ååŒä½œç”¨ï¼š</b>è§†è§‰å’Œè¯­è¨€ä»»åŠ¡åœ¨è®­ç»ƒæœŸé—´ç›¸äº’å¼ºåŒ–ï¼Œå…¶ä¸­AUåˆ†ç±»å—ç›Šäºè¯­è¨€è¯­ä¹‰ï¼Œè¯­è¨€ç”Ÿæˆé€šè¿‡è§†è§‰ grounding å¾—åˆ°æ”¹å–„ã€‚</li>
                <li style="margin-bottom:6px;"><b>è®¾è®¡ä¸Šå¯è§£é‡Šï¼š</b>ä¸é»‘ç›’åˆ†ç±»å™¨ä¸åŒï¼ŒVL-FAUæä¾›äº†ä¸FACSå®šä¹‰ä¸€è‡´çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä½¿AUé¢„æµ‹å€¼å¾—ä¿¡èµ–ä¸”åœ¨ä¸´åºŠä¸Šæœ‰ç”¨ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>VL-FAU represents a significant advancement in explainable facial action unit recognition by successfully integrating vision and language modalities. The framework achieves state-of-the-art performance on DISFA and BP4D datasets while providing interpretable language descriptions for both individual AU predictions and global facial states. By leveraging dual-level AU representation learning and local-global language generation supervision, VL-FAU addresses the critical challenges of AU discriminability and explainability. The approach opens new avenues for multimodal affective computing applications where both accuracy and interpretability are essential, such as in clinical diagnosis, human-computer interaction, and emotion recognition systems.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>VL-FAUé€šè¿‡æˆåŠŸæ•´åˆè§†è§‰å’Œè¯­è¨€æ¨¡æ€ï¼Œåœ¨å¯è§£é‡Šé¢éƒ¨åŠ¨ä½œå•å…ƒè¯†åˆ«æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚è¯¥æ¡†æ¶åœ¨DISFAå’ŒBP4Dæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ºå•ä¸ªAUé¢„æµ‹å’Œå…¨å±€é¢éƒ¨çŠ¶æ€æä¾›å¯è§£é‡Šçš„è¯­è¨€æè¿°ã€‚é€šè¿‡åˆ©ç”¨åŒå±‚AUè¡¨ç¤ºå­¦ä¹ å’Œå±€éƒ¨-å…¨å±€è¯­è¨€ç”Ÿæˆç›‘ç£ï¼ŒVL-FAUè§£å†³äº†AUå¯åŒºåˆ†æ€§å’Œå¯è§£é‡Šæ€§çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿè®¡ç®—åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œåœ¨è¿™äº›åº”ç”¨ä¸­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§éƒ½æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä¾‹å¦‚ä¸´åºŠè¯Šæ–­ã€äººæœºäº¤äº’å’Œæƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
          <p><b>DOI:</b> <a href="https://doi.org/10.1145/3664647.3681443" style="color:#8bffcf;">10.1145/3664647.3681443</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import torch<br>
import torch.nn as nn<br>
from transformers import SwinModel<br>
import torch.nn.functional as F<br>
<br>
// VL-FAU: Vision-Language Joint Learning for FAU Recognition<br>
class VLFAU(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, num_aus=12):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(VLFAU, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Multi-scale Swin-Transformer backbone<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.backbone = SwinModel.from_pretrained('microsoft/swin-tiny-patch4-window7-224')<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Multi-scale combination<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.msc_layers = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(768, 512, 3, padding=1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(768, 512, 3, padding=1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(768, 512, 3, padding=1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(768, 512, 3, padding=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.msc_linear = nn.Linear(512*4, 512)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# AU branches with DAIR<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.au_branches = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DualLevelAURefinement() for _ in range(num_aus)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Language decoders<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.local_decoders = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AULanguageDecoder() for _ in range(num_aus)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.global_decoder = GlobalLanguageDecoder()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Multi-label classifier<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.classifier = nn.Linear(512, num_aus)<br>
<br>
// Dual-level AU Individual Refinement (DAIR)<br>
class DualLevelAURefinement(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(DualLevelAURefinement, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Channel attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.channel_mlps = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Sequential(nn.Linear(512, 512//16), nn.ReLU(), nn.Linear(512//16, 512))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for _ in range(2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Spatial attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.spatial_conv = nn.Conv2d(2, 1, 3, padding=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Channel attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_c = F.adaptive_max_pool2d(x, 1).squeeze(-1).squeeze(-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avg_c = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;channel_attn = torch.sigmoid(self.channel_mlps[0](max_c) + self.channel_mlps[1](avg_c))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = x * channel_attn.unsqueeze(-1).unsqueeze(-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Spatial attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_s = torch.max(x, dim=1, keepdim=True)[0]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avg_s = torch.mean(x, dim=1, keepdim=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spatial_attn = torch.sigmoid(self.spatial_conv(torch.cat([max_s, avg_s], dim=1)))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = x * spatial_attn<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return x<br>
<br>
// AU Language Decoder<br>
class AULanguageDecoder(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, vocab_size=1000, hidden_size=512):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(AULanguageDecoder, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.attention = nn.Linear(hidden_size * 2, 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.output = nn.Linear(hidden_size, vocab_size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, visual_features, captions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# LSTM decoding with attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs, _ = self.lstm(captions)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Attention mechanism<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attn_weights = F.softmax(self.attention(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.cat([outputs.unsqueeze(1).expand(-1, visual_features.size(1), -1, -1), <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;visual_features.unsqueeze(2).expand(-1, -1, outputs.size(1), -1)], dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).squeeze(-1), dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context = torch.sum(attn_weights.unsqueeze(-1) * visual_features, dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;predictions = self.output(outputs + context)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return predictions<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
