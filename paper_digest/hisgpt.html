<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ICCV 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Jiahe Zhao, Ruibing Hou, Zejie Tian, Hong Chang, Shiguang Shan<br>
        <span style="opacity:0.8">Key Laboratory of AI Safety of CAS, Institute of Computing Technology, CAS & University of CAS</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to enable embodied agents to understand and answer questions about human behaviors and states within complex 3D scenes?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•è®©å…·èº«æ™ºèƒ½ä½“èƒ½å¤Ÿç†è§£å¹¶å›ç­”å…³äºäººç±»åœ¨å¤æ‚3Dåœºæ™¯ä¸­çš„è¡Œä¸ºå’ŒçŠ¶æ€çš„é—®é¢˜ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>HIS-QA Task:</b> Introduces Human-In-Scene Question Answering (HIS-QA), a novel task requiring agents to comprehend human states, behaviors, and interactions within 3D scenes.</div>
            <div class="lang-zh" style="display:none"><b>HIS-QAä»»åŠ¡ï¼š</b>å¼•å…¥äººç±»-åœºæ™¯é—®ç­”(HIS-QA)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“ç†è§£äººç±»åœ¨3Dåœºæ™¯ä¸­çš„çŠ¶æ€ã€è¡Œä¸ºå’Œäº¤äº’ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>HIS-Bench Benchmark:</b> Creates HIS-Bench, the first multimodal benchmark with 800 questions spanning basic perception to advanced reasoning, prediction, and planning tasks.</div>
            <div class="lang-zh" style="display:none"><b>HIS-BenchåŸºå‡†ï¼š</b>åˆ›å»ºHIS-Benchï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«800ä¸ªé—®é¢˜çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œæ¶µç›–ä»åŸºæœ¬æ„ŸçŸ¥åˆ°é«˜çº§æ¨ç†ã€é¢„æµ‹å’Œè§„åˆ’çš„ä»»åŠ¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>HIS-GPT Model:</b> Develops HIS-GPT, the first foundation model for HIS understanding, integrating 3D scene context and human motion dynamics into LLMs.</div>
            <div class="lang-zh" style="display:none"><b>HIS-GPTæ¨¡å‹ï¼š</b>å¼€å‘HIS-GPTï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹HISç†è§£çš„åŸºç¡€æ¨¡å‹ï¼Œå°†3Dåœºæ™¯ä¸Šä¸‹æ–‡å’Œäººç±»è¿åŠ¨åŠ¨æ€é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Specialized Mechanisms:</b> Introduces Auxiliary Interaction (AInt) module and Layout-Trajectory Position Encoding (LTP) to capture human-scene interactions and spatiotemporal dynamics.</div>
            <div class="lang-zh" style="display:none"><b>ä¸“é—¨æœºåˆ¶ï¼š</b>å¼•å…¥è¾…åŠ©äº¤äº’(AInt)æ¨¡å—å’Œå¸ƒå±€-è½¨è¿¹ä½ç½®ç¼–ç (LTP)æ¥æ•æ‰äººç±»-åœºæ™¯äº¤äº’å’Œæ—¶ç©ºåŠ¨æ€ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>State-of-the-Art Performance:</b> Achieves new state-of-the-art results on HIS-QA tasks, establishing foundation for embodied AI and world models research.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›æ€§èƒ½ï¼š</b>åœ¨HIS-QAä»»åŠ¡ä¸Šå–å¾—æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œä¸ºå…·èº«AIå’Œä¸–ç•Œæ¨¡å‹ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/hisgpt_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Joint Human-Scene Understanding:</b> Existing models excel at either 3D scene or human understanding separately, but fail to integrate both modalities for comprehensive HIS comprehension.</div>
            <div class="lang-zh" style="display:none"><b>è”åˆäººç±»-åœºæ™¯ç†è§£ï¼š</b>ç°æœ‰æ¨¡å‹æ“…é•¿åˆ†åˆ«ç†è§£3Dåœºæ™¯æˆ–äººç±»ï¼Œä½†æ— æ³•é›†æˆä¸¤ç§æ¨¡æ€è¿›è¡Œå…¨é¢çš„HISç†è§£ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Complex Interaction Modeling:</b> Capturing intricate human-scene interactions requires understanding spatial relationships, temporal dynamics, and contextual reasoning.</div>
            <div class="lang-zh" style="display:none"><b>å¤æ‚äº¤äº’å»ºæ¨¡ï¼š</b>æ•æ‰å¤æ‚çš„äººç±»-åœºæ™¯äº¤äº’éœ€è¦ç†è§£ç©ºé—´å…³ç³»ã€æ—¶é—´åŠ¨æ€å’Œä¸Šä¸‹æ–‡æ¨ç†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Limited HIS Annotations:</b> Existing HIS datasets lack detailed textual annotations needed for comprehensive evaluation of human-scene understanding tasks.</div>
            <div class="lang-zh" style="display:none"><b>æœ‰é™çš„HISæ ‡æ³¨ï¼š</b>ç°æœ‰çš„HISæ•°æ®é›†ç¼ºä¹è¿›è¡Œå…¨é¢çš„äººç±»-åœºæ™¯ç†è§£ä»»åŠ¡è¯„ä¼°æ‰€éœ€çš„è¯¦ç»†æ–‡æœ¬æ ‡æ³¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Spatiotemporal Reasoning:</b> HIS understanding requires modeling the dynamic interplay between human motions and 3D environments over time.</div>
            <div class="lang-zh" style="display:none"><b>æ—¶ç©ºæ¨ç†ï¼š</b>HISç†è§£éœ€è¦å»ºæ¨¡äººç±»è¿åŠ¨ä¸3Dç¯å¢ƒä¹‹é—´éšæ—¶é—´çš„åŠ¨æ€ç›¸äº’ä½œç”¨ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Benchmark Evaluation:</b> Need for comprehensive benchmarks that systematically evaluate HIS understanding across perception, reasoning, and planning abilities.</div>
            <div class="lang-zh" style="display:none"><b>åŸºå‡†è¯„ä¼°ï¼š</b>éœ€è¦å…¨é¢çš„åŸºå‡†æ¥ç³»ç»Ÿåœ°è¯„ä¼°è·¨æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’èƒ½åŠ›çš„HISç†è§£ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>HIS-QA Task Definition:</b> Formulates Human-In-Scene Question Answering where agents answer questions about human states and behaviors within 3D scenes.</div>
            <div class="lang-zh" style="display:none"><b>HIS-QAä»»åŠ¡å®šä¹‰ï¼š</b>åˆ¶å®šäººç±»-åœºæ™¯é—®ç­”ï¼Œå…¶ä¸­æ™ºèƒ½ä½“å›ç­”å…³äºäººç±»åœ¨3Dåœºæ™¯ä¸­çš„çŠ¶æ€å’Œè¡Œä¸ºçš„é—®é¢˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>HIS-Bench Construction:</b> Builds comprehensive benchmark with 800 questions organized hierarchically into 3 abilities, 7 tasks, and 16 sub-tasks using specialized annotation pipeline.</div>
            <div class="lang-zh" style="display:none"><b>HIS-Benchæ„å»ºï¼š</b>ä½¿ç”¨ä¸“é—¨çš„æ ‡æ³¨æµæ°´çº¿æ„å»ºåŒ…å«800ä¸ªé—®é¢˜çš„ç»¼åˆåŸºå‡†ï¼ŒæŒ‰å±‚æ¬¡ç»„ç»‡ä¸º3ä¸ªèƒ½åŠ›ã€7ä¸ªä»»åŠ¡å’Œ16ä¸ªå­ä»»åŠ¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>HIS-GPT Architecture:</b> Integrates scene encoder and motion encoder with LLM, enabling joint interpretation of 3D scenes and human motions.</div>
            <div class="lang-zh" style="display:none"><b>HIS-GPTæ¶æ„ï¼š</b>å°†åœºæ™¯ç¼–ç å™¨å’Œè¿åŠ¨ç¼–ç å™¨ä¸LLMé›†æˆï¼Œå®ç°å¯¹3Dåœºæ™¯å’Œäººç±»è¿åŠ¨çš„è”åˆè§£é‡Šã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Auxiliary Interaction Module:</b> Enhances interactive cues within modalities through multiple training objectives requiring joint understanding of humans and surroundings.</div>
            <div class="lang-zh" style="display:none"><b>è¾…åŠ©äº¤äº’æ¨¡å—ï¼š</b>é€šè¿‡éœ€è¦è”åˆç†è§£äººç±»å’Œå‘¨å›´ç¯å¢ƒçš„å¤šä¸ªè®­ç»ƒç›®æ ‡æ¥å¢å¼ºæ¨¡æ€å†…çš„äº¤äº’çº¿ç´¢ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Layout-Trajectory Position Encoding:</b> Generates position embeddings by encoding spatial distribution of scene objects and temporal trajectories of human motion.</div>
            <div class="lang-zh" style="display:none"><b>å¸ƒå±€-è½¨è¿¹ä½ç½®ç¼–ç ï¼š</b>é€šè¿‡ç¼–ç åœºæ™¯å¯¹è±¡çš„ç©ºé—´åˆ†å¸ƒå’Œäººç±»è¿åŠ¨çš„æ—¶é—´è½¨è¿¹æ¥ç”Ÿæˆä½ç½®åµŒå…¥ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/hisgpt_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2503.12955.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2503.12955.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <b>Key Advantages:</b> HIS-GPT pioneers multimodal understanding by jointly modeling 3D scenes and human dynamics, addressing the critical gap in embodied AI. The HIS-Bench benchmark establishes a comprehensive evaluation framework for HIS understanding, enabling systematic assessment from perception to planning. By introducing specialized mechanisms for capturing human-scene interactions, HIS-GPT achieves superior performance on complex reasoning tasks that previous models struggled with.
            <br><br>
            <b>Limitations:</b> While effective for understanding human behaviors in static scenes, the current approach may require extensions for highly dynamic environments or multi-person scenarios. The quality of HIS understanding depends on the accuracy of underlying 3D scene and motion representations.
            <br><br>
            <b>Future Impact:</b> This work establishes HIS understanding as a core capability for embodied agents, paving the way for more intelligent robotic assistants and advanced world modeling systems. The HIS-QA framework and HIS-Bench could inspire similar multimodal understanding tasks in other domains.
          </div>
          <div class="lang-zh" style="display:none">
            <b>å…³é”®ä¼˜åŠ¿ï¼š</b>HIS-GPTé€šè¿‡è”åˆå»ºæ¨¡3Dåœºæ™¯å’Œäººç±»åŠ¨æ€å¼€åˆ›äº†å¤šæ¨¡æ€ç†è§£ï¼Œè§£å†³äº†å…·èº«AIä¸­çš„å…³é”®ç©ºç™½ã€‚HIS-BenchåŸºå‡†ä¸ºHISç†è§£å»ºç«‹äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ„ŸçŸ¥åˆ°è§„åˆ’è¿›è¡Œç³»ç»Ÿè¯„ä¼°ã€‚é€šè¿‡å¼•å…¥ä¸“é—¨æœºåˆ¶æ¥æ•æ‰äººç±»-åœºæ™¯äº¤äº’ï¼ŒHIS-GPTåœ¨ä»¥å‰æ¨¡å‹éš¾ä»¥å¤„ç†çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚
            <br><br>
            <b>å±€é™æ€§ï¼š</b>è™½ç„¶åœ¨ç†è§£é™æ€åœºæ™¯ä¸­çš„äººç±»è¡Œä¸ºæ–¹é¢æœ‰æ•ˆï¼Œä½†å½“å‰æ–¹æ³•å¯èƒ½éœ€è¦æ‰©å±•åˆ°é«˜åº¦åŠ¨æ€ç¯å¢ƒæˆ–å¤šäººåœºæ™¯ã€‚HISç†è§£çš„è´¨é‡å–å†³äºåº•å±‚3Dåœºæ™¯å’Œè¿åŠ¨è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚
            <br><br>
            <b>æœªæ¥å½±å“ï¼š</b>è¿™é¡¹å·¥ä½œå°†HISç†è§£ç¡®ç«‹ä¸ºå…·èº«æ™ºèƒ½ä½“çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œä¸ºæ›´æ™ºèƒ½çš„æœºå™¨äººåŠ©æ‰‹å’Œå…ˆè¿›çš„ä¸–ç•Œå»ºæ¨¡ç³»ç»Ÿé“ºå¹³é“è·¯ã€‚HIS-QAæ¡†æ¶å’ŒHIS-Benchå¯èƒ½å¯å‘å…¶ä»–é¢†åŸŸç±»ä¼¼çš„ multimodal ç†è§£ä»»åŠ¡ã€‚
          </div>
        </div>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because HIS-GPT leverages the LLM's proven ability to model complex relationships and generate coherent sequences. By discretizing motions and projecting all modalities into the LLM's token space, it transforms the notoriously difficult problem of multimodal scene-motion reasoning into the well-solved domain of language modeling. The key is treating "human motion in 3D scenes" as a form of "non-verbal language" that LLMs can learn to generate, complete, and understand through unified token prediction.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºHIS-GPTåˆ©ç”¨äº†LLMå»ºæ¨¡å¤æ‚å…³ç³»å’Œç”Ÿæˆè¿è´¯åºåˆ—çš„æˆç†Ÿèƒ½åŠ›ã€‚é€šè¿‡ç¦»æ•£åŒ–è¿åŠ¨å¹¶å°†æ‰€æœ‰æ¨¡æ€æŠ•å½±åˆ°LLMçš„tokenç©ºé—´ï¼Œå®ƒå°†å¤šæ¨¡æ€åœºæ™¯-è¿åŠ¨æ¨ç†è¿™ä¸€å…¬è®¤çš„éš¾é¢˜è½¬æ¢ä¸ºè¯­è¨€å»ºæ¨¡è¿™ä¸€å·²è§£å†³é¢†åŸŸã€‚å…³é”®æ˜¯å°†"3Dåœºæ™¯ä¸­çš„äººç±»è¿åŠ¨"è§†ä¸ºLLMå¯ä»¥é€šè¿‡ç»Ÿä¸€tokené¢„æµ‹æ¥å­¦ä¹ ç”Ÿæˆã€è¡¥å…¨å’Œç†è§£çš„ä¸€ç§"éè¯­è¨€å½¢å¼"ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            HIS-GPT represents a significant advancement in multimodal understanding for embodied agents by introducing the HIS-QA task and establishing comprehensive evaluation through HIS-Bench. The model's innovative integration of 3D scene context and human motion dynamics, enhanced by specialized mechanisms for capturing human-scene interactions, achieves state-of-the-art performance across a wide spectrum of HIS understanding tasks. This work bridges the critical gap between separate scene and human understanding capabilities, enabling embodied agents to comprehensively comprehend human behaviors within 3D environments. By providing both the foundational model and evaluation framework, HIS-GPT establishes a new paradigm for human-in-scene understanding, with broad implications for embodied AI, robotics, and world modeling research. The open-source release of codes and data will facilitate further advancements in this emerging field.
          </div>
          <div class="lang-zh" style="display:none">
            HIS-GPTé€šè¿‡å¼•å…¥HIS-QAä»»åŠ¡å¹¶é€šè¿‡HIS-Benchå»ºç«‹å…¨é¢è¯„ä¼°ï¼Œä»£è¡¨äº†å…·èº«æ™ºèƒ½ä½“å¤šæ¨¡æ€ç†è§£çš„é‡å¤§è¿›æ­¥ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ›æ–°åœ°é›†æˆ3Dåœºæ™¯ä¸Šä¸‹æ–‡å’Œäººç±»è¿åŠ¨åŠ¨æ€ï¼Œå¹¶é€šè¿‡ä¸“é—¨æœºåˆ¶å¢å¼ºäººç±»-åœºæ™¯äº¤äº’çš„æ•æ‰ï¼Œåœ¨å¹¿æ³›çš„HISç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼¥åˆäº†åˆ†ç¦»çš„åœºæ™¯å’Œäººç±»ç†è§£èƒ½åŠ›ä¹‹é—´çš„å…³é”®ç©ºç™½ï¼Œä½¿å…·èº«æ™ºèƒ½ä½“èƒ½å¤Ÿå…¨é¢ç†è§£3Dç¯å¢ƒä¸­çš„äººç±»è¡Œä¸ºã€‚é€šè¿‡æä¾›åŸºç¡€æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶ï¼ŒHIS-GPTä¸ºäººç±»-åœºæ™¯ç†è§£å»ºç«‹äº†æ–°èŒƒå¼ï¼Œå¯¹å…·èº«AIã€æœºå™¨äººå’Œä¸–ç•Œå»ºæ¨¡ç ”ç©¶å…·æœ‰å¹¿æ³›å½±å“ã€‚ä»£ç å’Œæ•°æ®çš„å¼€æºå‘å¸ƒå°†ä¿ƒè¿›è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="font-family:ui-monospace,monospace;font-size:13px;color:rgba(232,236,255,.8);background:rgba(0,0,0,.3);padding:16px;border-radius:8px;border:1px solid rgba(255,255,255,.05);">
          <div class="lang-en">
            <pre><code>import torch
import torch.nn as nn
from transformers import LLaMAForCausalLM, LLaMATokenizer

class HIS_GPT(nn.Module):
    """é€šç”¨åœºæ™¯-è¿åŠ¨-è¯­è¨€æ¨¡å‹"""

    def __init__(self, motion_vocab_size=1024, scene_dim=768):
        super().__init__()
        # å¤šæ¨¡æ€ç¼–ç å™¨
        self.scene_encoder = PointNetSceneEncoder()      # 3Dåœºæ™¯ç¼–ç 
        self.motion_vqvae = MotionVQVAE(vocab_size=motion_vocab_size)  # è¿åŠ¨VQ-VAE
        self.language_model = LLaMAForCausalLM.from_pretrained('llama')

        # å¯¹é½å’Œäº¤äº’æ¨¡å—
        self.scene_lang_aligner = SceneLanguageAligner(scene_dim, 4096)
        self.motion_lang_aligner = MotionLanguageAligner(motion_vocab_size, 4096)
        self.mia = MultiModalInteractionAggregator()     # å¤šæ¨¡æ€äº¤äº’èšåˆå™¨

    def forward(self, scene, motion, text_tokens):
        # 1. å¤šæ¨¡æ€tokenization
        scene_features = self.scene_encoder(scene)              # [B, N_points, D]
        motion_tokens, _ = self.motion_vqvae.encode(motion)     # [B, seq_len]

        # 2. æ¨¡æ€å¯¹é½
        scene_aligned = self.scene_lang_aligner(scene_features, text_tokens)
        motion_aligned = self.motion_lang_aligner(motion_tokens, text_tokens)

        # 3. å¤šæ¨¡æ€äº¤äº’èšåˆ
        interaction_features = self.mia(scene_aligned, motion_aligned)

        # 4. è¯­è¨€æ¨¡å‹æ¨ç†
        combined_tokens = torch.cat([text_tokens, interaction_features], dim=1)
        output = self.language_model(combined_tokens)

        return output

class MotionVQVAE(nn.Module):
    """è¿åŠ¨å‘é‡é‡åŒ–è‡ªç¼–ç å™¨"""

    def __init__(self, vocab_size=1024, hidden_dim=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(135, hidden_dim, 3, padding=1),  # SMPLå…³èŠ‚ç»´åº¦
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, 3, padding=1),
            nn.ReLU(),
        )
        self.codebook = nn.Embedding(vocab_size, hidden_dim)
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(hidden_dim, hidden_dim, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(hidden_dim, 135, 3, padding=1),
        )

    def encode(self, motion):
        """ç¼–ç è¿åŠ¨ä¸ºç¦»æ•£token"""
        features = self.encoder(motion.transpose(1, 2))  # [B, T, 135] -> [B, hidden_dim, T]
        features = features.transpose(1, 2)  # [B, T, hidden_dim]

        # å‘é‡é‡åŒ–
        distances = torch.cdist(features, self.codebook.weight.unsqueeze(0))
        tokens = torch.argmin(distances, dim=-1)  # [B, T]

        return tokens, features

    def decode(self, tokens):
        """ä»tokenè§£ç è¿åŠ¨"""
        embeddings = self.codebook(tokens)  # [B, T, hidden_dim]
        embeddings = embeddings.transpose(1, 2)  # [B, hidden_dim, T]

        motion_recon = self.decoder(embeddings)  # [B, 135, T]
        motion_recon = motion_recon.transpose(1, 2)  # [B, T, 135]

        return motion_recon

class MultiModalInteractionAggregator(nn.Module):
    """å¤šæ¨¡æ€äº¤äº’èšåˆå™¨"""

    def __init__(self, embed_dim=4096, num_heads=8):
        super().__init__()
        self.scene_proj = nn.Linear(768, embed_dim)
        self.motion_proj = nn.Linear(1024, embed_dim)

        self.cross_attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            batch_first=True
        )

        self.affordance_encoder = nn.Sequential(
            nn.Linear(135 * 24, 512),  # å…³èŠ‚æ•° * å¸§æ•°
            nn.ReLU(),
            nn.Linear(512, embed_dim)
        )

    def forward(self, scene_features, motion_tokens, affordance_map=None):
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        scene_emb = self.scene_proj(scene_features)      # [B, seq_len, embed_dim]
        motion_emb = self.motion_proj(motion_tokens)     # [B, seq_len, embed_dim]

        # åœºæ™¯-è¿åŠ¨äº¤å‰æ³¨æ„åŠ›
        attended_scene, _ = self.cross_attention(
            query=scene_emb,
            key=motion_emb,
            value=motion_emb
        )

        # æ•´åˆå¯åŠæ€§ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
        if affordance_map is not None:
            affordance_emb = self.affordance_encoder(
                affordance_map.view(affordance_map.size(0), -1)
            ).unsqueeze(1)  # [B, 1, embed_dim]
            attended_scene = attended_scene + affordance_emb

        return attended_scene

# ä¸‰é˜¶æ®µè®­ç»ƒ
def train_his_gpt():
    """HIS-GPTä¸‰é˜¶æ®µè®­ç»ƒ"""

    model = HIS_GPT()

    # Stage 1: å¤šæ¨¡æ€Tokenizersè®­ç»ƒ
    print("Stage 1: Training multimodal tokenizers...")
    motion_optimizer = torch.optim.Adam(model.motion_vqvae.parameters())

    for epoch in range(100):
        for batch in motion_dataset:
            motion_tokens, reconstructed = model.motion_vqvae(batch['motion'])

            # VQ-VAEæŸå¤±
            recon_loss = F.mse_loss(reconstructed, batch['motion'])
            vq_loss = F.mse_loss(model.motion_vqvae.codebook(motion_tokens.detach()),
                               model.motion_vqvae.encoder(batch['motion']))

            total_loss = recon_loss + 0.25 * vq_loss
            motion_optimizer.zero_grad()
            total_loss.backward()
            motion_optimizer.step()

    # Stage 2: åœºæ™¯-è¿åŠ¨-è¯­è¨€å¯¹é½
    print("Stage 2: Multimodal alignment...")
    full_optimizer = torch.optim.Adam([
        {'params': model.scene_encoder.parameters()},
        {'params': model.scene_lang_aligner.parameters()},
        {'params': model.motion_lang_aligner.parameters()},
        {'params': model.mia.parameters()},
        {'params': model.language_model.parameters(), 'lr': 1e-5}  # è¾ƒä½çš„å­¦ä¹ ç‡
    ])

    for epoch in range(50):
        for batch in alignment_dataset:
            scene, motion, text = batch['scene'], batch['motion'], batch['text']

            # å‰å‘ä¼ æ’­
            output = model(scene, motion, text)

            # å¤šä»»åŠ¡æŸå¤±
            lm_loss = F.cross_entropy(output.view(-1, output.size(-1)), text.view(-1))
            scene_loss = scene_understanding_loss(output, scene)
            motion_loss = motion_reconstruction_loss(output, motion)

            total_loss = lm_loss + 0.1 * scene_loss + 0.1 * motion_loss
            full_optimizer.zero_grad()
            total_loss.backward()
            full_optimizer.step()

    # Stage 3: æŒ‡ä»¤å¾®è°ƒ
    print("Stage 3: Instruction tuning...")
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none"
    )
    model = get_peft_model(model, lora_config)

    instruction_templates = {
        'generation': "Generate motion for '{}' in this 3D scene: {}",
        'completion': "Complete motion {} in scene: {}",
        'captioning': "Describe motion {} in scene: {}",
        'qa': "Answer about human in scene: {}"
    }

    lora_optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

    for epoch in range(10):
        for batch in instruction_dataset:
            task_type = batch['task']
            template = instruction_templates[task_type]

            # æ„å»ºæŒ‡ä»¤
            instruction = template.format(batch['text'], batch['scene_desc'])
            target = batch['target']

            # ç”Ÿæˆå“åº”
            response = model.generate(instruction, scene=batch['scene'])
            loss = instruction_following_loss(response, target)

            lora_optimizer.zero_grad()
            loss.backward()
            lora_optimizer.step()

    print("Training completed!")
    return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = train_his_gpt()

    # ç¤ºä¾‹æ¨ç†
    scene_data = load_3d_scene("living_room.ply")
    instruction = "A person walks to the chair and sits down"

    motion_sequence = model.generate_motion(scene_data, instruction)
    print(f"Generated motion sequence with {motion_sequence.shape[0]} frames")</code></pre>
          </div>
          <div class="lang-zh" style="display:none">
            <pre><code>import torch
import torch.nn as nn
from transformers import LLaMAForCausalLM, LLaMATokenizer

class HIS_GPT(nn.Module):
    """é€šç”¨åœºæ™¯-è¿åŠ¨-è¯­è¨€æ¨¡å‹"""

    def __init__(self, motion_vocab_size=1024, scene_dim=768):
        super().__init__()
        # å¤šæ¨¡æ€ç¼–ç å™¨
        self.scene_encoder = PointNetSceneEncoder()      # 3Dåœºæ™¯ç¼–ç 
        self.motion_vqvae = MotionVQVAE(vocab_size=motion_vocab_size)  # è¿åŠ¨VQ-VAE
        self.language_model = LLaMAForCausalLM.from_pretrained('llama')

        # å¯¹é½å’Œäº¤äº’æ¨¡å—
        self.scene_lang_aligner = SceneLanguageAligner(scene_dim, 4096)
        self.motion_lang_aligner = MotionLanguageAligner(motion_vocab_size, 4096)
        self.mia = MultiModalInteractionAggregator()     # å¤šæ¨¡æ€äº¤äº’èšåˆå™¨

    def forward(self, scene, motion, text_tokens):
        # 1. å¤šæ¨¡æ€tokenization
        scene_features = self.scene_encoder(scene)              # [B, N_points, D]
        motion_tokens, _ = self.motion_vqvae.encode(motion)     # [B, seq_len]

        # 2. æ¨¡æ€å¯¹é½
        scene_aligned = self.scene_lang_aligner(scene_features, text_tokens)
        motion_aligned = self.motion_lang_aligner(motion_tokens, text_tokens)

        # 3. å¤šæ¨¡æ€äº¤äº’èšåˆ
        interaction_features = self.mia(scene_aligned, motion_aligned)

        # 4. è¯­è¨€æ¨¡å‹æ¨ç†
        combined_tokens = torch.cat([text_tokens, interaction_features], dim=1)
        output = self.language_model(combined_tokens)

        return output

class MotionVQVAE(nn.Module):
    """è¿åŠ¨å‘é‡é‡åŒ–è‡ªç¼–ç å™¨"""

    def __init__(self, vocab_size=1024, hidden_dim=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(135, hidden_dim, 3, padding=1),  # SMPLå…³èŠ‚ç»´åº¦
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, 3, padding=1),
            nn.ReLU(),
        )
        self.codebook = nn.Embedding(vocab_size, hidden_dim)
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(hidden_dim, hidden_dim, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(hidden_dim, 135, 3, padding=1),
        )

    def encode(self, motion):
        """ç¼–ç è¿åŠ¨ä¸ºç¦»æ•£token"""
        features = self.encoder(motion.transpose(1, 2))  # [B, T, 135] -> [B, hidden_dim, T]
        features = features.transpose(1, 2)  # [B, T, hidden_dim]

        # å‘é‡é‡åŒ–
        distances = torch.cdist(features, self.codebook.weight.unsqueeze(0))
        tokens = torch.argmin(distances, dim=-1)  # [B, T]

        return tokens, features

    def decode(self, tokens):
        """ä»tokenè§£ç è¿åŠ¨"""
        embeddings = self.codebook(tokens)  # [B, T, hidden_dim]
        embeddings = embeddings.transpose(1, 2)  # [B, hidden_dim, T]

        motion_recon = self.decoder(embeddings)  # [B, 135, T]
        motion_recon = motion_recon.transpose(1, 2)  # [B, T, 135]

        return motion_recon

class MultiModalInteractionAggregator(nn.Module):
    """å¤šæ¨¡æ€äº¤äº’èšåˆå™¨"""

    def __init__(self, embed_dim=4096, num_heads=8):
        super().__init__()
        self.scene_proj = nn.Linear(768, embed_dim)
        self.motion_proj = nn.Linear(1024, embed_dim)

        self.cross_attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            batch_first=True
        )

        self.affordance_encoder = nn.Sequential(
            nn.Linear(135 * 24, 512),  # å…³èŠ‚æ•° * å¸§æ•°
            nn.ReLU(),
            nn.Linear(512, embed_dim)
        )

    def forward(self, scene_features, motion_tokens, affordance_map=None):
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        scene_emb = self.scene_proj(scene_features)      # [B, seq_len, embed_dim]
        motion_emb = self.motion_proj(motion_tokens)     # [B, seq_len, embed_dim]

        # åœºæ™¯-è¿åŠ¨äº¤å‰æ³¨æ„åŠ›
        attended_scene, _ = self.cross_attention(
            query=scene_emb,
            key=motion_emb,
            value=motion_emb
        )

        # æ•´åˆå¯åŠæ€§ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
        if affordance_map is not None:
            affordance_emb = self.affordance_encoder(
                affordance_map.view(affordance_map.size(0), -1)
            ).unsqueeze(1)  # [B, 1, embed_dim]
            attended_scene = attended_scene + affordance_emb

        return attended_scene

# ä¸‰é˜¶æ®µè®­ç»ƒ
def train_his_gpt():
    """HIS-GPTä¸‰é˜¶æ®µè®­ç»ƒ"""

    model = HIS_GPT()

    # Stage 1: å¤šæ¨¡æ€Tokenizersè®­ç»ƒ
    print("Stage 1: å¤šæ¨¡æ€tokenizersè®­ç»ƒ...")
    motion_optimizer = torch.optim.Adam(model.motion_vqvae.parameters())

    for epoch in range(100):
        for batch in motion_dataset:
            motion_tokens, reconstructed = model.motion_vqvae(batch['motion'])

            # VQ-VAEæŸå¤±
            recon_loss = F.mse_loss(reconstructed, batch['motion'])
            vq_loss = F.mse_loss(model.motion_vqvae.codebook(motion_tokens.detach()),
                               model.motion_vqvae.encoder(batch['motion']))

            total_loss = recon_loss + 0.25 * vq_loss
            motion_optimizer.zero_grad()
            total_loss.backward()
            motion_optimizer.step()

    # Stage 2: åœºæ™¯-è¿åŠ¨-è¯­è¨€å¯¹é½
    print("Stage 2: å¤šæ¨¡æ€å¯¹é½...")
    full_optimizer = torch.optim.Adam([
        {'params': model.scene_encoder.parameters()},
        {'params': model.scene_lang_aligner.parameters()},
        {'params': model.motion_lang_aligner.parameters()},
        {'params': model.mia.parameters()},
        {'params': model.language_model.parameters(), 'lr': 1e-5}  # è¾ƒä½çš„å­¦ä¹ ç‡
    ])

    for epoch in range(50):
        for batch in alignment_dataset:
            scene, motion, text = batch['scene'], batch['motion'], batch['text']

            # å‰å‘ä¼ æ’­
            output = model(scene, motion, text)

            # å¤šä»»åŠ¡æŸå¤±
            lm_loss = F.cross_entropy(output.view(-1, output.size(-1)), text.view(-1))
            scene_loss = scene_understanding_loss(output, scene)
            motion_loss = motion_reconstruction_loss(output, motion)

            total_loss = lm_loss + 0.1 * scene_loss + 0.1 * motion_loss
            full_optimizer.zero_grad()
            total_loss.backward()
            full_optimizer.step()

    # Stage 3: æŒ‡ä»¤å¾®è°ƒ
    print("Stage 3: æŒ‡ä»¤å¾®è°ƒ...")
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none"
    )
    model = get_peft_model(model, lora_config)

    instruction_templates = {
        'generation': "ä¸ºæ­¤3Dåœºæ™¯ç”Ÿæˆè¿åŠ¨ '{}': {}",
        'completion': "åœ¨åœºæ™¯ä¸­è¡¥å…¨è¿åŠ¨ {}: {}",
        'captioning': "æè¿°åœºæ™¯ä¸­çš„è¿åŠ¨ {}: {}",
        'qa': "å›ç­”åœºæ™¯ä¸­å…³äºäººç±»çš„é—®é¢˜: {}"
    }

    lora_optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

    for epoch in range(10):
        for batch in instruction_dataset:
            task_type = batch['task']
            template = instruction_templates[task_type]

            # æ„å»ºæŒ‡ä»¤
            instruction = template.format(batch['text'], batch['scene_desc'])
            target = batch['target']

            # ç”Ÿæˆå“åº”
            response = model.generate(instruction, scene=batch['scene'])
            loss = instruction_following_loss(response, target)

            lora_optimizer.zero_grad()
            loss.backward()
            lora_optimizer.step()

    print("è®­ç»ƒå®Œæˆ!")
    return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    model = train_his_gpt()

    # ç¤ºä¾‹æ¨ç†
    scene_data = load_3d_scene("living_room.ply")
    instruction = "ä¸€ä¸ªäººèµ°åˆ°æ¤…å­æ—è¾¹åä¸‹æ¥"

    motion_sequence = model.generate_motion(scene_data, instruction)
    print(f"ç”Ÿæˆäº† {motion_sequence.shape[0]} å¸§çš„è¿åŠ¨åºåˆ—")</code></pre>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://github.com/ZJHTerry18/HumanInScene" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          GitHub Repo
        </a>
      </div>
    </div>
</section>
</body>
</html>
