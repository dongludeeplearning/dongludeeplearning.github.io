<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ CVPR 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, Hanbyul Joo<br>
        <span style="opacity:0.8">Seoul National University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to capture high-quality, natural 3D human-object interactions in everyday home environments for generative modeling?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åœ¨æ—¥å¸¸å®¶åº­ç¯å¢ƒä¸­æ•è·é«˜è´¨é‡ã€è‡ªç„¶çš„3Däººç±»-ç‰©ä½“äº¤äº’ä»¥è¿›è¡Œç”Ÿæˆå»ºæ¨¡ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>ParaHome Capture System:</b> Multi-view setup with 70 synchronized RGB cameras combined with IMU-based body suit and hand motion capture gloves for robust 3D HOI capture in natural home environments, handling severe occlusions and multiple scales.</div>
            <div class="lang-zh" style="display:none"><b>ParaHomeæ•è·ç³»ç»Ÿï¼š</b>70ä¸ªåŒæ­¥RGBç›¸æœºçš„å¤šè§†è§’è®¾ç½®ï¼Œç»“åˆIMU-basedèº«ä½“å¥—è£…å’Œæ‰‹éƒ¨è¿åŠ¨æ•æ‰æ‰‹å¥—ï¼Œç”¨äºåœ¨è‡ªç„¶å®¶åº­ç¯å¢ƒä¸­è¿›è¡Œé²æ£’çš„3D HOIæ•è·ï¼Œå¤„ç†ä¸¥é‡é®æŒ¡å’Œå¤šå°ºåº¦é—®é¢˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Large-Scale ParaHome Dataset:</b> Comprehensive dataset with 486 minutes of sequences across 207 captures from 38 participants, featuring full-body motion, dexterous hand manipulation, multiple object interactions, sequential/concurrent manipulations with text descriptions, and articulated objects with 3D parametric models.</div>
            <div class="lang-zh" style="display:none"><b>å¤§è§„æ¨¡ParaHomeæ•°æ®é›†ï¼š</b>å…¨é¢æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª38ä¸ªå‚ä¸è€…çš„207ä¸ªæ•è·ä¸­çš„486åˆ†é’Ÿåºåˆ—ï¼Œå…·æœ‰å…¨èº«è¿åŠ¨ã€çµå·§æ‰‹éƒ¨æ“ä½œã€å¤šç‰©ä½“äº¤äº’ã€é…å¯¹æ–‡æœ¬æè¿°çš„é¡ºåº/å¹¶å‘æ“ä½œï¼Œä»¥åŠå…·æœ‰3Då‚æ•°åŒ–æ¨¡å‹çš„å…³èŠ‚ç‰©ä½“ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>3D Parametric Representation:</b> Unified parameterization of human motions (body, hands), object motions (rigid and articulated), and spatio-temporal relations in common 3D spaces, enabling detailed modeling of complex HOI dynamics.</div>
            <div class="lang-zh" style="display:none"><b>3Då‚æ•°åŒ–è¡¨ç¤ºï¼š</b>åœ¨å…¬å…±3Dç©ºé—´ä¸­ç»Ÿä¸€å‚æ•°åŒ–äººç±»è¿åŠ¨ï¼ˆèº«ä½“ã€æ‰‹éƒ¨ï¼‰ã€ç‰©ä½“è¿åŠ¨ï¼ˆåˆšæ€§å’Œå…³èŠ‚ï¼‰ï¼Œä»¥åŠæ—¶ç©ºå…³ç³»ï¼Œå®ç°å¯¹å¤æ‚HOIåŠ¨åŠ›å­¦çš„è¯¦ç»†å»ºæ¨¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>System Design Justifications:</b> Hardware and algorithmic solutions for spatial alignment of heterogeneous systems, robust marker-based object tracking, and calibration protocols ensuring accurate reconstruction of human-object interactions.</div>
            <div class="lang-zh" style="display:none"><b>ç³»ç»Ÿè®¾è®¡ç†ç”±ï¼š</b>å¼‚æ„ç³»ç»Ÿçš„ç©ºé—´å¯¹é½ã€é²æ£’çš„åŸºäºæ ‡è®°çš„ç‰©ä½“è·Ÿè¸ªä»¥åŠæ ¡å‡†åè®®çš„ç¡¬ä»¶å’Œç®—æ³•è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿äººç±»-ç‰©ä½“äº¤äº’çš„å‡†ç¡®é‡å»ºã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Generative Modeling Demonstrations:</b> Text-conditioned and object-guided motion synthesis experiments showcasing the dataset's potential for advancing 3D HOI generative modeling with realistic, natural interactions.</div>
            <div class="lang-zh" style="display:none"><b>ç”Ÿæˆå»ºæ¨¡æ¼”ç¤ºï¼š</b>æ–‡æœ¬æ¡ä»¶å’Œç‰©ä½“å¼•å¯¼çš„è¿åŠ¨åˆæˆå®éªŒï¼Œå±•ç¤ºäº†æ•°æ®é›†åœ¨æ¨è¿›å…·æœ‰çœŸå®ã€è‡ªç„¶äº¤äº’çš„3D HOIç”Ÿæˆå»ºæ¨¡æ–¹é¢çš„æ½œåŠ›ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/parahome_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Scale Motion Capture:</b> Simultaneous capture of large-scale body movements across rooms and subtle hand manipulations requires heterogeneous systems that can handle different scales and severe occlusions during interactions.</div>
            <div class="lang-zh" style="display:none"><b>å¤šå°ºåº¦è¿åŠ¨æ•è·ï¼š</b>åŒæ—¶æ•è·æˆ¿é—´èŒƒå›´å†…çš„å…¨èº«å¤§å°ºåº¦è¿åŠ¨å’Œæ‰‹éƒ¨ç»†å¾®æ“ä½œï¼Œéœ€è¦èƒ½å¤Ÿå¤„ç†ä¸åŒå°ºåº¦å’Œäº¤äº’æœŸé—´ä¸¥é‡é®æŒ¡çš„å¼‚æ„ç³»ç»Ÿã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Spatial Alignment of Systems:</b> Integrating multi-camera RGB system with wearable IMU devices requires robust calibration and alignment protocols to establish common 3D spatiotemporal coordinates for accurate interaction reconstruction.</div>
            <div class="lang-zh" style="display:none"><b>ç³»ç»Ÿç©ºé—´å¯¹é½ï¼š</b>å°†å¤šç›¸æœºRGBç³»ç»Ÿä¸ç©¿æˆ´å¼IMUè®¾å¤‡é›†æˆéœ€è¦é²æ£’çš„æ ¡å‡†å’Œå¯¹é½åè®®ï¼Œä»¥å»ºç«‹å…¬å…±3Dæ—¶ç©ºåæ ‡ç³»ï¼Œå®ç°å‡†ç¡®çš„äº¤äº’é‡å»ºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Articulated Object Tracking:</b> Capturing dynamic motions of articulated objects with multiple moving parts requires sophisticated marker designs and tracking algorithms that work under severe occlusions during manipulation.</div>
            <div class="lang-zh" style="display:none"><b>å…³èŠ‚ç‰©ä½“è·Ÿè¸ªï¼š</b>æ•è·å…·æœ‰å¤šä¸ªç§»åŠ¨éƒ¨ä»¶çš„å…³èŠ‚ç‰©ä½“çš„åŠ¨æ€è¿åŠ¨ï¼Œéœ€è¦åœ¨æ“ä½œæœŸé—´ä¸¥é‡é®æŒ¡ä¸‹å·¥ä½œçš„å¤æ‚æ ‡è®°è®¾è®¡å’Œè·Ÿè¸ªç®—æ³•ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Natural Interaction Diversity:</b> Collecting diverse, natural human-object interactions in home environments requires capture protocols that encourage casual, sequential, and concurrent manipulations while maintaining data quality and annotations.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªç„¶äº¤äº’å¤šæ ·æ€§ï¼š</b>åœ¨å®¶åº­ç¯å¢ƒä¸­æ”¶é›†å¤šæ ·åŒ–ã€è‡ªç„¶çš„äººç±»-ç‰©ä½“äº¤äº’éœ€è¦é¼“åŠ±éšæ„ã€é¡ºåºå’Œå¹¶å‘æ“ä½œçš„æ•è·åè®®ï¼ŒåŒæ—¶ä¿æŒæ•°æ®è´¨é‡å’Œæ ‡æ³¨ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Unified Parameterization:</b> Creating consistent 3D parametric representations for heterogeneous data sources (human motions, object states, articulations) to enable joint modeling and generative synthesis of complex HOI scenarios.</div>
            <div class="lang-zh" style="display:none"><b>ç»Ÿä¸€å‚æ•°åŒ–ï¼š</b>ä¸ºå¼‚æ„æ•°æ®æºï¼ˆäººç±»è¿åŠ¨ã€ç‰©ä½“çŠ¶æ€ã€å…³èŠ‚ï¼‰åˆ›å»ºä¸€è‡´çš„3Då‚æ•°åŒ–è¡¨ç¤ºï¼Œä»¥å®ç°å¤æ‚HOIåœºæ™¯çš„è”åˆå»ºæ¨¡å’Œç”Ÿæˆåˆæˆã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-View Camera System:</b> 70 synchronized RGB industrial cameras covering 12.4mÂ² area to capture full-body motions, subtle hand manipulations, and 3D object movements with reduced occlusion issues.</div>
            <div class="lang-zh" style="display:none"><b>å¤šè§†è§’ç›¸æœºç³»ç»Ÿï¼š</b>70ä¸ªåŒæ­¥RGBå·¥ä¸šç›¸æœºè¦†ç›–12.4mÂ²åŒºåŸŸï¼Œä»¥æ•è·å…¨èº«è¿åŠ¨ã€ç»†å¾®æ‰‹éƒ¨æ“ä½œå’Œ3Dç‰©ä½“è¿åŠ¨ï¼Œå‡å°‘é®æŒ¡é—®é¢˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Wearable Motion Capture:</b> IMU-based Xsens body suit and Manus hand gloves for occlusion-robust capture of human motions, with spatial alignment protocols to integrate with camera system coordinates.</div>
            <div class="lang-zh" style="display:none"><b>ç©¿æˆ´å¼è¿åŠ¨æ•è·ï¼š</b>åŸºäºIMUçš„Xsensèº«ä½“å¥—è£…å’ŒManusæ‰‹å¥—ç”¨äºé®æŒ¡é²æ£’çš„äººç±»è¿åŠ¨æ•è·ï¼Œå…·æœ‰ç©ºé—´å¯¹é½åè®®ä»¥é›†æˆç›¸æœºç³»ç»Ÿåæ ‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>3D ArUco Marker System:</b> Robust 3D marker design with markers attached to all cube faces for reliable object tracking during severe occlusions, enabling capture of both rigid and articulated object motions.</div>
            <div class="lang-zh" style="display:none"><b>3D ArUcoæ ‡è®°ç³»ç»Ÿï¼š</b>é²æ£’çš„3Dæ ‡è®°è®¾è®¡ï¼Œå°†æ ‡è®°é™„ç€åœ¨æ‰€æœ‰ç«‹æ–¹ä½“é¢ä¸Šï¼Œç”¨äºåœ¨ä¸¥é‡é®æŒ¡æœŸé—´å¯é çš„ç‰©ä½“è·Ÿè¸ªï¼Œå®ç°åˆšæ€§å’Œå…³èŠ‚ç‰©ä½“è¿åŠ¨çš„æ•è·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>System Calibration Protocols:</b> Body alignment using attached markers and hand calibration using structured touch protocols to spatially align wearable capture with multi-camera system in common 3D coordinates.</div>
            <div class="lang-zh" style="display:none"><b>ç³»ç»Ÿæ ¡å‡†åè®®ï¼š</b>ä½¿ç”¨é™„ç€æ ‡è®°çš„èº«ä½“å¯¹é½å’Œä½¿ç”¨ç»“æ„åŒ–è§¦æ‘¸åè®®çš„æ‰‹éƒ¨æ ¡å‡†ï¼Œå°†ç©¿æˆ´å¼æ•è·ä¸å¤šç›¸æœºç³»ç»Ÿåœ¨å…¬å…±3Dåæ ‡ä¸­è¿›è¡Œç©ºé—´å¯¹é½ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Parametric Representation:</b> Unified 3D parameterization capturing human status (body, hands), environment status (object positions, orientations, articulations), and language descriptions for comprehensive HOI modeling.</div>
            <div class="lang-zh" style="display:none"><b>å‚æ•°åŒ–è¡¨ç¤ºï¼š</b>ç»Ÿä¸€çš„3Då‚æ•°åŒ–ï¼Œæ•è·äººç±»çŠ¶æ€ï¼ˆèº«ä½“ã€æ‰‹éƒ¨ï¼‰ã€ç¯å¢ƒçŠ¶æ€ï¼ˆç‰©ä½“ä½ç½®ã€æ–¹å‘ã€å…³èŠ‚ï¼‰å’Œè¯­è¨€æè¿°ï¼Œç”¨äºå…¨é¢çš„HOIå»ºæ¨¡ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/parahome_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2401.10232.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2401.10232.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Scalability vs. Naturalness Trade-off:</b> ParaHome demonstrates that combining multi-view cameras with wearable devices can capture natural, unconstrained interactions at scale, providing a foundation for generative models that understand real-world HOI dynamics.
            </div>
            <div class="lang-zh" style="display:none">
                <b>å¯æ‰©å±•æ€§ä¸è‡ªç„¶æ€§çš„æƒè¡¡ï¼š</b>ParaHomeè¯æ˜äº†å°†å¤šè§†è§’ç›¸æœºä¸ç©¿æˆ´è®¾å¤‡ç›¸ç»“åˆå¯ä»¥å¤§è§„æ¨¡æ•è·è‡ªç„¶ã€ä¸å—çº¦æŸçš„äº¤äº’ï¼Œä¸ºç†è§£çœŸå®ä¸–ç•ŒHOIåŠ¨æ€çš„ç”Ÿæˆæ¨¡å‹æä¾›åŸºç¡€ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Multi-Modal Integration:</b> The system's ability to jointly capture visual, inertial, and parametric data creates rich multi-modal representations essential for understanding the complex relationships between human intentions, physical constraints, and object affordances.
            </div>
            <div class="lang-zh" style="display:none">
                <b>å¤šæ¨¡æ€é›†æˆï¼š</b>ç³»ç»ŸåŒæ—¶æ•è·è§†è§‰ã€æƒ¯æ€§å’Œå‚æ•°åŒ–æ•°æ®çš„èƒ½åŠ›ï¼Œåˆ›å»ºäº†ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œå¯¹äºç†è§£äººç±»æ„å›¾ã€ç‰©ç†çº¦æŸå’Œç‰©ä½“å¯åŠæ€§ä¹‹é—´çš„å¤æ‚å…³ç³»è‡³å…³é‡è¦ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Occlusion-Robust Capture:</b> The hybrid approach of cameras and IMUs provides complementary strengths, enabling capture of interactions that would be impossible with single-modality systems due to occlusions and scale differences.
            </div>
            <div class="lang-zh" style="display:none">
                <b>é®æŒ¡é²æ£’æ•è·ï¼š</b>ç›¸æœºå’ŒIMUçš„æ··åˆæ–¹æ³•æä¾›äº†äº’è¡¥ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ•è·ç”±äºé®æŒ¡å’Œå°ºåº¦å·®å¼‚è€Œæ— æ³•ç”¨å•æ¨¡æ€ç³»ç»Ÿå®ç°çš„äº¤äº’ã€‚
            </div>
          </li>
        </ul>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because ParaHome addresses the fundamental capture challenges in natural HOI scenarios by integrating complementary sensing modalities. The multi-view camera system handles visual reconstruction while wearable devices provide occlusion-robust motion capture, creating a comprehensive 3D parametric space that captures the full complexity of human-object interactions. By parameterizing everyday activities with detailed spatio-temporal relationships, the system enables generative models to learn realistic interaction patterns that go beyond simple grasping to include complex sequential and concurrent manipulations in natural home environments.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºParaHomeé€šè¿‡é›†æˆäº’è¡¥çš„ä¼ æ„Ÿæ¨¡æ€æ¥è§£å†³è‡ªç„¶HOIåœºæ™¯ä¸­çš„åŸºæœ¬æ•è·æŒ‘æˆ˜ã€‚å¤šè§†è§’ç›¸æœºç³»ç»Ÿå¤„ç†è§†è§‰é‡å»ºï¼Œè€Œç©¿æˆ´è®¾å¤‡æä¾›é®æŒ¡é²æ£’çš„è¿åŠ¨æ•è·ï¼Œåˆ›å»ºä¸€ä¸ªæ•è·äººç±»-ç‰©ä½“äº¤äº’å®Œæ•´å¤æ‚æ€§çš„å…¨é¢3Då‚æ•°ç©ºé—´ã€‚é€šè¿‡ç”¨è¯¦ç»†çš„æ—¶ç©ºå…³ç³»å‚æ•°åŒ–æ—¥å¸¸æ´»åŠ¨ï¼Œè¯¥ç³»ç»Ÿä½¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¶…è¶Šç®€å•æŠ“å–çš„çœŸå®äº¤äº’æ¨¡å¼ï¼ŒåŒ…æ‹¬è‡ªç„¶å®¶åº­ç¯å¢ƒä¸­çš„å¤æ‚é¡ºåºå’Œå¹¶å‘æ“ä½œã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            This work presents ParaHome, a comprehensive capture system and dataset for 3D human-object interactions in natural home environments. By integrating multi-view cameras with wearable motion capture devices and developing sophisticated calibration protocols, the system overcomes fundamental challenges in capturing complex, multi-scale interactions. The resulting dataset with 486 minutes of parameterized sequences provides unprecedented detail and naturalness for HOI research. Through detailed design justifications and generative modeling experiments, ParaHome establishes a new benchmark for capturing and modeling everyday human-object interactions, enabling future advances in embodied AI, robotics, and computer vision applications that require understanding of natural human behavior in physical environments.
          </div>
          <div class="lang-zh" style="display:none">
            æœ¬å·¥ä½œæå‡ºäº†ParaHomeï¼Œä¸€ä¸ªç”¨äºè‡ªç„¶å®¶åº­ç¯å¢ƒä¸­3Däººç±»-ç‰©ä½“äº¤äº’çš„ç»¼åˆæ•è·ç³»ç»Ÿå’Œæ•°æ®é›†ã€‚é€šè¿‡å°†å¤šè§†è§’ç›¸æœºä¸ç©¿æˆ´å¼è¿åŠ¨æ•è·è®¾å¤‡é›†æˆï¼Œå¹¶å¼€å‘å¤æ‚çš„æ ¡å‡†åè®®ï¼Œè¯¥ç³»ç»Ÿå…‹æœäº†æ•è·å¤æ‚ã€å¤šå°ºåº¦äº¤äº’çš„åŸºæœ¬æŒ‘æˆ˜ã€‚ç”±æ­¤äº§ç”Ÿçš„åŒ…å«486åˆ†é’Ÿå‚æ•°åŒ–åºåˆ—çš„æ•°æ®é›†ä¸ºHOIç ”ç©¶æä¾›äº†å‰æ‰€æœªæœ‰çš„ç»†èŠ‚å’Œè‡ªç„¶æ€§ã€‚é€šè¿‡è¯¦ç»†çš„è®¾è®¡ç†ç”±å’Œç”Ÿæˆå»ºæ¨¡å®éªŒï¼ŒParaHomeä¸ºæ•è·å’Œå»ºæ¨¡æ—¥å¸¸äººç±»-ç‰©ä½“äº¤äº’å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œä½¿æœªæ¥åœ¨éœ€è¦ç†è§£ç‰©ç†ç¯å¢ƒä¸­è‡ªç„¶äººç±»è¡Œä¸ºçš„å…·èº«AIã€æœºå™¨äººå’Œè®¡ç®—æœºè§†è§‰åº”ç”¨æ–¹é¢å–å¾—è¿›å±•ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="font-family:ui-monospace,monospace;font-size:13px;color:rgba(232,236,255,.8);background:rgba(0,0,0,.3);padding:16px;border-radius:8px;border:1px solid rgba(255,255,255,.05);">
          <div class="lang-en">
            <pre><code>import numpy as np
import torch
import torch.nn as nn
from scipy.spatial import KDTree
from smplx import SMPLH, SMPLX

class ParaHomeCaptureSystem:
    """Multi-view capture system for natural HOI in home environments"""

    def __init__(self, num_cameras=70, body_markers=11, hand_calibration_points=6):
        self.num_cameras = num_cameras
        self.body_markers = body_markers
        self.hand_calibration_points = hand_calibration_points

        # Initialize capture devices
        self.camera_system = MultiViewCameraSystem(num_cameras)
        self.imu_body_suit = XsensBodySuit()
        self.hand_gloves = ManusHandGloves()

        # 3D parametric models
        self.smplh_model = SMPLH()
        self.smplx_model = SMPLX()

    def capture_hoi_sequence(self, participant_id, activity_type):
        """Main capture pipeline for HOI sequences"""
        # 1. System calibration
        body_transform = self.calibrate_body_alignment()
        hand_transform = self.calibrate_hand_alignment()

        # 2. Multi-view capture
        frames_data = []
        for frame_idx in range(self.capture_duration):
            # Capture multi-view images
            multi_view_images = self.camera_system.capture_frame()

            # Capture IMU data
            body_imu_data = self.imu_body_suit.get_pose()
            hand_imu_data = self.hand_gloves.get_pose()

            # Process frame
            frame_data = self.process_frame(
                multi_view_images, body_imu_data, hand_imu_data,
                body_transform, hand_transform
            )
            frames_data.append(frame_data)

        return self.postprocess_sequence(frames_data)

    def calibrate_body_alignment(self):
        """Body calibration using attached markers"""
        print("Performing body alignment calibration...")

        # Capture calibration sequence with visible markers
        calibration_frames = []
        for _ in range(100):  # Capture calibration motions
            multi_view_images = self.camera_system.capture_frame()
            body_imu_data = self.imu_body_suit.get_pose()

            # Extract marker positions from images
            camera_markers = self.extract_markers_from_images(multi_view_images)
            # Get IMU-based marker predictions
            imu_markers = self.predict_markers_from_imu(body_imu_data)

            calibration_frames.append((camera_markers, imu_markers))

        # Optimize transformation between camera and IMU spaces
        body_transform = self.optimize_body_transform(calibration_frames)

        return body_transform

    def calibrate_hand_alignment(self):
        """Hand calibration using structured touch protocol"""
        print("Performing hand alignment calibration...")

        # Define calibration structure vertices
        calibration_vertices = self.define_calibration_structure()

        calibration_frames = []
        for vertex in calibration_vertices:
            print(f"Touch calibration point {len(calibration_frames)+1}/6")

            # Participant touches the calibration point
            multi_view_images = self.camera_system.capture_frame()
            hand_imu_data = self.hand_gloves.get_pose()

            # Extract fingertip positions from images
            camera_fingertips = self.extract_fingertips_from_images(multi_view_images)
            # Get IMU-based fingertip predictions
            imu_fingertips = self.predict_fingertips_from_imu(hand_imu_data)

            calibration_frames.append((camera_fingertips, imu_fingertips, vertex))

        # Optimize hand transformation
        hand_transform = self.optimize_hand_transform(calibration_frames)

        return hand_transform

    def process_frame(self, multi_view_images, body_imu_data, hand_imu_data,
                     body_transform, hand_transform):
        """Process single frame data"""
        # 1. Extract 3D markers and objects from multi-view images
        markers_3d, objects_3d = self.reconstruct_3d_from_multiview(multi_view_images)

        # 2. Fuse IMU and camera data for human pose
        human_pose = self.fuse_imu_camera_data(
            body_imu_data, hand_imu_data, markers_3d,
            body_transform, hand_transform
        )

        # 3. Track and parameterize objects
        object_params = self.track_objects(objects_3d)

        # 4. Create parametric representation
        frame_data = {
            'human_pose': human_pose,  # SMPL-H parameters
            'object_params': object_params,  # Rigid + articulated params
            'markers_3d': markers_3d,  # 3D marker positions
            'timestamp': self.get_timestamp()
        }

        return frame_data

    def reconstruct_3d_from_multiview(self, multi_view_images):
        """Reconstruct 3D scene from multi-view images"""
        markers_3d = []
        objects_3d = []

        # Extract 2D detections from each view
        all_detections = []
        for view_idx, image in enumerate(multi_view_images):
            detections = self.detect_markers_and_objects(image, view_idx)
            all_detections.append(detections)

        # Triangulate 3D positions
        for marker_id in range(self.get_total_markers()):
            marker_2d_points = []
            camera_indices = []

            for view_idx, detections in enumerate(all_detections):
                if marker_id in detections['markers']:
                    marker_2d_points.append(detections['markers'][marker_id])
                    camera_indices.append(view_idx)

            if len(marker_2d_points) >= 2:  # Need at least 2 views
                marker_3d = self.triangulate_point(marker_2d_points, camera_indices)
                markers_3d.append(marker_3d)

        # Similar process for objects
        for obj_id in self.object_ids:
            obj_2d_detections = []
            for view_idx, detections in enumerate(all_detections):
                if obj_id in detections['objects']:
                    obj_2d_detections.append((detections['objects'][obj_id], view_idx))

            if len(obj_2d_detections) >= 2:
                obj_3d = self.reconstruct_object_3d(obj_2d_detections)
                objects_3d.append(obj_3d)

        return markers_3d, objects_3d

    def fuse_imu_camera_data(self, body_imu, hand_imu, markers_3d,
                           body_transform, hand_transform):
        """Fuse IMU predictions with camera observations"""
        # Apply transformations to IMU data
        body_imu_transformed = self.apply_transform(body_imu, body_transform)
        hand_imu_transformed = self.apply_transform(hand_imu, hand_transform)

        # Use markers for refinement
        refined_pose = self.refine_pose_with_markers(
            body_imu_transformed, hand_imu_transformed, markers_3d
        )

        return refined_pose

    def track_objects(self, objects_3d):
        """Track and parameterize 3D objects"""
        object_params = {}

        for obj_3d in objects_3d:
            obj_id = obj_3d['id']

            # Rigid transformation
            rigid_transform = self.compute_rigid_transform(obj_3d)

            # Articulated parameters (if applicable)
            articulated_params = self.compute_articulated_params(obj_3d)

            object_params[obj_id] = {
                'rigid': rigid_transform,  # [R, t, scale]
                'articulated': articulated_params,  # Joint angles/states
                'type': obj_3d['type']  # chair, laptop, etc.
            }

        return object_params

    def optimize_body_transform(self, calibration_frames):
        """Optimize transformation between IMU and camera spaces"""
        def objective(transform):
            total_error = 0
            for camera_markers, imu_markers in calibration_frames:
                # Transform IMU markers to camera space
                transformed_imu = self.apply_transform_to_points(imu_markers, transform)

                # Compute alignment error
                error = np.mean(np.linalg.norm(camera_markers - transformed_imu, axis=1))
                total_error += error

            return total_error

        # Optimize using gradient-based method
        from scipy.optimize import minimize
        initial_transform = np.eye(4)  # Initial rigid transform
        result = minimize(objective, initial_transform.flatten(),
                         method='L-BFGS-B')

        return result.x.reshape(4, 4)

    def optimize_hand_transform(self, calibration_frames):
        """Optimize hand transformation using calibration protocol"""
        def objective(transform):
            total_error = 0
            for camera_fingertips, imu_fingertips, target_point in calibration_frames:
                # Transform IMU fingertips to camera space
                transformed_imu = self.apply_transform_to_points(imu_fingertips, transform)

                # Error between transformed fingertips and target point
                error = np.mean(np.linalg.norm(camera_fingertips - target_point, axis=1))
                total_error += error

            return total_error

        # Similar optimization as body transform
        initial_transform = np.eye(4)
        result = minimize(objective, initial_transform.flatten(),
                         method='L-BFGS-B')

        return result.x.reshape(4, 4)

    def postprocess_sequence(self, frames_data):
        """Post-process captured sequence"""
        # Temporal smoothing
        smoothed_frames = self.temporal_smooth(frames_data)

        # Physical constraint checking
        validated_frames = self.validate_physics_constraints(smoothed_frames)

        # Create final HOI dataset entry
        hoi_sequence = {
            'participant_id': self.current_participant,
            'activity_type': self.current_activity,
            'frames': validated_frames,
            'metadata': {
                'duration': len(frames_data) / 30.0,  # Assuming 30fps
                'objects_present': list(set([obj for frame in frames_data
                                           for obj in frame['object_params'].keys()])),
                'quality_score': self.compute_sequence_quality(validated_frames)
            }
        }

        return hoi_sequence

# Usage example
if __name__ == "__main__":
    # Initialize ParaHome system
    parahome = ParaHomeCaptureSystem(num_cameras=70)

    # Capture HOI sequences
    activities = ["making_coffee", "setting_table", "folding_laundry"]

    for participant_id in range(1, 39):  # 38 participants
        for activity in activities:
            print(f"Capturing participant {participant_id}, activity: {activity}")

            # Perform system calibration
            body_transform = parahome.calibrate_body_alignment()
            hand_transform = parahome.calibrate_hand_alignment()

            # Capture sequence
            hoi_sequence = parahome.capture_hoi_sequence(participant_id, activity)

            # Save to dataset
            save_to_dataset(hoi_sequence)

    print("ParaHome capture completed!")</code></pre>
          </div>
          <div class="lang-zh" style="display:none">
            <pre><code>import numpy as np
import torch
import torch.nn as nn
from scipy.spatial import KDTree
from smplx import SMPLH, SMPLX

class ParaHomeCaptureSystem:
    """å®¶åº­ç¯å¢ƒä¸­è‡ªç„¶HOIçš„å¤šè§†è§’æ•è·ç³»ç»Ÿ"""

    def __init__(self, num_cameras=70, body_markers=11, hand_calibration_points=6):
        self.num_cameras = num_cameras
        self.body_markers = body_markers
        self.hand_calibration_points = hand_calibration_points

        # åˆå§‹åŒ–æ•è·è®¾å¤‡
        self.camera_system = MultiViewCameraSystem(num_cameras)
        self.imu_body_suit = XsensBodySuit()
        self.hand_gloves = ManusHandGloves()

        # 3Då‚æ•°åŒ–æ¨¡å‹
        self.smplh_model = SMPLH()
        self.smplx_model = SMPLX()

    def capture_hoi_sequence(self, participant_id, activity_type):
        """HOIåºåˆ—çš„ä¸»è¦æ•è·æµæ°´çº¿"""
        # 1. ç³»ç»Ÿæ ¡å‡†
        body_transform = self.calibrate_body_alignment()
        hand_transform = self.calibrate_hand_alignment()

        # 2. å¤šè§†è§’æ•è·
        frames_data = []
        for frame_idx in range(self.capture_duration):
            # æ•è·å¤šè§†è§’å›¾åƒ
            multi_view_images = self.camera_system.capture_frame()

            # æ•è·IMUæ•°æ®
            body_imu_data = self.imu_body_suit.get_pose()
            hand_imu_data = self.hand_gloves.get_pose()

            # å¤„ç†å¸§
            frame_data = self.process_frame(
                multi_view_images, body_imu_data, hand_imu_data,
                body_transform, hand_transform
            )
            frames_data.append(frame_data)

        return self.postprocess_sequence(frames_data)

    def calibrate_body_alignment(self):
        """ä½¿ç”¨é™„ç€æ ‡è®°çš„èº«ä½“æ ¡å‡†"""
        print("æ‰§è¡Œèº«ä½“å¯¹é½æ ¡å‡†...")

        # ä½¿ç”¨å¯è§æ ‡è®°æ•è·æ ¡å‡†åºåˆ—
        calibration_frames = []
        for _ in range(100):  # æ•è·æ ¡å‡†åŠ¨ä½œ
            multi_view_images = self.camera_system.capture_frame()
            body_imu_data = self.imu_body_suit.get_pose()

            # ä»å›¾åƒä¸­æå–æ ‡è®°ä½ç½®
            camera_markers = self.extract_markers_from_images(multi_view_images)
            # è·å–åŸºäºIMUçš„æ ‡è®°é¢„æµ‹
            imu_markers = self.predict_markers_from_imu(body_imu_data)

            calibration_frames.append((camera_markers, imu_markers))

        # ä¼˜åŒ–ç›¸æœºå’ŒIMUç©ºé—´ä¹‹é—´çš„å˜æ¢
        body_transform = self.optimize_body_transform(calibration_frames)

        return body_transform

    def calibrate_hand_alignment(self):
        """ä½¿ç”¨ç»“æ„åŒ–è§¦æ‘¸åè®®çš„æ‰‹éƒ¨æ ¡å‡†"""
        print("æ‰§è¡Œæ‰‹éƒ¨å¯¹é½æ ¡å‡†...")

        # å®šä¹‰æ ¡å‡†ç»“æ„é¡¶ç‚¹
        calibration_vertices = self.define_calibration_structure()

        calibration_frames = []
        for vertex in calibration_vertices:
            print(f"è§¦æ‘¸æ ¡å‡†ç‚¹ {len(calibration_frames)+1}/6")

            # å‚ä¸è€…è§¦æ‘¸æ ¡å‡†ç‚¹
            multi_view_images = self.camera_system.capture_frame()
            hand_imu_data = self.hand_gloves.get_pose()

            # ä»å›¾åƒä¸­æå–æŒ‡å°–ä½ç½®
            camera_fingertips = self.extract_fingertips_from_images(multi_view_images)
            # è·å–åŸºäºIMUçš„æŒ‡å°–é¢„æµ‹
            imu_fingertips = self.predict_fingertips_from_imu(hand_imu_data)

            calibration_frames.append((camera_fingertips, imu_fingertips, vertex))

        # ä¼˜åŒ–æ‰‹éƒ¨å˜æ¢
        hand_transform = self.optimize_hand_transform(calibration_frames)

        return hand_transform

    def process_frame(self, multi_view_images, body_imu_data, hand_imu_data,
                     body_transform, hand_transform):
        """å¤„ç†å•å¸§æ•°æ®"""
        # 1. ä»å¤šè§†è§’å›¾åƒä¸­æå–3Dæ ‡è®°å’Œç‰©ä½“
        markers_3d, objects_3d = self.reconstruct_3d_from_multiview(multi_view_images)

        # 2. ä¸ºäººç±»å§¿æ€èåˆIMUå’Œç›¸æœºæ•°æ®
        human_pose = self.fuse_imu_camera_data(
            body_imu_data, hand_imu_data, markers_3d,
            body_transform, hand_transform
        )

        # 3. è·Ÿè¸ªå’Œå‚æ•°åŒ–ç‰©ä½“
        object_params = self.track_objects(objects_3d)

        # 4. åˆ›å»ºå‚æ•°åŒ–è¡¨ç¤º
        frame_data = {
            'human_pose': human_pose,  # SMPL-Hå‚æ•°
            'object_params': object_params,  # åˆšæ€§ + å…³èŠ‚å‚æ•°
            'markers_3d': markers_3d,  # 3Dæ ‡è®°ä½ç½®
            'timestamp': self.get_timestamp()
        }

        return frame_data

    def reconstruct_3d_from_multiview(self, multi_view_images):
        """ä»å¤šè§†è§’å›¾åƒé‡å»º3Dåœºæ™¯"""
        markers_3d = []
        objects_3d = []

        # ä»æ¯ä¸ªè§†å›¾æå–2Dæ£€æµ‹
        all_detections = []
        for view_idx, image in enumerate(multi_view_images):
            detections = self.detect_markers_and_objects(image, view_idx)
            all_detections.append(detections)

        # ä¸‰è§’æµ‹é‡3Dä½ç½®
        for marker_id in range(self.get_total_markers()):
            marker_2d_points = []
            camera_indices = []

            for view_idx, detections in enumerate(all_detections):
                if marker_id in detections['markers']:
                    marker_2d_points.append(detections['markers'][marker_id])
                    camera_indices.append(view_idx)

            if len(marker_2d_points) >= 2:  # éœ€è¦è‡³å°‘2ä¸ªè§†å›¾
                marker_3d = self.triangulate_point(marker_2d_points, camera_indices)
                markers_3d.append(marker_3d)

        # ç±»ä¼¼è¿‡ç¨‹ç”¨äºç‰©ä½“
        for obj_id in self.object_ids:
            obj_2d_detections = []
            for view_idx, detections in enumerate(all_detections):
                if obj_id in detections['objects']:
                    obj_2d_detections.append((detections['objects'][obj_id], view_idx))

            if len(obj_2d_detections) >= 2:
                obj_3d = self.reconstruct_object_3d(obj_2d_detections)
                objects_3d.append(obj_3d)

        return markers_3d, objects_3d

    def fuse_imu_camera_data(self, body_imu, hand_imu, markers_3d,
                           body_transform, hand_transform):
        """èåˆIMUé¢„æµ‹å’Œç›¸æœºè§‚æµ‹"""
        # åº”ç”¨å˜æ¢åˆ°IMUæ•°æ®
        body_imu_transformed = self.apply_transform(body_imu, body_transform)
        hand_imu_transformed = self.apply_transform(hand_imu, hand_transform)

        # ä½¿ç”¨æ ‡è®°è¿›è¡Œç»†åŒ–
        refined_pose = self.refine_pose_with_markers(
            body_imu_transformed, hand_imu_transformed, markers_3d
        )

        return refined_pose

    def track_objects(self, objects_3d):
        """è·Ÿè¸ªå’Œå‚æ•°åŒ–3Dç‰©ä½“"""
        object_params = {}

        for obj_3d in objects_3d:
            obj_id = obj_3d['id']

            # åˆšæ€§å˜æ¢
            rigid_transform = self.compute_rigid_transform(obj_3d)

            # å…³èŠ‚å‚æ•°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            articulated_params = self.compute_articulated_params(obj_3d)

            object_params[obj_id] = {
                'rigid': rigid_transform,  # [R, t, scale]
                'articulated': articulated_params,  # å…³èŠ‚è§’åº¦/çŠ¶æ€
                'type': obj_3d['type']  # æ¤…å­ã€ç¬”è®°æœ¬ç”µè„‘ç­‰
            }

        return object_params

    def optimize_body_transform(self, calibration_frames):
        """ä¼˜åŒ–IMUå’Œç›¸æœºç©ºé—´ä¹‹é—´çš„å˜æ¢"""
        def objective(transform):
            total_error = 0
            for camera_markers, imu_markers in calibration_frames:
                # å°†IMUæ ‡è®°å˜æ¢åˆ°ç›¸æœºç©ºé—´
                transformed_imu = self.apply_transform_to_points(imu_markers, transform)

                # è®¡ç®—å¯¹é½è¯¯å·®
                error = np.mean(np.linalg.norm(camera_markers - transformed_imu, axis=1))
                total_error += error

            return total_error

        # ä½¿ç”¨åŸºäºæ¢¯åº¦çš„æ–¹æ³•ä¼˜åŒ–
        from scipy.optimize import minimize
        initial_transform = np.eye(4)  # åˆå§‹åˆšæ€§å˜æ¢
        result = minimize(objective, initial_transform.flatten(),
                         method='L-BFGS-B')

        return result.x.reshape(4, 4)

    def optimize_hand_transform(self, calibration_frames):
        """ä½¿ç”¨æ ¡å‡†åè®®ä¼˜åŒ–æ‰‹éƒ¨å˜æ¢"""
        def objective(transform):
            total_error = 0
            for camera_fingertips, imu_fingertips, target_point in calibration_frames:
                # å°†IMUæŒ‡å°–å˜æ¢åˆ°ç›¸æœºç©ºé—´
                transformed_imu = self.apply_transform_to_points(imu_fingertips, transform)

                # å˜æ¢åæŒ‡å°–ä¸ç›®æ ‡ç‚¹ä¹‹é—´çš„è¯¯å·®
                error = np.mean(np.linalg.norm(camera_fingertips - target_point, axis=1))
                total_error += error

            return total_error

        # ä¸èº«ä½“å˜æ¢ç±»ä¼¼çš„ä¼˜åŒ–
        initial_transform = np.eye(4)
        result = minimize(objective, initial_transform.flatten(),
                         method='L-BFGS-B')

        return result.x.reshape(4, 4)

    def postprocess_sequence(self, frames_data):
        """åå¤„ç†æ•è·åºåˆ—"""
        # æ—¶é—´å¹³æ»‘
        smoothed_frames = self.temporal_smooth(frames_data)

        # ç‰©ç†çº¦æŸæ£€æŸ¥
        validated_frames = self.validate_physics_constraints(smoothed_frames)

        # åˆ›å»ºæœ€ç»ˆHOIæ•°æ®é›†æ¡ç›®
        hoi_sequence = {
            'participant_id': self.current_participant,
            'activity_type': self.current_activity,
            'frames': validated_frames,
            'metadata': {
                'duration': len(frames_data) / 30.0,  # å‡è®¾30fps
                'objects_present': list(set([obj for frame in frames_data
                                           for obj in frame['object_params'].keys()])),
                'quality_score': self.compute_sequence_quality(validated_frames)
            }
        }

        return hoi_sequence

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–ParaHomeç³»ç»Ÿ
    parahome = ParaHomeCaptureSystem(num_cameras=70)

    # æ•è·HOIåºåˆ—
    activities = ["making_coffee", "setting_table", "folding_laundry"]

    for participant_id in range(1, 39):  # 38ä¸ªå‚ä¸è€…
        for activity in activities:
            print(f"æ•è·å‚ä¸è€… {participant_id}, æ´»åŠ¨: {activity}")

            # æ‰§è¡Œç³»ç»Ÿæ ¡å‡†
            body_transform = parahome.calibrate_body_alignment()
            hand_transform = parahome.calibrate_hand_alignment()

            # æ•è·åºåˆ—
            hoi_sequence = parahome.capture_hoi_sequence(participant_id, activity)

            # ä¿å­˜åˆ°æ•°æ®é›†
            save_to_dataset(hoi_sequence)

    print("ParaHomeæ•è·å®Œæˆ!")</code></pre>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://jlogkim.github.io/parahome" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Project Page
        </a>
        <a href="https://github.com/jlogkim/parahome" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          GitHub Repo
        </a>
      </div>
    </div>
</section>
</body>
</html>
