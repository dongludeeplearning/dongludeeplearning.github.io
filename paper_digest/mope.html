<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ACL â€¢  LREC1-COLING 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu<br>
        <span style="opacity:0.8">Peking University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we develop few-shot learning approaches for multi-modal semantic understanding tasks like sarcasm detection and sentiment analysis using unified vision-language models with efficient soft prompt frameworks?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•å¼€å‘å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¤šæ¨¡æ€è¯­ä¹‰ç†è§£ä»»åŠ¡å¦‚è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æï¼Œä½¿ç”¨ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹å’Œé«˜æ•ˆçš„è½¯æç¤ºæ¡†æ¶ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Mixture-of-Prompt-Experts Framework:</b> Introduced MoPE-BAF, a novel multi-modal soft prompt framework with three specialized prompt experts (text, image, and unified) that extract modality-specific features and facilitate cross-modal interaction in unified VLMs.</div>
            <div class="lang-zh" style="display:none"><b>æ··åˆæç¤ºä¸“å®¶æ¡†æ¶ï¼š</b>å¼•å…¥äº†MoPE-BAFï¼Œä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è½¯æç¤ºæ¡†æ¶ï¼Œå…·æœ‰ä¸‰ä¸ªä¸“é—¨çš„æç¤ºä¸“å®¶ï¼ˆæ–‡æœ¬ã€å›¾åƒå’Œç»Ÿä¸€ï¼‰ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„VLMä¸­æå–æ¨¡æ€ç‰¹å®šç‰¹å¾å¹¶ä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Block-Aware Prompt Fusion:</b> Developed a block-aware prompt fusion mechanism that reorganizes Transformer layers into blocks and applies cross-modal prompt attention between adjacent blocks, enabling smooth transitions from single-modal to multi-modal fusion.</div>
            <div class="lang-zh" style="display:none"><b>å—æ„ŸçŸ¥æç¤ºèåˆï¼š</b>å¼€å‘äº†å—æ„ŸçŸ¥æç¤ºèåˆæœºåˆ¶ï¼Œå°†Transformerå±‚é‡ç»„ä¸ºå—ï¼Œå¹¶åœ¨ç›¸é‚»å—ä¹‹é—´åº”ç”¨è·¨æ¨¡æ€æç¤ºæ³¨æ„åŠ›ï¼Œå®ç°ä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€èåˆçš„å¹³æ»‘è¿‡æ¸¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Superior Few-Shot Performance:</b> Achieved state-of-the-art performance on few-shot multi-modal sarcasm detection and sentiment analysis, surpassing 8.2B parameter InstructBLIP with only 150M parameters (2% of the size) and outperforming task-specific methods.</div>
            <div class="lang-zh" style="display:none"><b>å“è¶Šçš„å°‘æ ·æœ¬æ€§èƒ½ï¼š</b>åœ¨å°‘æ ·æœ¬å¤šæ¨¡æ€è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»¥ä»…150Må‚æ•°ï¼ˆè§„æ¨¡çš„2%ï¼‰è¶…è¶Š82Bå‚æ•°çš„InstructBLIPï¼Œå¹¶ä¼˜äºä»»åŠ¡ç‰¹å®šæ–¹æ³•ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Efficient Parameter Utilization:</b> Demonstrated that specialized prompt experts with cross-modal attention can achieve superior performance compared to general prompts, showing the effectiveness of mixture-of-experts approach in multi-modal prompt learning.</div>
            <div class="lang-zh" style="display:none"><b>é«˜æ•ˆå‚æ•°åˆ©ç”¨ï¼š</b>è¯æ˜äº†å…·æœ‰è·¨æ¨¡æ€æ³¨æ„åŠ›çš„ä¸“é—¨æç¤ºä¸“å®¶å¯ä»¥å®ç°ä¼˜äºé€šç”¨æç¤ºçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†ä¸“å®¶æ··åˆæ–¹æ³•åœ¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Few-Shot Multi-Modal Learning:</b> Collecting and annotating high-quality multi-modal data is challenging, especially for nuanced tasks like sarcasm detection, requiring effective few-shot learning approaches that can generalize from limited examples.</div>
            <div class="lang-zh" style="display:none"><b>å°‘æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ ï¼š</b>æ”¶é›†å’Œæ ‡æ³¨é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒè®½åˆºæ£€æµ‹è¿™æ ·çš„ç»†å¾®ä»»åŠ¡ï¼Œéœ€è¦æœ‰æ•ˆçš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æœ‰é™ç¤ºä¾‹ä¸­æ³›åŒ–ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Modality Alignment and Fusion:</b> Different modalities (text and image) have distinct feature spaces and semantic representations, requiring sophisticated fusion mechanisms to capture cross-modal incongruities and interactions.</div>
            <div class="lang-zh" style="display:none"><b>æ¨¡æ€å¯¹é½å’Œèåˆï¼š</b>ä¸åŒæ¨¡æ€ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰å…·æœ‰ä¸åŒçš„ç‰¹å¾ç©ºé—´å’Œè¯­ä¹‰è¡¨ç¤ºï¼Œéœ€è¦å¤æ‚çš„èåˆæœºåˆ¶æ¥æ•æ‰è·¨æ¨¡æ€çš„ä¸ä¸€è‡´æ€§å’Œäº¤äº’ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Unified VLM Adaptation:</b> Adapting unified vision-language models to downstream multi-modal tasks while maintaining their pre-trained cross-modal alignments and avoiding catastrophic forgetting of general capabilities.</div>
            <div class="lang-zh" style="display:none"><b>ç»Ÿä¸€VLMé€‚åº”ï¼š</b>å°†ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€‚åº”ä¸‹æ¸¸å¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒå…¶é¢„è®­ç»ƒçš„è·¨æ¨¡æ€å¯¹é½å¹¶é¿å…ç¾éš¾æ€§é—å¿˜ä¸€èˆ¬èƒ½åŠ›ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Prompt Expert Coordination:</b> Designing and coordinating multiple prompt experts to specialize in different modalities while enabling effective information exchange and fusion across the entire model architecture.</div>
            <div class="lang-zh" style="display:none"><b>æç¤ºä¸“å®¶åè°ƒï¼š</b>è®¾è®¡å’Œåè°ƒå¤šä¸ªæç¤ºä¸“å®¶ä»¥ä¸“é—¨å¤„ç†ä¸åŒæ¨¡æ€ï¼ŒåŒæ—¶å®ç°æ•´ä¸ªæ¨¡å‹æ¶æ„ä¸­çš„æœ‰æ•ˆä¿¡æ¯äº¤æ¢å’Œèåˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Block-Level Interaction:</b> Enabling smooth transitions between single-modal processing and multi-modal fusion across different layers/blocks of Transformer architectures while maintaining computational efficiency.</div>
            <div class="lang-zh" style="display:none"><b>å—çº§äº¤äº’ï¼š</b>å®ç°Transformeræ¶æ„ä¸åŒå±‚/å—ä¹‹é—´çš„å•æ¨¡æ€å¤„ç†å’Œå¤šæ¨¡æ€èåˆçš„å¹³æ»‘è¿‡æ¸¡ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>MoPE-BAF addresses few-shot multi-modal semantic understanding through specialized prompt experts and block-aware fusion:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Mixture-of-Prompt-Experts:</b> Three soft prompt experts - text prompt for textual semantic extraction, image prompt for visual feature enrichment, and unified prompt for cross-modal interaction within a unified VLM architecture.</li>
                <li style="margin-bottom:6px;"><b>Block-Aware Prompt Fusion:</b> Transformer layers reorganized into blocks with cross-modal prompt attention applied between adjacent blocks, enabling progressive fusion from single-modal specialization to multi-modal understanding.</li>
                <li style="margin-bottom:6px;"><b>Modality-Specific Feature Extraction:</b> Text and image prompts extract modality-specific semantic features to enrich single-modal representations before cross-modal fusion.</li>
                <li style="margin-bottom:6px;"><b>Unified Prompt Integration:</b> Unified prompt expert facilitates inter-modality information exchange and assists in capturing complex cross-modal relationships for sarcasm and sentiment detection.</li>
                <li style="margin-bottom:6px;"><b>Efficient Parameter Learning:</b> Soft prompts trained with minimal parameters while achieving superior performance compared to large-scale fine-tuned models in few-shot settings.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>MoPE-BAFé€šè¿‡ä¸“é—¨çš„æç¤ºä¸“å®¶å’Œå—æ„ŸçŸ¥èåˆæ¥è§£å†³å°‘æ ·æœ¬å¤šæ¨¡æ€è¯­ä¹‰ç†è§£ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ··åˆæç¤ºä¸“å®¶ï¼š</b>ä¸‰ä¸ªè½¯æç¤ºä¸“å®¶ - æ–‡æœ¬æç¤ºç”¨äºæ–‡æœ¬è¯­ä¹‰æå–ï¼Œå›¾åƒæç¤ºç”¨äºè§†è§‰ç‰¹å¾ä¸°å¯Œï¼Œä»¥åŠç»Ÿä¸€æç¤ºç”¨äºç»Ÿä¸€VLMæ¶æ„å†…çš„è·¨æ¨¡æ€äº¤äº’ã€‚</li>
                <li style="margin-bottom:6px;"><b>å—æ„ŸçŸ¥æç¤ºèåˆï¼š</b>Transformerå±‚é‡ç»„ä¸ºå—ï¼Œåœ¨ç›¸é‚»å—ä¹‹é—´åº”ç”¨è·¨æ¨¡æ€æç¤ºæ³¨æ„åŠ›ï¼Œå®ç°ä»å•æ¨¡æ€ä¸“ä¸šåŒ–åˆ°å¤šæ¨¡æ€ç†è§£çš„æ¸è¿›èåˆã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¨¡æ€ç‰¹å®šç‰¹å¾æå–ï¼š</b>æ–‡æœ¬å’Œå›¾åƒæç¤ºæå–æ¨¡æ€ç‰¹å®šè¯­ä¹‰ç‰¹å¾ï¼Œä»¥ä¸°å¯Œè·¨æ¨¡æ€èåˆä¹‹å‰çš„å•æ¨¡æ€è¡¨ç¤ºã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»Ÿä¸€æç¤ºé›†æˆï¼š</b>ç»Ÿä¸€æç¤ºä¸“å®¶ä¿ƒè¿›æ¨¡æ€é—´ä¿¡æ¯äº¤æ¢ï¼Œå¹¶å¸®åŠ©æ•æ‰è®½åˆºå’Œæƒ…æ„Ÿæ£€æµ‹çš„å¤æ‚è·¨æ¨¡æ€å…³ç³»ã€‚</li>
                <li style="margin-bottom:6px;"><b>é«˜æ•ˆå‚æ•°å­¦ä¹ ï¼š</b>è½¯æç¤ºä»¥æœ€å°‘çš„å‚æ•°è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­å®ç°ä¼˜äºå¤§è§„æ¨¡å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/mope_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p>MoPE-BAF framework with three prompt experts (text, image, unified) and block-aware prompt fusion for multi-modal semantic understanding in few-shot settings.</p>
            <p>Note: The framework achieves superior performance on MSD and MSA tasks, surpassing large-scale models with minimal parameters through efficient prompt learning.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_mope.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_mope.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of MoPE-BAF stems from principled design choices that optimize multi-modal prompt learning:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Modality-Specific Specialization:</b> Separate prompt experts for text and image modalities allow each to focus on extracting rich, modality-specific semantic features before fusion, preventing interference and information loss.</li>
                <li style="margin-bottom:6px;"><b>Unified Architecture Exploitation:</b> Leveraging unified VLMs instead of dual-encoder architectures maintains pre-trained cross-modal alignments while enabling fine-grained prompt control over different processing stages.</li>
                <li style="margin-bottom:6px;"><b>Progressive Fusion Strategy:</b> Block-aware prompt fusion ensures smooth transitions from single-modal processing to multi-modal interaction, allowing the model to build complex cross-modal understanding incrementally.</li>
                <li style="margin-bottom:6px;"><b>Parameter Efficiency:</b> Soft prompts with minimal trainable parameters (150M vs 8.2B) demonstrate that intelligent prompt design can achieve superior performance compared to full model fine-tuning in few-shot scenarios.</li>
                <li style="margin-bottom:6px;"><b>Task-Specific Adaptation:</b> Mixture-of-experts approach enables the framework to adapt to different multi-modal understanding tasks (sarcasm vs sentiment) by learning appropriate expert specializations and fusion strategies.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>MoPE-BAFçš„æˆåŠŸæºäºä¼˜åŒ–å¤šæ¨¡æ€æç¤ºå­¦ä¹ çš„åŸºæœ¬è®¾è®¡é€‰æ‹©ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ¨¡æ€ç‰¹å®šä¸“ä¸šåŒ–ï¼š</b>æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„å•ç‹¬æç¤ºä¸“å®¶å…è®¸æ¯ä¸ªä¸“æ³¨äºæå–ä¸°å¯Œçš„æ¨¡æ€ç‰¹å®šè¯­ä¹‰ç‰¹å¾ï¼Œç„¶åè¿›è¡Œèåˆï¼Œé˜²æ­¢å¹²æ‰°å’Œä¿¡æ¯ä¸¢å¤±ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»Ÿä¸€æ¶æ„åˆ©ç”¨ï¼š</b>åˆ©ç”¨ç»Ÿä¸€çš„VLMè€Œä¸æ˜¯åŒç¼–ç å™¨æ¶æ„ï¼Œåœ¨å¯ç”¨ä¸åŒå¤„ç†é˜¶æ®µçš„ç»†ç²’åº¦æç¤ºæ§åˆ¶çš„åŒæ—¶ä¿æŒé¢„è®­ç»ƒçš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¸è¿›èåˆç­–ç•¥ï¼š</b>å—æ„ŸçŸ¥æç¤ºèåˆç¡®ä¿ä»å•æ¨¡æ€å¤„ç†åˆ°å¤šæ¨¡æ€äº¤äº’çš„å¹³æ»‘è¿‡æ¸¡ï¼Œå…è®¸æ¨¡å‹é€æ­¥æ„å»ºå¤æ‚çš„è·¨æ¨¡æ€ç†è§£ã€‚</li>
                <li style="margin-bottom:6px;"><b>å‚æ•°æ•ˆç‡ï¼š</b>å…·æœ‰æœ€å°å¯è®­ç»ƒå‚æ•°ï¼ˆ150M vs 82Bï¼‰çš„è½¯æç¤ºè¯æ˜ï¼Œæ™ºèƒ½æç¤ºè®¾è®¡å¯ä»¥åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­å®ç°ä¼˜äºå®Œæ•´æ¨¡å‹å¾®è°ƒçš„æ€§èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»»åŠ¡ç‰¹å®šé€‚åº”ï¼š</b>ä¸“å®¶æ··åˆæ–¹æ³•é€šè¿‡å­¦ä¹ é€‚å½“çš„ä¸“å®¶ä¸“ä¸šåŒ–å’Œèåˆç­–ç•¥ï¼Œä½¿æ¡†æ¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ï¼ˆè®½åˆºvsæƒ…æ„Ÿï¼‰ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This pioneering work establishes MoPE-BAF as a breakthrough in few-shot multi-modal semantic understanding, demonstrating that carefully designed prompt mixtures can achieve remarkable performance on complex tasks like sarcasm detection and sentiment analysis. By introducing specialized prompt experts for different modalities and enabling progressive fusion through block-aware attention, the framework achieves state-of-the-art results while using only 2% of the parameters of competing large-scale models. The work addresses fundamental challenges in multi-modal learning, including modality alignment, few-shot adaptation, and efficient parameter utilization. Experimental results on MSD and MSA datasets showcase consistent superiority over both VLM-based prompt methods and task-specific approaches, validating the effectiveness of the mixture-of-experts paradigm in prompt learning. The framework's ability to surpass 8.2B parameter InstructBLIP with a mere 150M parameters highlights the potential of intelligent prompt design to democratize access to advanced multi-modal capabilities. By maintaining the general capabilities of pre-trained VLMs while enabling task-specific adaptation, MoPE-BAF represents a significant advancement toward practical few-shot multi-modal understanding systems. The work opens new research directions in prompt-based multi-modal learning and provides a blueprint for developing efficient, scalable solutions to complex semantic understanding tasks.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å¼€åˆ›æ€§å·¥ä½œå°†MoPE-BAFç¡®ç«‹ä¸ºå°‘æ ·æœ¬å¤šæ¨¡æ€è¯­ä¹‰ç†è§£çš„çªç ´ï¼Œè¯æ˜ç²¾å¿ƒè®¾è®¡çš„æç¤ºæ··åˆå¯ä»¥åœ¨å¤æ‚ä»»åŠ¡å¦‚è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚é€šè¿‡ä¸ºä¸åŒæ¨¡æ€å¼•å…¥ä¸“é—¨çš„æç¤ºä¸“å®¶å¹¶é€šè¿‡å—æ„ŸçŸ¥æ³¨æ„åŠ›å¯ç”¨æ¸è¿›èåˆï¼Œè¯¥æ¡†æ¶åœ¨ä½¿ç”¨ä»…å ç«äº‰å¤§å‹æ¨¡å‹2%å‚æ•°çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¿™é¡¹å·¥ä½œè§£å†³äº†å¤šæ¨¡æ€å­¦ä¹ çš„åŸºæœ¬æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡æ€å¯¹é½ã€å°‘æ ·æœ¬é€‚åº”å’Œé«˜æ•ˆå‚æ•°åˆ©ç”¨ã€‚MSDå’ŒMSAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå±•ç¤ºäº†å…¶åœ¨åŸºäºVLMçš„æç¤ºæ–¹æ³•å’Œä»»åŠ¡ç‰¹å®šæ–¹æ³•æ–¹é¢çš„æŒç»­ä¼˜åŠ¿ï¼ŒéªŒè¯äº†ä¸“å®¶æ··åˆèŒƒå¼åœ¨æç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶ä»¥ä»…150Må‚æ•°è¶…è¶Š82Bå‚æ•°InstructBLIPçš„èƒ½åŠ›çªæ˜¾äº†æ™ºèƒ½æç¤ºè®¾è®¡æ°‘ä¸»åŒ–è®¿é—®é«˜çº§å¤šæ¨¡æ€èƒ½åŠ›çš„æ½œåŠ›ã€‚é€šè¿‡ä¿æŒé¢„è®­ç»ƒVLMçš„ä¸€èˆ¬èƒ½åŠ›åŒæ—¶å¯ç”¨ä»»åŠ¡ç‰¹å®šé€‚åº”ï¼ŒMoPE-BAFä»£è¡¨äº†æœç€å®ç”¨å°‘æ ·æœ¬å¤šæ¨¡æ€ç†è§£ç³»ç»Ÿçš„é‡è¦è¿›æ­¥ã€‚è¿™é¡¹å·¥ä½œä¸ºåŸºäºæç¤ºçš„å¤šæ¨¡æ€å­¦ä¹ å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå¹¶ä¸ºå¼€å‘é«˜æ•ˆã€å¯æ‰©å±•çš„å¤æ‚è¯­ä¹‰ç†è§£ä»»åŠ¡è§£å†³æ–¹æ¡ˆæä¾›äº†è“å›¾ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2403.11311" target="_blank" style="color:#8bffcf;">2403.11311</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# MoPE-BAF Implementation<br>
<br>
// Mixture-of-Prompt-Experts Framework<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">MixtureOfPromptExperts</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""MoPE-BAF: Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, vlm_model, num_blocks=4):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.vlm = vlm_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_blocks = num_blocks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Three prompt experts: text, image, unified</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.text_prompt = nn.Parameter(torch.randn(1, prompt_length, hidden_dim))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.image_prompt = nn.Parameter(torch.randn(1, prompt_length, hidden_dim))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.unified_prompt = nn.Parameter(torch.randn(1, prompt_length, hidden_dim))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Cross-modal attention for block fusion</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">organize_into_blocks</span>(self, layers):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Reorganize Transformer layers into blocks for fusion"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block_size = len(layers) // self.num_blocks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;blocks = [layers[i:i+block_size] <span style="color:#569cd6;">for</span> i <span style="color:#569cd6;">in</span> range(0, len(layers), block_size)]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> blocks<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">forward</span>(self, text_input, image_input):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Forward pass with prompt experts and block fusion"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Initial processing with modality-specific prompts</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_features = self.process_text_with_prompt(text_input, self.text_prompt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_features = self.process_image_with_prompt(image_input, self.image_prompt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Organize VLM layers into blocks</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;blocks = self.organize_into_blocks(self.vlm.layers)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Process through blocks with cross-modal fusion</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined_features = torch.cat([text_features, image_features], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> i, block <span style="color:#569cd6;">in</span> enumerate(blocks):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Apply unified prompt for cross-modal interaction</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unified_features = self.apply_unified_prompt(combined_features, self.unified_prompt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Process through current block</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined_features = self.process_block(block, unified_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Cross-modal attention between adjacent blocks</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">if</span> i < len(blocks) - 1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined_features = self.cross_modal_attention(combined_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> self.classification_head(combined_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">process_text_with_prompt</span>(self, text_input, prompt):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Extract text features with text prompt expert"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_with_prompt = torch.cat([prompt, text_input], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> self.vlm.text_encoder(text_with_prompt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">process_image_with_prompt</span>(self, image_input, prompt):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Extract image features with image prompt expert"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_with_prompt = torch.cat([prompt, image_input], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> self.vlm.image_encoder(image_with_prompt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">apply_unified_prompt</span>(self, features, prompt):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Apply unified prompt for cross-modal interaction"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> torch.cat([prompt, features], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">cross_modal_attention</span>(self, features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Apply cross-modal attention between text and image features"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_features, image_features = torch.split(features, features.size(1)//2, dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attended_text, _ = self.cross_attention(text_features, image_features, image_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attended_image, _ = self.cross_attention(image_features, text_features, text_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> torch.cat([attended_text, attended_image], dim=1)<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
