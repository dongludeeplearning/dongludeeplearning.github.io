<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Yanchen Xu, Ziheng Jiao, Hongyuan Zhang, Xuelong Li<br>
        <span style="opacity:0.8">China Telecom, Northwestern Polytechnical University, Huawei, University of Hong Kong</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we generalize the GRPO algorithm from LLM fine-tuning to representation learning models, enabling reinforcement learning-based post-training for visual models like DINOv2?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•å°†GRPOç®—æ³•ä»LLMå¾®è°ƒæ¨å¹¿åˆ°è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œå®ç°åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰æ¨¡å‹ï¼ˆå¦‚DINOv2ï¼‰åè®­ç»ƒï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>GRPO for Representation Models:</b> First adaptation of GRPO algorithm from LLM fine-tuning to representation learning, establishing GRPO-RM as a novel reinforcement learning method for post-training visual models.</div>
            <div class="lang-zh" style="display:none"><b>è¡¨ç¤ºæ¨¡å‹çš„GRPOï¼š</b>é¦–æ¬¡å°†GRPOç®—æ³•ä»LLMå¾®è°ƒé€‚åº”åˆ°è¡¨ç¤ºå­¦ä¹ ï¼Œå»ºç«‹GRPO-RMä½œä¸ºè§†è§‰æ¨¡å‹åè®­ç»ƒçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Predefined Output Set:</b> Designed a predefined output set mechanism to functionally replace token sequence sampling in LLMs, enabling probability-driven GRPO optimization for representation models.</div>
            <div class="lang-zh" style="display:none"><b>é¢„å®šä¹‰è¾“å‡ºé›†ï¼š</b>è®¾è®¡äº†é¢„å®šä¹‰è¾“å‡ºé›†æœºåˆ¶æ¥åŠŸèƒ½æ€§æ›¿ä»£LLMä¸­çš„tokenåºåˆ—é‡‡æ ·ï¼Œå®ç°è¡¨ç¤ºæ¨¡å‹çš„æ¦‚ç‡é©±åŠ¨GRPOä¼˜åŒ–ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Specialized Reward Functions:</b> Developed accuracy rewards and uniformity rewards tailored for representation learning properties, encouraging correct predictions while adaptively discouraging incorrect ones.</div>
            <div class="lang-zh" style="display:none"><b>ä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼š</b>ä¸ºè¡¨ç¤ºå­¦ä¹ å±æ€§å¼€å‘äº†å‡†ç¡®ç‡å¥–åŠ±å’Œå‡åŒ€æ€§å¥–åŠ±ï¼Œé¼“åŠ±æ­£ç¡®é¢„æµ‹åŒæ—¶è‡ªé€‚åº”åœ°é˜»æ­¢é”™è¯¯é¢„æµ‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Superior Performance:</b> Achieved significant improvements over standard fine-tuning, with average 4.26% accuracy improvement on out-of-distribution datasets across image classification and semantic segmentation tasks.</div>
            <div class="lang-zh" style="display:none"><b>å“è¶Šæ€§èƒ½ï¼š</b>åœ¨æ ‡å‡†å¾®è°ƒåŸºç¡€ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œåœ¨åˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡4.26%çš„å‡†ç¡®ç‡æå‡ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>GRPO Generalization:</b> Adapting GRPO algorithm from sequential text generation in LLMs to visual representation learning, where outputs are feature embeddings rather than token sequences.</div>
            <div class="lang-zh" style="display:none"><b>GRPOæ³›åŒ–ï¼š</b>å°†GRPOç®—æ³•ä»LLMä¸­çš„åºåˆ—æ–‡æœ¬ç”Ÿæˆé€‚åº”åˆ°è§†è§‰è¡¨ç¤ºå­¦ä¹ ï¼Œå…¶ä¸­è¾“å‡ºæ˜¯ç‰¹å¾åµŒå…¥è€Œä¸æ˜¯tokenåºåˆ—ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Output Group Generation:</b> Creating a mechanism to generate output groups for representation models, since they don't naturally sample multiple outputs like LLMs do.</div>
            <div class="lang-zh" style="display:none"><b>è¾“å‡ºç»„ç”Ÿæˆï¼š</b>ä¸ºè¡¨ç¤ºæ¨¡å‹åˆ›å»ºç”Ÿæˆè¾“å‡ºç»„çš„æœºåˆ¶ï¼Œå› ä¸ºå®ƒä»¬ä¸åƒLLMé‚£æ ·è‡ªç„¶åœ°é‡‡æ ·å¤šä¸ªè¾“å‡ºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Reward Function Design:</b> Designing reward functions that capture the properties of representation learning, including both accuracy incentives and regularization terms for feature quality.</div>
            <div class="lang-zh" style="display:none"><b>å¥–åŠ±å‡½æ•°è®¾è®¡ï¼š</b>è®¾è®¡æ•æ‰è¡¨ç¤ºå­¦ä¹ å±æ€§çš„å¥–åŠ±å‡½æ•°ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡æ¿€åŠ±å’Œç‰¹å¾è´¨é‡çš„æ­£åˆ™åŒ–é¡¹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Probability-Driven Optimization:</b> Enabling probability-based optimization for representation models that typically use deterministic forward passes rather than probabilistic sampling.</div>
            <div class="lang-zh" style="display:none"><b>æ¦‚ç‡é©±åŠ¨ä¼˜åŒ–ï¼š</b>ä¸ºé€šå¸¸ä½¿ç”¨ç¡®å®šæ€§å‰å‘ä¼ é€’è€Œä¸æ˜¯æ¦‚ç‡é‡‡æ ·çš„è¡¨ç¤ºæ¨¡å‹å¯ç”¨åŸºäºæ¦‚ç‡çš„ä¼˜åŒ–ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Task-Specific Adaptation:</b> Adapting GRPO-RM to different downstream tasks like image classification and semantic segmentation, each requiring different output formats and evaluation metrics.</div>
            <div class="lang-zh" style="display:none"><b>ä»»åŠ¡ç‰¹å®šé€‚åº”ï¼š</b>å°†GRPO-RMé€‚åº”åˆ°ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ï¼Œæ¯ä¸ªéƒ½éœ€è¦ä¸åŒçš„è¾“å‡ºæ ¼å¼å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>GRPO-RM extends the GRPO algorithm from LLM fine-tuning to representation learning models:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Predefined Output Set:</b> Establishes a fixed set of possible outputs to replace token sequence sampling, enabling group-based optimization for deterministic representation models.</li>
                <li style="margin-bottom:6px;"><b>Specialized Reward Functions:</b> Combines accuracy rewards (for correct predictions) with uniformity rewards (to discourage incorrect predictions), tailored for representation learning properties.</li>
                <li style="margin-bottom:6px;"><b>Group Relative Optimization:</b> Applies GRPO's group-relative advantage computation to representation models, using predefined output groups instead of sampled token sequences.</li>
                <li style="margin-bottom:6px;"><b>DINOv2 Post-Training:</b> Post-trains DINOv2 model using GRPO-RM for image classification and semantic segmentation tasks, leveraging both class tokens and patch tokens.</li>
                <li style="margin-bottom:6px;"><b>Task-Specific Adaptation:</b> Adapts the framework for different downstream tasks, using appropriate output formats and reward calculations for each task type.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>GRPO-RMå°†GRPOç®—æ³•ä»LLMå¾®è°ƒæ‰©å±•åˆ°è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>é¢„å®šä¹‰è¾“å‡ºé›†ï¼š</b>å»ºç«‹å›ºå®šçš„å¯èƒ½è¾“å‡ºé›†æ¥æ›¿ä»£tokenåºåˆ—é‡‡æ ·ï¼Œä¸ºç¡®å®šæ€§è¡¨ç¤ºæ¨¡å‹å¯ç”¨åŸºäºç»„çš„ä¼˜åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼š</b>å°†å‡†ç¡®ç‡å¥–åŠ±ï¼ˆæ­£ç¡®é¢„æµ‹ï¼‰å’Œå‡åŒ€æ€§å¥–åŠ±ï¼ˆé˜»æ­¢é”™è¯¯é¢„æµ‹ï¼‰ç›¸ç»“åˆï¼Œä¸ºè¡¨ç¤ºå­¦ä¹ å±æ€§é‡èº«å®šåˆ¶ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»„ç›¸å¯¹ä¼˜åŒ–ï¼š</b>å°†GRPOçš„ç»„ç›¸å¯¹ä¼˜åŠ¿è®¡ç®—åº”ç”¨äºè¡¨ç¤ºæ¨¡å‹ï¼Œä½¿ç”¨é¢„å®šä¹‰è¾“å‡ºç»„è€Œä¸æ˜¯é‡‡æ ·çš„tokenåºåˆ—ã€‚</li>
                <li style="margin-bottom:6px;"><b>DINOv2åè®­ç»ƒï¼š</b>ä½¿ç”¨GRPO-RMå¯¹DINOv2æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œé’ˆå¯¹å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼ŒåŒæ—¶åˆ©ç”¨ç±»tokenå’Œpatch tokenã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»»åŠ¡ç‰¹å®šé€‚åº”ï¼š</b>ä¸ºä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è°ƒæ•´æ¡†æ¶ï¼Œä¸ºæ¯ç§ä»»åŠ¡ç±»å‹ä½¿ç”¨é€‚å½“çš„è¾“å‡ºæ ¼å¼å’Œå¥–åŠ±è®¡ç®—ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆæ€»ä½“ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/grpo_rm_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p>GRPO-RM framework showing the adaptation of GRPO algorithm for representation models, with predefined output sets and specialized reward functions for post-training DINOv2.</p>
            <p>Note: The method achieves significant improvements over standard fine-tuning, demonstrating that GRPO can be successfully generalized from LLMs to visual representation learning.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_grpo_rm.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_grpo_rm.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of GRPO-RM lies in its principled adaptation of reinforcement learning principles to representation learning:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Architectural Similarities:</b> Leveraging the Transformer architecture common to both LLMs and ViT-based representation models like DINOv2, enabling the transfer of RL optimization techniques.</li>
                <li style="margin-bottom:6px;"><b>Predefined Output Groups:</b> The innovative use of predefined output sets creates the necessary probabilistic framework for GRPO optimization in deterministic representation models.</li>
                <li style="margin-bottom:6px;"><b>Task-Aligned Rewards:</b> Specialized reward functions that capture both accuracy incentives and feature quality regularization, directly addressing the unique properties of visual representation learning.</li>
                <li style="margin-bottom:6px;"><b>Group Relative Advantages:</b> GRPO's group-based advantage computation provides stable optimization without requiring complex value function estimation, making it suitable for representation learning.</li>
                <li style="margin-bottom:6px;"><b>Out-of-Distribution Generalization:</b> The RL approach enables better generalization to unseen data distributions, as evidenced by the 4.26% average accuracy improvement on out-of-distribution datasets.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>GRPO-RMçš„æˆåŠŸåœ¨äºå…¶å°†å¼ºåŒ–å­¦ä¹ åŸç†åŸåˆ™æ€§é€‚åº”åˆ°è¡¨ç¤ºå­¦ä¹ çš„åšæ³•ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ¶æ„ç›¸ä¼¼æ€§ï¼š</b>åˆ©ç”¨LLMå’ŒåŸºäºViTçš„è¡¨ç¤ºæ¨¡å‹ï¼ˆå¦‚DINOv2ï¼‰å…±æœ‰çš„Transformeræ¶æ„ï¼Œå®ç°RLä¼˜åŒ–æŠ€æœ¯çš„è½¬ç§»ã€‚</li>
                <li style="margin-bottom:6px;"><b>é¢„å®šä¹‰è¾“å‡ºç»„ï¼š</b>é¢„å®šä¹‰è¾“å‡ºé›†çš„åˆ›æ–°ä½¿ç”¨ä¸ºç¡®å®šæ€§è¡¨ç¤ºæ¨¡å‹ä¸­çš„GRPOä¼˜åŒ–åˆ›å»ºäº†å¿…è¦çš„æ¦‚ç‡æ¡†æ¶ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»»åŠ¡å¯¹é½å¥–åŠ±ï¼š</b>ä¸“é—¨çš„å¥–åŠ±å‡½æ•°æ•æ‰å‡†ç¡®ç‡æ¿€åŠ±å’Œç‰¹å¾è´¨é‡æ­£åˆ™åŒ–ï¼Œç›´æ¥è§£å†³è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„ç‹¬ç‰¹å±æ€§ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼š</b>GRPOçš„åŸºäºç»„çš„ä¼˜åŠ¿è®¡ç®—æä¾›äº†ç¨³å®šçš„ä¼˜åŒ–ï¼Œè€Œä¸éœ€è¦å¤æ‚çš„ä»·å€¼å‡½æ•°ä¼°è®¡ï¼Œä½¿å…¶é€‚åˆè¡¨ç¤ºå­¦ä¹ ã€‚</li>
                <li style="margin-bottom:6px;"><b>åˆ†å¸ƒå¤–æ³›åŒ–ï¼š</b>RLæ–¹æ³•å®ç°äº†å¯¹æœªè§æ•°æ®åˆ†å¸ƒçš„æ›´å¥½æ³›åŒ–ï¼Œæ­£å¦‚åœ¨åˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šå¹³å‡4.26%çš„å‡†ç¡®ç‡æå‡æ‰€è¯æ˜çš„ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>GRPO-RM represents a groundbreaking extension of reinforcement learning techniques from language models to visual representation learning, successfully demonstrating that GRPO can be generalized beyond LLMs. By establishing a predefined output set mechanism to replace token sequence sampling and designing specialized reward functions for representation learning properties, the method enables probability-driven optimization of visual models. The framework's ability to significantly outperform standard fine-tuning approaches, particularly on out-of-distribution datasets, validates the potential of reinforcement learning for enhancing representation model performance. Extensive experiments on image classification and semantic segmentation tasks showcase the versatility and effectiveness of GRPO-RM across different computer vision applications. This work opens new research directions for applying reinforcement learning in computer vision, particularly for post-training representation models. The success of GRPO-RM suggests that many RL techniques developed for LLMs may be transferable to other domains, potentially revolutionizing how we approach model fine-tuning in computer vision. Future work can explore extending GRPO-RM to other vision tasks and investigating other RL algorithms' applicability to representation learning.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>GRPO-RMä»£è¡¨äº†å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ä»è¯­è¨€æ¨¡å‹åˆ°è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„çªç ´æ€§æ‰©å±•ï¼ŒæˆåŠŸè¯æ˜GRPOå¯ä»¥è¶…è¶ŠLLMè¿›è¡Œæ³›åŒ–ã€‚é€šè¿‡å»ºç«‹é¢„å®šä¹‰è¾“å‡ºé›†æœºåˆ¶æ¥æ›¿ä»£tokenåºåˆ—é‡‡æ ·ï¼Œå¹¶ä¸ºè¡¨ç¤ºå­¦ä¹ å±æ€§è®¾è®¡ä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥æ–¹æ³•å®ç°äº†è§†è§‰æ¨¡å‹çš„æ¦‚ç‡é©±åŠ¨ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶åœ¨æ˜¾è‘—ä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•æ–¹é¢çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šï¼ŒéªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºè¡¨ç¤ºæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚åœ¨å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒå±•ç¤ºäº†GRPO-RMåœ¨ä¸åŒè®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨è®¡ç®—æœºè§†è§‰ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è¡¨ç¤ºæ¨¡å‹çš„åè®­ç»ƒã€‚GRPO-RMçš„æˆåŠŸè¡¨æ˜ï¼Œè®¸å¤šä¸ºLLMå¼€å‘çš„RLæŠ€æœ¯å¯èƒ½æ˜¯å¯è½¬ç§»çš„ï¼Œå¯èƒ½å½»åº•æ”¹å˜æˆ‘ä»¬åœ¨è®¡ç®—æœºè§†è§‰ä¸­å¤„ç†æ¨¡å‹å¾®è°ƒçš„æ–¹å¼ã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥æ¢ç´¢å°†GRPO-RMæ‰©å±•åˆ°å…¶ä»–è§†è§‰ä»»åŠ¡ï¼Œå¹¶è°ƒæŸ¥å…¶ä»–RLç®—æ³•å¯¹è¡¨ç¤ºå­¦ä¹ çš„é€‚ç”¨æ€§ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2511.15256v1" target="_blank" style="color:#8bffcf;">2511.15256v1</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# GRPO-RM Implementation<br>
<br>
// GRPO for Representation Models<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">GRPORepresentationTrainer</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""GRPO-RM: Group Relative Policy Optimization for Representation Models"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, model, num_classes, group_size=8, epsilon=0.2):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model = model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_classes = num_classes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.group_size = group_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.epsilon = epsilon<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Predefined output set for group generation</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.output_set = torch.eye(num_classes)  <span style="color:#6a9955;"># One-hot vectors as predefined outputs</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.optimizer = AdamW(model.parameters(), lr=1e-5)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">generate_output_group</span>(self, features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Generate output group using predefined set (replaces token sampling)"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size = features.size(0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_groups = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log_probs = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> _ <span style="color:#569cd6;">in</span> range(self.group_size):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Sample from predefined output set based on model predictions</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logits = self.model.classifier(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probs = F.softmax(logits, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Sample outputs from the predefined set</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sampled_indices = torch.multinomial(probs, 1).squeeze(-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sampled_outputs = self.output_set[sampled_indices]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_groups.append(sampled_outputs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log_probs.append(probs.gather(1, sampled_indices.unsqueeze(-1)).log())<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> torch.stack(output_groups, dim=1), torch.stack(log_probs, dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">compute_grpo_rewards</span>(self, outputs, targets):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Compute specialized rewards for representation learning"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size, group_size, _ = outputs.shape<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards = torch.zeros(batch_size, group_size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> b <span style="color:#569cd6;">in</span> range(batch_size):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> g <span style="color:#569cd6;">in</span> range(group_size):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;predicted_class = outputs[b, g].argmax().item()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;true_class = targets[b].item()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Accuracy reward: +1 for correct, 0 for incorrect</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accuracy_reward = 1.0 <span style="color:#569cd6;">if</span> predicted_class == true_class <span style="color:#569cd6;">else</span> 0.0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Uniformity reward: discourage wrong predictions uniformly</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uniformity_penalty = -0.1 <span style="color:#569cd6;">if</span> predicted_class != true_class <span style="color:#569cd6;">else</span> 0.0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards[b, g] = accuracy_reward + uniformity_penalty<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> rewards<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">compute_group_relative_advantages</span>(self, rewards):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Compute group-relative advantages (core GRPO innovation)"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Group-relative normalization</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_mean = rewards.mean(dim=1, keepdim=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_std = rewards.std(dim=1, keepdim=True) + 1e-8<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalized_rewards = (rewards - group_mean) / group_std<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> normalized_rewards<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">train_step</span>(self, images, targets):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Single GRPO-RM training step"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Extract features</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = self.model.backbone(images)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Generate output groups and log probabilities</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output_groups, old_log_probs = self.generate_output_group(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Compute rewards</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards = self.compute_grpo_rewards(output_groups, targets)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Compute group-relative advantages</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;advantages = self.compute_group_relative_advantages(rewards)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Get new log probabilities after model update</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_, new_log_probs = self.generate_output_group(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># PPO-style clipped loss with GRPO advantages</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ratio = torch.exp(new_log_probs - old_log_probs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clipped_ratio = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Update model</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.optimizer.zero_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;policy_loss.backward()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.optimizer.step()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> policy_loss.item()<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
