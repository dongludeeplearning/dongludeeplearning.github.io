<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Engagement Measurement Based on Facial Landmarks and Spatial-Temporal Graph Convolutional Networks</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Ali Abedi and Shehroz S. Khan<br>
        <span style="opacity:0.8">KITE Research Institute, University Health Network, Toronto, Canada</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we develop a privacy-preserving, computationally efficient method for measuring student engagement in virtual learning using only facial landmarks analyzed through spatial-temporal graph convolutional networks?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>æˆ‘ä»¬å¦‚ä½•å¼€å‘ä¸€ç§éšç§ä¿æŠ¤ã€è®¡ç®—é«˜æ•ˆçš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨é€šè¿‡æ—¶ç©ºå›¾å·ç§¯ç½‘ç»œåˆ†æçš„é¢éƒ¨åœ°æ ‡æ¥æµ‹é‡è™šæ‹Ÿå­¦ä¹ ä¸­çš„å­¦ç”Ÿå‚ä¸åº¦ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Novel Engagement Measurement Framework:</b> First approach using facial landmarks with ST-GCNs for video-based engagement measurement, eliminating need for raw facial videos and multiple handcrafted features while maintaining privacy.</div>
            <div class="lang-zh" style="display:none"><b>æ–°é¢–çš„å‚ä¸åº¦æµ‹é‡æ¡†æ¶ï¼š</b>é¦–æ¬¡ä½¿ç”¨é¢éƒ¨åœ°æ ‡ä¸ST-GCNsè¿›è¡ŒåŸºäºè§†é¢‘çš„å‚ä¸åº¦æµ‹é‡çš„æ–¹æ³•ï¼Œæ¶ˆé™¤äº†å¯¹åŸå§‹é¢éƒ¨è§†é¢‘å’Œå¤šä¸ªæ‰‹å·¥ç‰¹å¾çš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒéšç§æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Ordinal Learning Integration:</b> Novel ordinal classification framework using transfer learning to incorporate the ordinal nature of engagement levels, improving classification accuracy through better modeling of engagement hierarchies.</div>
            <div class="lang-zh" style="display:none"><b>åºæ•°å­¦ä¹ æ•´åˆï¼š</b>ä½¿ç”¨è¿ç§»å­¦ä¹ çš„æ–°é¢–åºæ•°åˆ†ç±»æ¡†æ¶ï¼Œä»¥æ•´åˆå‚ä¸åº¦æ°´å¹³çš„åºæ•°æ€§è´¨ï¼Œé€šè¿‡æ›´å¥½åœ°å»ºæ¨¡å‚ä¸åº¦å±‚æ¬¡æ¥æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>State-of-the-Art Performance:</b> Achieved improved state-of-the-art results with 3.1% accuracy gain on EngageNet dataset and 1.5% gain on Online Student Engagement dataset, while being computationally efficient and real-time capable.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›æ€§èƒ½ï¼š</b>åœ¨EngageNetæ•°æ®é›†ä¸Šå®ç°äº†3.1%çš„å‡†ç¡®æ€§æå‡ï¼Œåœ¨åœ¨çº¿å­¦ç”Ÿå‚ä¸åº¦æ•°æ®é›†ä¸Šå®ç°äº†1.5%çš„æå‡ï¼ŒåŒæ—¶è®¡ç®—é«˜æ•ˆä¸”èƒ½å¤Ÿå®æ—¶è¿è¡Œã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Interpretability and Explainability:</b> Applied Grad-CAM to ST-GCNs for spatial and temporal interpretation of engagement measurements, enabling instructors to understand which facial regions and time points contribute to engagement levels.</div>
            <div class="lang-zh" style="display:none"><b>å¯è§£é‡Šæ€§å’Œå¯è¯´æ˜æ€§ï¼š</b>å°†Grad-CAMåº”ç”¨äºST-GCNsï¼Œä»¥å®ç°å‚ä¸åº¦æµ‹é‡çš„ç©ºé—´å’Œæ—¶é—´è§£é‡Šï¼Œä½¿æ•™å¸ˆèƒ½å¤Ÿäº†è§£å“ªäº›é¢éƒ¨åŒºåŸŸå’Œæ—¶é—´ç‚¹æœ‰åŠ©äºå‚ä¸åº¦æ°´å¹³ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Privacy Preservation:</b> Need to avoid using personally identifiable information from raw facial videos while maintaining engagement measurement accuracy in virtual learning environments.</div>
            <div class="lang-zh" style="display:none"><b>éšç§ä¿æŠ¤ï¼š</b>éœ€è¦åœ¨è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­é¿å…ä½¿ç”¨åŸå§‹é¢éƒ¨è§†é¢‘ä¸­çš„ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒå‚ä¸åº¦æµ‹é‡å‡†ç¡®æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Computational Efficiency:</b> Developing lightweight models suitable for real-time deployment on local devices without requiring cloud-based processing of privacy-sensitive video data.</div>
            <div class="lang-zh" style="display:none"><b>è®¡ç®—æ•ˆç‡ï¼š</b>å¼€å‘é€‚åˆåœ¨æœ¬åœ°è®¾å¤‡ä¸Šå®æ—¶éƒ¨ç½²çš„è½»é‡çº§æ¨¡å‹ï¼Œè€Œä¸éœ€è¦å¯¹éšç§æ•æ„Ÿçš„è§†é¢‘æ•°æ®è¿›è¡ŒåŸºäºäº‘çš„å¤„ç†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Ordinal Nature Modeling:</b> Properly modeling engagement as an ordinal variable with inherent hierarchical relationships (Not-Engaged â†’ Barely-Engaged â†’ Engaged â†’ Highly-Engaged) rather than treating it as categorical.</div>
            <div class="lang-zh" style="display:none"><b>åºæ•°æ€§è´¨å»ºæ¨¡ï¼š</b>å°†å‚ä¸åº¦æ­£ç¡®å»ºæ¨¡ä¸ºå…·æœ‰å›ºæœ‰å±‚æ¬¡å…³ç³»ï¼ˆæœªå‚ä¸â†’è½»å¾®å‚ä¸â†’å‚ä¸â†’é«˜åº¦å‚ä¸ï¼‰çš„åºæ•°å˜é‡ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºåˆ†ç±»å˜é‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Component Engagement:</b> Capturing the behavioral, affective, and cognitive components of engagement through facial landmarks, including head pose, eye gaze, facial expressions, and blink patterns.</div>
            <div class="lang-zh" style="display:none"><b>å¤šç»„ä»¶å‚ä¸åº¦ï¼š</b>é€šè¿‡é¢éƒ¨åœ°æ ‡æ•è·å‚ä¸åº¦çš„è¡Œä¸ºã€æƒ…æ„Ÿå’Œè®¤çŸ¥ç»„ä»¶ï¼ŒåŒ…æ‹¬å¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€é¢éƒ¨è¡¨æƒ…å’Œçœ¨çœ¼æ¨¡å¼ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The proposed method introduces a comprehensive framework for privacy-preserving engagement measurement:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Facial Landmark Extraction:</b> Uses MediaPipe to extract 78 3D facial landmarks (68 Dlib-compatible + 10 iris landmarks) from video frames, providing geometric features for head pose, AUs, eye gaze, and affect analysis without personal identifiers.</li>
                <li style="margin-bottom:6px;"><b>Spatiotemporal Graph Construction:</b> Constructs spatiotemporal graphs where nodes represent facial landmarks across T consecutive frames, with edges defined by Delaunay triangulation within frames and temporal connections between consecutive frames.</li>
                <li style="margin-bottom:6px;"><b>ST-GCN Architecture:</b> Employs Spatial-Temporal Graph Convolutional Networks with batch normalization, dropout, and residual connections to analyze spatial-temporal facial landmark patterns for engagement inference.</li>
                <li style="margin-bottom:6px;"><b>Ordinal Transfer Learning:</b> Implements transfer learning framework where pre-trained ST-GCN layers are frozen and multiple binary classifiers are trained for ordinal engagement levels, then combined into multi-class probabilities.</li>
                <li style="margin-bottom:6px;"><b>Interpretability Analysis:</b> Applies Grad-CAM to visualize spatial-temporal contributions of facial landmarks to engagement predictions, enabling instructors to understand engagement indicators.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>æ‰€æå‡ºçš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„éšç§ä¿æŠ¤å‚ä¸åº¦æµ‹é‡æ¡†æ¶ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>é¢éƒ¨åœ°æ ‡æå–ï¼š</b>ä½¿ç”¨MediaPipeä»è§†é¢‘å¸§ä¸­æå–78ä¸ª3Dé¢éƒ¨åœ°æ ‡ï¼ˆ68ä¸ªDlibå…¼å®¹+10ä¸ªè™¹è†œåœ°æ ‡ï¼‰ï¼Œä¸ºå¤´éƒ¨å§¿åŠ¿ã€AUã€çœ¼éƒ¨æ³¨è§†å’Œæƒ…æ„Ÿåˆ†ææä¾›å‡ ä½•ç‰¹å¾ï¼Œè€Œä¸ä½¿ç”¨ä¸ªäººæ ‡è¯†ç¬¦ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ—¶ç©ºå›¾æ„å»ºï¼š</b>æ„å»ºæ—¶ç©ºå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºTä¸ªè¿ç»­å¸§ä¸­çš„é¢éƒ¨åœ°æ ‡ï¼Œè¾¹ç”±å¸§å†…çš„Delaunayä¸‰è§’å‰–åˆ†å’Œè¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´è¿æ¥å®šä¹‰ã€‚</li>
                <li style="margin-bottom:6px;"><b>ST-GCNæ¶æ„ï¼š</b>é‡‡ç”¨å…·æœ‰æ‰¹å½’ä¸€åŒ–ã€dropoutå’Œæ®‹å·®è¿æ¥çš„æ—¶ç©ºå›¾å·ç§¯ç½‘ç»œæ¥åˆ†ææ—¶ç©ºé¢éƒ¨åœ°æ ‡æ¨¡å¼ä»¥è¿›è¡Œå‚ä¸åº¦æ¨ç†ã€‚</li>
                <li style="margin-bottom:6px;"><b>åºæ•°è¿ç§»å­¦ä¹ ï¼š</b>å®ç°è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„ST-GCNå±‚è¢«å†»ç»“ï¼Œè®­ç»ƒå¤šä¸ªäºŒå…ƒåˆ†ç±»å™¨ç”¨äºåºæ•°å‚ä¸åº¦æ°´å¹³ï¼Œç„¶åç»„åˆæˆå¤šç±»æ¦‚ç‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯è§£é‡Šæ€§åˆ†æï¼š</b>åº”ç”¨Grad-CAMå¯è§†åŒ–é¢éƒ¨åœ°æ ‡å¯¹å‚ä¸åº¦é¢„æµ‹çš„ç©ºé—´æ—¶é—´è´¡çŒ®ï¼Œä½¿æ•™å¸ˆèƒ½å¤Ÿç†è§£å‚ä¸åº¦æŒ‡æ ‡ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <p>Overview figure showing Grad-CAM interpretation of engagement measurements with facial landmarks colored by activation values (blue to red indicating contribution to Not-Engaged classification).</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_facial_landmarks_engagement.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_facial_landmarks_engagement.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of the ST-GCN approach for engagement measurement stems from several key innovations:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Geometric Feature Preservation:</b> Facial landmarks capture essential geometric information about head pose, eye gaze, blink patterns, and facial expressions that are direct indicators of behavioral and affective engagement components.</li>
                <li style="margin-bottom:6px;"><b>Spatiotemporal Dynamics:</b> ST-GCNs excel at modeling both spatial relationships between facial landmarks and their temporal evolution, capturing how engagement manifests as dynamic facial behavior over time.</li>
                <li style="margin-bottom:6px;"><b>Ordinal Constraint Integration:</b> The transfer learning framework properly models engagement as an ordinal variable, allowing the model to learn meaningful relationships between adjacent engagement levels rather than treating them as independent categories.</li>
                <li style="margin-bottom:6px;"><b>Privacy-by-Design:</b> By using only anonymized landmark coordinates, the approach eliminates privacy concerns while maintaining sufficient information for accurate engagement assessment.</li>
                <li style="margin-bottom:6px;"><b>Computational Efficiency:</b> The lightweight ST-GCN architecture combined with MediaPipe's real-time landmark extraction enables deployment on edge devices without compromising accuracy.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>ST-GCNæ–¹æ³•åœ¨å‚ä¸åº¦æµ‹é‡æ–¹é¢çš„æˆåŠŸæºäºå‡ ä¸ªå…³é”®åˆ›æ–°ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>å‡ ä½•ç‰¹å¾ä¿å­˜ï¼š</b>é¢éƒ¨åœ°æ ‡æ•è·å¤´éƒ¨å§¿åŠ¿ã€çœ¼éƒ¨æ³¨è§†ã€çœ¨çœ¼æ¨¡å¼å’Œé¢éƒ¨è¡¨æƒ…çš„å…³é”®å‡ ä½•ä¿¡æ¯ï¼Œè¿™äº›æ˜¯è¡Œä¸ºå’Œæƒ…æ„Ÿå‚ä¸åº¦ç»„ä»¶çš„ç›´æ¥æŒ‡æ ‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ—¶ç©ºåŠ¨æ€ï¼š</b>ST-GCNsæ“…é•¿å»ºæ¨¡é¢éƒ¨åœ°æ ‡ä¹‹é—´çš„ç©ºé—´å…³ç³»åŠå…¶æ—¶é—´æ¼”å˜ï¼Œæ•è·å‚ä¸åº¦å¦‚ä½•è¡¨ç°ä¸ºéšæ—¶é—´å˜åŒ–çš„åŠ¨æ€é¢éƒ¨è¡Œä¸ºã€‚</li>
                <li style="margin-bottom:6px;"><b>åºæ•°çº¦æŸæ•´åˆï¼š</b>è¿ç§»å­¦ä¹ æ¡†æ¶æ­£ç¡®åœ°å°†å‚ä¸åº¦å»ºæ¨¡ä¸ºåºæ•°å˜é‡ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ ç›¸é‚»å‚ä¸åº¦æ°´å¹³ä¹‹é—´çš„æœ‰æ„ä¹‰å…³ç³»ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬è§†ä¸ºç‹¬ç«‹ç±»åˆ«ã€‚</li>
                <li style="margin-bottom:6px;"><b>è®¾è®¡éšç§ï¼š</b>é€šè¿‡ä»…ä½¿ç”¨åŒ¿ååŒ–çš„åœ°æ ‡åæ ‡ï¼Œè¯¥æ–¹æ³•æ¶ˆé™¤äº†éšç§é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒè¶³å¤Ÿçš„ä¿¡æ¯ç”¨äºå‡†ç¡®çš„å‚ä¸åº¦è¯„ä¼°ã€‚</li>
                <li style="margin-bottom:6px;"><b>è®¡ç®—æ•ˆç‡ï¼š</b>è½»é‡çº§ST-GCNæ¶æ„ä¸MediaPipeçš„å®æ—¶åœ°æ ‡æå–ç›¸ç»“åˆï¼Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This work presents a groundbreaking approach to engagement measurement that prioritizes privacy, efficiency, and interpretability. By leveraging facial landmarks as the sole input modality and employing ST-GCNs for spatiotemporal analysis, the method achieves state-of-the-art performance on two challenging engagement datasets while requiring only 861K parameters and 0.8ms inference time. The ordinal learning framework properly accounts for the hierarchical nature of engagement levels, and Grad-CAM provides spatial-temporal interpretability that can guide educational interventions. The integration with MediaPipe enables real-time deployment on virtual learning platforms, making it practically viable for large-scale educational applications. Future work could explore attention mechanisms within ST-GCNs and investigate multi-modal fusion with audio or physiological signals for even more comprehensive engagement assessment.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§å¼€åˆ›æ€§çš„å‚ä¸åº¦æµ‹é‡æ–¹æ³•ï¼Œä¼˜å…ˆè€ƒè™‘éšç§æ€§ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡åˆ©ç”¨é¢éƒ¨åœ°æ ‡ä½œä¸ºå”¯ä¸€çš„è¾“å…¥æ¨¡æ€å¹¶é‡‡ç”¨ST-GCNsè¿›è¡Œæ—¶ç©ºåˆ†æï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å‚ä¸åº¦æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åªéœ€è¦861Kå‚æ•°å’Œ0.8msæ¨ç†æ—¶é—´ã€‚åºæ•°å­¦ä¹ æ¡†æ¶æ­£ç¡®åœ°è€ƒè™‘äº†å‚ä¸åº¦æ°´å¹³çš„å±‚æ¬¡æ€§è´¨ï¼ŒGrad-CAMæä¾›äº†å¯ä»¥æŒ‡å¯¼æ•™è‚²å¹²é¢„çš„ç©ºé—´æ—¶é—´å¯è§£é‡Šæ€§ã€‚ä¸MediaPipeçš„é›†æˆå®ç°äº†åœ¨è™šæ‹Ÿå­¦ä¹ å¹³å°ä¸Šçš„å®æ—¶éƒ¨ç½²ï¼Œä½¿å…¶åœ¨å¤§å‹æ•™è‚²åº”ç”¨ä¸­å…·æœ‰å®é™…å¯è¡Œæ€§ã€‚æœªæ¥å·¥ä½œå¯ä»¥æ¢ç´¢ST-GCNsä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ç ”ç©¶ä¸éŸ³é¢‘æˆ–ç”Ÿç†ä¿¡å·çš„å¤šæ¨¡æ€èåˆï¼Œä»¥å®ç°æ›´å…¨é¢çš„å‚ä¸åº¦è¯„ä¼°ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/pdf/2403.17175v2" style="color:#8bffcf;">2403.17175v2</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import torch<br>
import torch.nn as nn<br>
import torch.nn.functional as F<br>
from torch_geometric.nn import GCNConv<br>
<br>
// ST-GCN for Engagement Measurement<br>
class STGCNLayer(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, in_channels, out_channels, temporal_kernel=9):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(STGCNLayer, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.temporal_kernel = temporal_kernel<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Spatial convolution (1x1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.spatial_conv = nn.Conv2d(in_channels, out_channels, 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Temporal convolution (1xT)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.temporal_conv = nn.Conv2d(out_channels, out_channels,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1, temporal_kernel))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Learnable adjacency matrix scaling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.adj_scale = nn.Parameter(torch.ones(78, 78))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, x, adjacency_matrix):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# x: (batch_size, channels, num_nodes, time_steps)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Spatial convolution<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = self.spatial_conv(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Apply learnable adjacency scaling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adj_scaled = adjacency_matrix * self.adj_scale<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adj_norm = self.normalize_adjacency(adj_scaled)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Apply spatial graph convolution<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = torch.einsum('bctn,nm->bcmn', x, adj_norm)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Temporal convolution<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = self.temporal_conv(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return x<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def normalize_adjacency(self, adj):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Normalize adjacency matrix<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d = torch.sum(adj, dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d_inv_sqrt = torch.pow(d, -0.5)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d_mat = torch.diag(d_inv_sqrt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return torch.mm(torch.mm(d_mat, adj), d_mat)<br>
<br>
// Ordinal Engagement Classifier<br>
class OrdinalEngagementClassifier(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, num_classes=4):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(OrdinalEngagementClassifier, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# ST-GCN layers<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.stgcn_layers = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STGCNLayer(3, 64),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STGCNLayer(64, 128),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STGCNLayer(128, 256)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Global pooling and classification<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.global_pool = nn.AdaptiveAvgPool2d((1, 1))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.classifier = nn.Linear(256, num_classes)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, x, adjacency_matrix):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# x: (batch_size, 3, num_nodes, time_steps)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for layer in self.stgcn_layers:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = layer(x, adjacency_matrix)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = F.relu(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Global pooling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = self.global_pool(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x = x.view(x.size(0), -1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Classification<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logits = self.classifier(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return logits<br>
<br>
// Ordinal Transfer Learning Framework<br>
class OrdinalTransferLearning:<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, base_model, num_classes=4):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.base_model = base_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_classes = num_classes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.binary_classifiers = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(256, 1) for _ in range(num_classes - 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def convert_to_ordinal_labels(self, labels):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Convert K-class labels to K-1 binary labels<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ordinal_labels = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for i in range(self.num_classes - 1):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;binary_labels = (labels > i).float()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ordinal_labels.append(binary_labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ordinal_labels<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def predict_ordinal(self, features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get binary predictions<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;binary_preds = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for classifier in self.binary_classifiers:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pred = torch.sigmoid(classifier(features)).squeeze()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;binary_preds.append(pred)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Convert to ordinal probabilities<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probabilities = torch.zeros(features.size(0), self.num_classes)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for i in range(self.num_classes):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if i == 0:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probabilities[:, i] = 1 - binary_preds[0]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif i == self.num_classes - 1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probabilities[:, i] = binary_preds[-1]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probabilities[:, i] = binary_preds[i-1] - binary_preds[i]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return probabilities<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
