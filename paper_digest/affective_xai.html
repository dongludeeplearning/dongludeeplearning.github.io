<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ICCV Workshop 2021
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Luke Guerdan, Alex Raymond, and Hatice Gunes<br>
        <span style="opacity:0.8">University of Cambridge, United Kingdom</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can facial affect analysis be used to understand how users emotionally respond to explainable AI interfaces, enabling personalization of explanations based on task difficulty and user interaction styles?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åˆ©ç”¨é¢éƒ¨æƒ…æ„Ÿåˆ†ææ¥ç†è§£ç”¨æˆ·å¯¹å¯è§£é‡ŠAIç•Œé¢çš„æƒ…æ„Ÿååº”ï¼Œä»è€Œæ ¹æ®ä»»åŠ¡éš¾åº¦å’Œç”¨æˆ·äº¤äº’é£æ ¼æ¥ä¸ªæ€§åŒ–è§£é‡Šï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Facial Affect Feature Identification:</b> First systematic investigation of which facial affect features (AU1, AU4, Arousal) are pronounced when users interact with XAI interfaces, identifying emotional markers of explanation efficacy.</div>
            <div class="lang-zh" style="display:none"><b>é¢éƒ¨æƒ…æ„Ÿç‰¹å¾è¯†åˆ«ï¼š</b>é¦–æ¬¡ç³»ç»Ÿæ€§è°ƒæŸ¥å“ªäº›é¢éƒ¨æƒ…æ„Ÿç‰¹å¾ï¼ˆAU1ã€AU4ã€å”¤é†’æ°´å¹³ï¼‰åœ¨ç”¨æˆ·ä¸XAIç•Œé¢äº¤äº’æ—¶è¡¨ç°çªå‡ºï¼Œè¯†åˆ«è§£é‡Šæœ‰æ•ˆæ€§çš„æƒ…æ„Ÿæ ‡è®°ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multitask Feature Embedding:</b> Developed a PCA-based multitask embedding that leverages combined affect predictions from multiple models (valence/arousal, categorical, AUs) to capture explanation-related affective signals.</div>
            <div class="lang-zh" style="display:none"><b>å¤šä»»åŠ¡ç‰¹å¾åµŒå…¥ï¼š</b>å¼€å‘äº†åŸºäºPCAçš„å¤šä»»åŠ¡åµŒå…¥ï¼Œåˆ©ç”¨æ¥è‡ªå¤šä¸ªæ¨¡å‹ï¼ˆæ•ˆä»·/å”¤é†’ã€åˆ†ç±»ã€AUï¼‰çš„ç»„åˆæƒ…æ„Ÿé¢„æµ‹æ¥æ•è·ä¸è§£é‡Šç›¸å…³çš„æ„Ÿæƒ…ä¿¡å·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Task Difficulty Interaction Analysis:</b> Demonstrated how facial affect signals vary with task difficulty and explanation provision, showing that explanations reduce negative affect in difficult tasks but may increase it in easy tasks.</div>
            <div class="lang-zh" style="display:none"><b>ä»»åŠ¡éš¾åº¦äº¤äº’åˆ†æï¼š</b>å±•ç¤ºäº†é¢éƒ¨æƒ…æ„Ÿä¿¡å·å¦‚ä½•éšä»»åŠ¡éš¾åº¦å’Œè§£é‡Šæä¾›è€Œå˜åŒ–ï¼Œè¡¨æ˜è§£é‡Šåœ¨å›°éš¾ä»»åŠ¡ä¸­å‡å°‘è´Ÿé¢æƒ…æ„Ÿï¼Œä½†åœ¨ç®€å•ä»»åŠ¡ä¸­å¯èƒ½å¢åŠ è´Ÿé¢æƒ…æ„Ÿã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Subjective-Objective Correlation:</b> Established correlations between objective facial affect predictions and subjective user perceptions of competence, task difficulty, agency, and explanation utility in XAI contexts.</div>
            <div class="lang-zh" style="display:none"><b>ä¸»å®¢è§‚ç›¸å…³æ€§ï¼š</b>å»ºç«‹äº†å®¢è§‚é¢éƒ¨æƒ…æ„Ÿé¢„æµ‹ä¸ç”¨æˆ·å¯¹èƒ½åŠ›ã€ä»»åŠ¡éš¾åº¦ã€ä»£ç†å’Œè§£é‡Šæ•ˆç”¨çš„ä¸»è§‚æ„ŸçŸ¥ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œåœ¨XAIèƒŒæ™¯ä¸‹ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Ground-Truth Label Absence:</b> XAI datasets lack ground-truth affect labels, making it challenging to evaluate facial affect prediction accuracy and requiring inter-model reliability analysis instead.</div>
            <div class="lang-zh" style="display:none"><b>ç¼ºä¹åœ°é¢çœŸå®æ ‡ç­¾ï¼š</b>XAIæ•°æ®é›†ç¼ºä¹åœ°é¢çœŸå®æƒ…æ„Ÿæ ‡ç­¾ï¼Œä½¿å¾—è¯„ä¼°é¢éƒ¨æƒ…æ„Ÿé¢„æµ‹å‡†ç¡®æ€§å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä½¿ç”¨æ¨¡å‹é—´å¯é æ€§åˆ†æè€Œä¸æ˜¯ç›´æ¥è¯„ä¼°ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multimodal Affect Representation:</b> Different affect models (dimensional, categorical, AUs) provide complementary but inconsistent information, requiring integration strategies for comprehensive affective analysis.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€æƒ…æ„Ÿè¡¨ç¤ºï¼š</b>ä¸åŒæƒ…æ„Ÿæ¨¡å‹ï¼ˆç»´åº¦ã€åˆ†ç±»ã€AUï¼‰æä¾›äº’è¡¥ä½†ä¸ä¸€è‡´çš„ä¿¡æ¯ï¼Œéœ€è¦æ•´åˆç­–ç•¥æ¥è¿›è¡Œå…¨é¢çš„æƒ…æ„Ÿåˆ†æã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Contextual Affective Interpretation:</b> Facial expressions in XAI contexts must be interpreted considering task difficulty, user goals, and interaction history, rather than isolated emotional states.</div>
            <div class="lang-zh" style="display:none"><b>æƒ…å¢ƒåŒ–æƒ…æ„Ÿè§£é‡Šï¼š</b>XAIä¸Šä¸‹æ–‡ä¸­çš„é¢éƒ¨è¡¨æƒ…å¿…é¡»è€ƒè™‘ä»»åŠ¡éš¾åº¦ã€ç”¨æˆ·ç›®æ ‡å’Œäº¤äº’å†å²ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„æƒ…ç»ªçŠ¶æ€ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Real-Time Processing Constraints:</b> Facial affect analysis for XAI personalization requires real-time processing capabilities that balance accuracy with computational efficiency for practical deployment.</div>
            <div class="lang-zh" style="display:none"><b>å®æ—¶å¤„ç†çº¦æŸï¼š</b>XAIä¸ªæ€§åŒ–çš„é¢éƒ¨æƒ…æ„Ÿåˆ†æéœ€è¦å®æ—¶å¤„ç†èƒ½åŠ›ï¼Œåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä»¥å®ç°å®é™…éƒ¨ç½²ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The paper presents a comprehensive framework for analyzing facial affect in XAI contexts:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Multitask Affect Prediction:</b> Employed three complementary affect representations (valence/arousal, categorical emotions, facial AUs) using OpenFace and multitask CNN/RNN models trained on Aff-Wild2 dataset with knowledge distillation.</li>
                <li style="margin-bottom:6px;"><b>Reliability Analysis:</b> Used Cohen's Îº and Concordance Correlation Coefficient (CCC) to assess inter-model agreement between affect predictions from different extraction methods when ground-truth labels are unavailable.</li>
                <li style="margin-bottom:6px;"><b>Explanation-Task Interaction Analysis:</b> Compared facial affect signals between explanation (X) and no-explanation (N) conditions across three difficulty levels, analyzing how explanations modulate affective responses to task demands.</li>
                <li style="margin-bottom:6px;"><b>Event-Based Affect Analysis:</b> Examined affective responses during key game events (collisions, success) by comparing affect predictions in event windows versus baseline periods to identify emotionally salient moments.</li>
                <li style="margin-bottom:6px;"><b>Multitask Feature Embedding:</b> Applied PCA to create low-dimensional embeddings from combined affect predictions across all models, identifying components that capture explanation-related affective patterns.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢æ¡†æ¶ï¼Œç”¨äºåˆ†æXAIä¸Šä¸‹æ–‡ä¸­çš„é¢éƒ¨æƒ…æ„Ÿï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>å¤šä»»åŠ¡æƒ…æ„Ÿé¢„æµ‹ï¼š</b>ä½¿ç”¨OpenFaceå’Œåœ¨Aff-Wild2æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤šä»»åŠ¡CNN/RNNæ¨¡å‹ï¼Œé‡‡ç”¨ä¸‰ç§äº’è¡¥çš„æƒ…æ„Ÿè¡¨ç¤ºï¼ˆæ•ˆä»·/å”¤é†’ã€åˆ†ç±»æƒ…ç»ªã€é¢éƒ¨AUï¼‰ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯é æ€§åˆ†æï¼š</b>ä½¿ç”¨Cohen's Îºå’Œä¸€è‡´æ€§ç›¸å…³ç³»æ•°ï¼ˆCCCï¼‰æ¥è¯„ä¼°ä¸åŒæå–æ–¹æ³•çš„æƒ…æ„Ÿé¢„æµ‹ä¹‹é—´çš„æ¨¡å‹é—´ä¸€è‡´æ€§ï¼Œå½“åœ°é¢çœŸå®æ ‡ç­¾ä¸å¯ç”¨æ—¶ã€‚</li>
                <li style="margin-bottom:6px;"><b>è§£é‡Š-ä»»åŠ¡äº¤äº’åˆ†æï¼š</b>åœ¨ä¸‰ç§éš¾åº¦æ°´å¹³ä¸Šæ¯”è¾ƒè§£é‡Šï¼ˆXï¼‰å’Œæ— è§£é‡Šï¼ˆNï¼‰æ¡ä»¶ä¹‹é—´çš„é¢éƒ¨æƒ…æ„Ÿä¿¡å·ï¼Œåˆ†æè§£é‡Šå¦‚ä½•è°ƒèŠ‚å¯¹ä»»åŠ¡éœ€æ±‚çš„æ„Ÿæƒ…ååº”ã€‚</li>
                <li style="margin-bottom:6px;"><b>åŸºäºäº‹ä»¶çš„æ„Ÿæƒ…åˆ†æï¼š</b>é€šè¿‡æ¯”è¾ƒäº‹ä»¶çª—å£ä¸åŸºçº¿æœŸé—´çš„æƒ…æ„Ÿé¢„æµ‹æ¥æ£€æŸ¥å…³é”®æ¸¸æˆäº‹ä»¶ï¼ˆç¢°æ’ã€æˆåŠŸï¼‰æœŸé—´çš„æ„Ÿæƒ…ååº”ï¼Œä»¥è¯†åˆ«æƒ…æ„Ÿæ˜¾è‘—æ—¶åˆ»ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šä»»åŠ¡ç‰¹å¾åµŒå…¥ï¼š</b>åº”ç”¨PCAä»æ‰€æœ‰æ¨¡å‹çš„ç»„åˆæƒ…æ„Ÿé¢„æµ‹ä¸­åˆ›å»ºä½ç»´åµŒå…¥ï¼Œè¯†åˆ«æ•è·è§£é‡Šç›¸å…³æƒ…æ„Ÿæ¨¡å¼çš„ç»„ä»¶ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/affective_xai_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p style="margin-top:12px;">Multitask emotion recognition pipeline showing CNN and CNN+RNN architectures with affect predictions from valence/arousal, categorical emotions, and facial AUs.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('iccv_affective_xai.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('iccv_affective_xai.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of affective XAI through facial affect analysis stems from several key methodological innovations:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Inter-Model Reliability Assessment:</b> By establishing agreement between different affect prediction models without ground-truth labels, the approach provides confidence in affect measurements and identifies consistent affective markers across extraction methods.</li>
                <li style="margin-bottom:6px;"><b>Multitask Affect Integration:</b> Combining dimensional, categorical, and AU-based representations captures complementary aspects of affect, enabling more comprehensive analysis of user emotional states during XAI interactions.</li>
                <li style="margin-bottom:6px;"><b>Contextual Event Analysis:</b> Examining affect during specific interaction events (collisions, successes) reveals how explanations modulate emotional responses to task outcomes, providing insights into explanation efficacy.</li>
                <li style="margin-bottom:6px;"><b>Subjective-Objective Alignment:</b> Correlating facial affect measurements with self-reported perceptions validates the ecological validity of affect predictions and bridges objective measurements with subjective user experience.</li>
                <li style="margin-bottom:6px;"><b>Unsupervised Pattern Discovery:</b> PCA-based component analysis identifies latent affective patterns related to explanation use, revealing how different affect features combine to represent user interaction styles and task engagement.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>é€šè¿‡é¢éƒ¨æƒ…æ„Ÿåˆ†æçš„æ„Ÿæƒ…XAIçš„æˆåŠŸæºäºå‡ ä¸ªå…³é”®æ–¹æ³•åˆ›æ–°ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ¨¡å‹é—´å¯é æ€§è¯„ä¼°ï¼š</b>é€šè¿‡åœ¨æ²¡æœ‰åœ°é¢çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹å»ºç«‹ä¸åŒæƒ…æ„Ÿé¢„æµ‹æ¨¡å‹ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè¯¥æ–¹æ³•ä¸ºæƒ…æ„Ÿæµ‹é‡æä¾›äº†ä¿¡å¿ƒï¼Œå¹¶åœ¨æå–æ–¹æ³•ä¸­è¯†åˆ«äº†ä¸€è‡´çš„æƒ…æ„Ÿæ ‡è®°ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šä»»åŠ¡æƒ…æ„Ÿæ•´åˆï¼š</b>ç»“åˆç»´åº¦ã€åˆ†ç±»å’ŒåŸºäºAUçš„è¡¨ç¤ºæ•è·æƒ…æ„Ÿçš„äº’è¡¥æ–¹é¢ï¼Œä½¿XAIäº¤äº’æœŸé—´ç”¨æˆ·æƒ…ç»ªçŠ¶æ€çš„æ›´å…¨é¢åˆ†ææˆä¸ºå¯èƒ½ã€‚</li>
                <li style="margin-bottom:6px;"><b>æƒ…å¢ƒäº‹ä»¶åˆ†æï¼š</b>æ£€æŸ¥ç‰¹å®šäº¤äº’äº‹ä»¶ï¼ˆç¢°æ’ã€æˆåŠŸï¼‰æœŸé—´çš„æƒ…æ„Ÿæ­ç¤ºäº†è§£é‡Šå¦‚ä½•è°ƒèŠ‚å¯¹ä»»åŠ¡ç»“æœçš„æƒ…æ„Ÿååº”ï¼Œæä¾›å¯¹è§£é‡Šæœ‰æ•ˆæ€§çš„è§è§£ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä¸»å®¢è§‚ä¸€è‡´æ€§ï¼š</b>å°†é¢éƒ¨æƒ…æ„Ÿæµ‹é‡ä¸è‡ªæˆ‘æŠ¥å‘Šæ„ŸçŸ¥ç›¸å…³è”éªŒè¯äº†æƒ…æ„Ÿé¢„æµ‹çš„ç”Ÿæ€æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å®¢è§‚æµ‹é‡ä¸ä¸»è§‚ç”¨æˆ·ä½“éªŒè¿æ¥èµ·æ¥ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ— ç›‘ç£æ¨¡å¼å‘ç°ï¼š</b>åŸºäºPCAçš„ç»„ä»¶åˆ†æè¯†åˆ«ä¸è§£é‡Šä½¿ç”¨ç›¸å…³çš„æ½œåœ¨æƒ…æ„Ÿæ¨¡å¼ï¼Œæ­ç¤ºäº†ä¸åŒæƒ…æ„Ÿç‰¹å¾å¦‚ä½•ç»„åˆä»¥è¡¨ç¤ºç”¨æˆ·äº¤äº’é£æ ¼å’Œä»»åŠ¡å‚ä¸åº¦ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This pioneering work establishes facial affect analysis as a critical component of explainable AI systems by demonstrating how users' emotional responses to explanations can be measured and utilized to improve human-AI interaction. The study shows that facial affect features like AU1, AU4, and arousal levels serve as reliable indicators of explanation efficacy, varying systematically with task difficulty and user interaction patterns. By developing multitask embeddings that combine multiple affect representations, the approach reveals latent affective components that capture how users engage with and understand explanations. The correlation between objective facial measurements and subjective user perceptions validates the approach's ecological validity and practical utility. This work opens new research directions for affective XAI, suggesting that future explanation systems should adapt not just to user performance but also to their emotional responses, potentially leading to more effective and user-centered AI interfaces across domains like healthcare, education, and collaborative decision-making.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å¼€åˆ›æ€§å·¥ä½œé€šè¿‡å±•ç¤ºå¦‚ä½•æµ‹é‡å’Œåˆ©ç”¨ç”¨æˆ·å¯¹è§£é‡Šçš„æƒ…æ„Ÿååº”æ¥æ”¹å–„äººæœºäº¤äº’ï¼Œå°†é¢éƒ¨æƒ…æ„Ÿåˆ†æç¡®ç«‹ä¸ºå¯è§£é‡ŠAIç³»ç»Ÿçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢éƒ¨æƒ…æ„Ÿç‰¹å¾å¦‚AU1ã€AU4å’Œå”¤é†’æ°´å¹³æ˜¯è§£é‡Šæœ‰æ•ˆæ€§çš„å¯é æŒ‡æ ‡ï¼Œéšç€ä»»åŠ¡éš¾åº¦å’Œç”¨æˆ·äº¤äº’æ¨¡å¼è€Œç³»ç»Ÿæ€§åœ°å˜åŒ–ã€‚é€šè¿‡å¼€å‘ç»“åˆå¤šç§æƒ…æ„Ÿè¡¨ç¤ºçš„å¤šä»»åŠ¡åµŒå…¥ï¼Œè¯¥æ–¹æ³•æ­ç¤ºäº†æ•è·ç”¨æˆ·å¦‚ä½•å‚ä¸å’Œç†è§£è§£é‡Šçš„æ½œåœ¨æƒ…æ„Ÿç»„ä»¶ã€‚å®¢è§‚é¢éƒ¨æµ‹é‡ä¸ä¸»è§‚ç”¨æˆ·æ„ŸçŸ¥ä¹‹é—´çš„ç›¸å…³æ€§éªŒè¯äº†è¯¥æ–¹æ³•çš„ç”Ÿæ€æœ‰æ•ˆæ€§å’Œå®é™…æ•ˆç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºæƒ…æ„ŸXAIå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œè¡¨æ˜æœªæ¥çš„è§£é‡Šç³»ç»Ÿä¸ä»…åº”è¯¥é€‚åº”ç”¨æˆ·æ€§èƒ½ï¼Œè¿˜åº”è¯¥é€‚åº”ä»–ä»¬çš„æƒ…æ„Ÿååº”ï¼Œå¯èƒ½åœ¨åŒ»ç–—ã€æ•™è‚²å’Œåä½œå†³ç­–ç­‰é¢†åŸŸå¸¦æ¥æ›´æœ‰æ•ˆå’Œä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½ç•Œé¢ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://openaccess.thecvf.com/content/ICCV2021W/RPRMI/papers/Guerdan_Toward_Affective_XAI_Facial_Affect_Analysis_for_Understanding_Explainable_Human-AI_ICCVW_2021_paper.pdf" style="color:#8bffcf;">ICCV Workshop 2021</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import torch<br>
import torch.nn as nn<br>
import numpy as np<br>
from sklearn.decomposition import PCA<br>
from sklearn.metrics import cohen_kappa_score<br>
<br>
// Multitask Emotion Recognition with Knowledge Distillation<br>
class MultitaskEmotionNet(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, num_emotions=7, num_aus=12):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(MultitaskEmotionNet, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Shared backbone<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.backbone = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(3, 64, 3, padding=1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.BatchNorm2d(64),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.MaxPool2d(2),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# ... more conv layers<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Task-specific heads<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.valence_head = nn.Linear(512, 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.arousal_head = nn.Linear(512, 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.emotion_head = nn.Linear(512, num_emotions)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.au_heads = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(512, 1) for _ in range(num_aus)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, x):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = self.backbone(x)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = features.view(features.size(0), -1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;valence = self.valence_head(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arousal = self.arousal_head(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotions = self.emotion_head(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aus = torch.cat([head(features) for head in self.au_heads], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return valence, arousal, emotions, aus<br>
<br>
// Knowledge Distillation Loss for Incomplete Labels<br>
def distillation_loss(student_output, teacher_output, ground_truth, lambda_distill=0.5):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;Combined loss with ground truth supervision and teacher distillation<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;supervision_loss = 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;distillation_loss = 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;if ground_truth is not None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;supervision_loss = F.mse_loss(student_output, ground_truth)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;if teacher_output is not None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;distillation_loss = F.kl_div(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F.log_softmax(student_output, dim=-1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F.softmax(teacher_output, dim=-1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reduction='batchmean'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return lambda_distill * distillation_loss + (1 - lambda_distill) * supervision_loss<br>
<br>
// Inter-Model Reliability Analysis<br>
def compute_reliability(predictions_a, predictions_b, metric='kappa'):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;Compute reliability between two prediction sets<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;if metric == 'kappa':<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return cohen_kappa_score(predictions_a, predictions_b)<br>
&nbsp;&nbsp;&nbsp;&nbsp;elif metric == 'ccc':<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Concordance Correlation Coefficient<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mean_a, mean_b = np.mean(predictions_a), np.mean(predictions_b)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var_a, var_b = np.var(predictions_a), np.var(predictions_b)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cov = np.cov(predictions_a, predictions_b)[0,1]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return 2 * cov / (var_a + var_b + (mean_a - mean_b)**2)<br>
<br>
// Multitask Affect Embedding with PCA<br>
class AffectEmbedding:<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, n_components=2):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.pca = PCA(n_components=n_components)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.fitted = False<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def fit_transform(self, affect_features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affect_features: (n_samples, n_features) containing valence, arousal,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emotion probs, AU predictions from multiple models<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Normalize within-subjects<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affect_features = self._normalize_within_subjects(affect_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Fit PCA<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedded_features = self.pca.fit_transform(affect_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.fitted = True<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return embedded_features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def transform(self, affect_features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if not self.fitted:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise ValueError("PCA must be fitted first")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affect_features = self._normalize_within_subjects(affect_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.pca.transform(affect_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _normalize_within_subjects(self, features):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Within-subject normalization (simplified)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for i in range(features.shape[1]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;feature_col = features[:, i]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features[:, i] = (feature_col - np.mean(feature_col)) / (np.std(feature_col) + 1e-8)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return features<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
