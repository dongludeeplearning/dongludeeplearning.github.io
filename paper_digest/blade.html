<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ CVPR2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Shengze Wang, Jiefeng Li, Tianye Li, Ye Yuan, Henry Fuchs, Koki Nagano, Shalini De Mello, Michael Stengel<br>
        <span style="opacity:0.8">UNC, NVIDIA</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we accurately recover human body mesh and camera parameters from a single in-the-wild image, especially for close-range images with strong perspective distortion, by accurately estimating the person's depth (T_z) without heuristic assumptions?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•é€šè¿‡å‡†ç¡®ä¼°è®¡äººä½“æ·±åº¦ï¼ˆT_zï¼‰è€Œæ— éœ€å¯å‘å¼å‡è®¾ï¼Œä»å•å¼ é‡å¤–å›¾åƒä¸­å‡†ç¡®æ¢å¤äººä½“ç½‘æ ¼å’Œç›¸æœºå‚æ•°ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¼ºé€è§†å¤±çœŸçš„è¿‘è·ç¦»å›¾åƒï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Accurate Depth Estimation:</b> Introduced BLADE, the first method to accurately recover perspective projection parameters from a single image without heuristic assumptions, by showing that perspective distortion is driven by the person's Z-translation T_z which can be reliably estimated from the input image.</div>
            <div class="lang-zh" style="display:none"><b>å‡†ç¡®æ·±åº¦ä¼°è®¡ï¼š</b>å¼•å…¥BLADEï¼Œé¦–ä¸ªæ— éœ€å¯å‘å¼å‡è®¾å³å¯ä»å•å¼ å›¾åƒå‡†ç¡®æ¢å¤é€è§†æŠ•å½±å‚æ•°çš„æ–¹æ³•ï¼Œé€šè¿‡è¯æ˜é€è§†å¤±çœŸç”±äººä½“çš„Zå¹³ç§»T_zé©±åŠ¨ï¼Œè€ŒT_zå¯ä»¥ä»è¾“å…¥å›¾åƒå¯é ä¼°è®¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>T_z Conditioned Pose Estimation:</b> Demonstrated that conditioning the pose estimator on T_z significantly improves accuracy of estimated human mesh, especially for close-range images with strong perspective distortion, by leveraging the inverse relationship between perspective distortion and T_z.</div>
            <div class="lang-zh" style="display:none"><b>T_zæ¡ä»¶åŒ–å§¿æ€ä¼°è®¡ï¼š</b>è¯æ˜å°†å§¿æ€ä¼°è®¡å™¨æ¡ä»¶åŒ–äºT_zå¯æ˜¾è‘—æé«˜ä¼°è®¡äººä½“ç½‘æ ¼çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¼ºé€è§†å¤±çœŸçš„è¿‘è·ç¦»å›¾åƒï¼Œé€šè¿‡åˆ©ç”¨é€è§†å¤±çœŸä¸T_zä¹‹é—´çš„é€†å…³ç³»ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Complete Perspective Recovery:</b> Showed that once T_z and the 3D human mesh are estimated, one can accurately recover the focal length and full 3D translation (T_x, T_y, T_z), solving the long-standing challenge of simultaneous 3D pose accuracy and 2D alignment.</div>
            <div class="lang-zh" style="display:none"><b>å®Œæ•´é€è§†æ¢å¤ï¼š</b>è¯æ˜ä¸€æ—¦ä¼°è®¡å‡ºT_zå’Œ3Däººä½“ç½‘æ ¼ï¼Œå°±å¯ä»¥å‡†ç¡®æ¢å¤ç„¦è·å’Œå®Œæ•´3Då¹³ç§»ï¼ˆT_x, T_y, T_zï¼‰ï¼Œè§£å†³äº†åŒæ—¶å®ç°3Då§¿æ€å‡†ç¡®æ€§å’Œ2Då¯¹é½çš„é•¿æœŸæŒ‘æˆ˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Large-Scale Synthetic Dataset:</b> Contributed BEDLAM-CC, a new large-scale synthetic dataset with 2 million images tailored for close-range human mesh recovery with strong perspective distortion, helping models learn accurate Z-translation and 3D pose across a wide range of depths.</div>
            <div class="lang-zh" style="display:none"><b>å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼š</b>è´¡çŒ®BEDLAM-CCï¼Œä¸€ä¸ªåŒ…å«200ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå…·æœ‰å¼ºé€è§†å¤±çœŸçš„è¿‘è·ç¦»äººä½“ç½‘æ ¼æ¢å¤ï¼Œå¸®åŠ©æ¨¡å‹åœ¨å¹¿æ³›æ·±åº¦èŒƒå›´å†…å­¦ä¹ å‡†ç¡®çš„Zå¹³ç§»å’Œ3Då§¿æ€ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Ill-posed Problem:</b> Single-image human mesh recovery is under-constrained due to the need to simultaneously estimate body shape, pose, and camera parameters from a single view, leading to scale ambiguity and potentially infinite valid yet incorrect solutions.</div>
            <div class="lang-zh" style="display:none"><b>ç—…æ€é—®é¢˜ï¼š</b>å•å›¾åƒäººä½“ç½‘æ ¼æ¢å¤ç”±äºéœ€è¦ä»å•è§†è§’åŒæ—¶ä¼°è®¡äººä½“å½¢çŠ¶ã€å§¿æ€å’Œç›¸æœºå‚æ•°è€Œæ¬ çº¦æŸï¼Œå¯¼è‡´å°ºåº¦æ¨¡ç³Šå’Œæ½œåœ¨æ— é™çš„æœ‰æ•ˆä½†é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Perspective Distortion in Close-ups:</b> Existing methods assume near-orthographic projection for far-away subjects but break down as the person moves close to the camera, where strong perspective distortion makes it impossible to achieve both accurate 3D pose and 2D alignment simultaneously.</div>
            <div class="lang-zh" style="display:none"><b>è¿‘æ™¯é€è§†å¤±çœŸï¼š</b>ç°æœ‰æ–¹æ³•å‡è®¾è¿œè·ç¦»ä¸»ä½“ä¸ºè¿‘æ­£äº¤æŠ•å½±ï¼Œä½†å½“äººé è¿‘ç›¸æœºæ—¶ä¼šå¤±æ•ˆï¼Œå¼ºé€è§†å¤±çœŸä½¿å¾—æ— æ³•åŒæ—¶å®ç°å‡†ç¡®çš„3Då§¿æ€å’Œ2Då¯¹é½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Inaccurate Projection Assumptions:</b> Current methods rely on heuristic derivation of perspective projection from orthographic parameters, leading to inaccurate focal length and 3D translation estimation, especially for close-range images with varying depths.</div>
            <div class="lang-zh" style="display:none"><b>ä¸å‡†ç¡®çš„æŠ•å½±å‡è®¾ï¼š</b>å½“å‰æ–¹æ³•ä¾èµ–ä»æ­£äº¤å‚æ•°å¯å‘å¼æ¨å¯¼é€è§†æŠ•å½±ï¼Œå¯¼è‡´ä¸å‡†ç¡®çš„ç„¦è·å’Œ3Då¹³ç§»ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å˜åŒ–æ·±åº¦çš„è¿‘è·ç¦»å›¾åƒã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Lack of Close-range Training Data:</b> Existing labeled datasets for HMR lack close-range images with strong perspective distortion, making it difficult for models to learn accurate depth estimation and handle perspective effects in real-world scenarios like video conferencing.</div>
            <div class="lang-zh" style="display:none"><b>ç¼ºä¹è¿‘è·ç¦»è®­ç»ƒæ•°æ®ï¼š</b>ç°æœ‰çš„HMRæ ‡æ³¨æ•°æ®é›†ç¼ºä¹å…·æœ‰å¼ºé€è§†å¤±çœŸçš„è¿‘è·ç¦»å›¾åƒï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥å­¦ä¹ å‡†ç¡®çš„æ·±åº¦ä¼°è®¡å¹¶åœ¨è§†é¢‘ä¼šè®®ç­‰çœŸå®åœºæ™¯ä¸­å¤„ç†é€è§†æ•ˆæœã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>BLADE solves the perspective parameter recovery problem through accurate depth estimation:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Depth (T_z) Estimation:</b> Leverages the key insight that perspective distortion is mathematically driven by the distance between camera and person (T_z) but not affected by focal length. Trains a T_z estimator to predict the depth of the person's pelvis with respect to the camera using one-shot metrical depth estimators as inspiration.</li>
                <li style="margin-bottom:6px;"><b>T_z Conditioned Pose Estimation:</b> Conditions the pose estimator on T_z to improve accuracy of estimated human mesh, recognizing that human pose estimators predict 3D human mesh from images affected by perspective distortion which is determined by T_z.</li>
                <li style="margin-bottom:6px;"><b>Complete Perspective Recovery:</b> Once T_z and the 3D human mesh are estimated, accurately recovers the focal length f and remaining translation parameters T_x and T_y using the geometric relationship between 2D projections, 3D mesh points, and camera parameters.</li>
                <li style="margin-bottom:6px;"><b>Synthetic Data Augmentation:</b> Uses BEDLAM-CC, a large-scale synthetic dataset with 2 million images containing strong variation in lighting, camera angles, and extreme close-up distortion, to train models on diverse depth ranges and perspective scenarios.</li>
                <li style="margin-bottom:6px;"><b>End-to-End Training:</b> Trains the T_z estimator and pose estimator jointly, enabling the system to learn optimal depth-pose relationships without heuristic assumptions about perspective projection.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>BLADEé€šè¿‡å‡†ç¡®çš„æ·±åº¦ä¼°è®¡è§£å†³é€è§†å‚æ•°æ¢å¤é—®é¢˜ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ·±åº¦ï¼ˆT_zï¼‰ä¼°è®¡ï¼š</b>åˆ©ç”¨å…³é”®æ´å¯Ÿï¼Œå³é€è§†å¤±çœŸåœ¨æ•°å­¦ä¸Šç”±ç›¸æœºä¸äººçš„è·ç¦»ï¼ˆT_zï¼‰é©±åŠ¨ï¼Œä½†ä¸å—ç„¦è·å½±å“ã€‚è®­ç»ƒT_zä¼°è®¡å™¨é¢„æµ‹äººä½“éª¨ç›†ç›¸å¯¹äºç›¸æœºçš„æ·±åº¦ï¼Œä»¥ä¸€ shotsåº¦é‡æ·±åº¦ä¼°è®¡å™¨ä¸ºçµæ„Ÿã€‚</li>
                <li style="margin-bottom:6px;"><b>T_zæ¡ä»¶åŒ–å§¿æ€ä¼°è®¡ï¼š</b>å°†å§¿æ€ä¼°è®¡å™¨æ¡ä»¶åŒ–äºT_zä»¥æé«˜ä¼°è®¡äººä½“ç½‘æ ¼çš„å‡†ç¡®æ€§ï¼Œè®¤è¯†åˆ°äººä½“å§¿æ€ä¼°è®¡å™¨ä»å—é€è§†å¤±çœŸå½±å“çš„å›¾åƒé¢„æµ‹3Däººä½“ç½‘æ ¼ï¼Œè€Œé€è§†å¤±çœŸç”±T_zå†³å®šã€‚</li>
                <li style="margin-bottom:6px;"><b>å®Œæ•´é€è§†æ¢å¤ï¼š</b>ä¸€æ—¦ä¼°è®¡å‡ºT_zå’Œ3Däººä½“ç½‘æ ¼ï¼Œåˆ©ç”¨2DæŠ•å½±ã€3Dç½‘æ ¼ç‚¹å’Œç›¸æœºå‚æ•°ä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼Œå‡†ç¡®æ¢å¤ç„¦è·få’Œå‰©ä½™å¹³ç§»å‚æ•°T_xå’ŒT_yã€‚</li>
                <li style="margin-bottom:6px;"><b>åˆæˆæ•°æ®å¢å¼ºï¼š</b>ä½¿ç”¨BEDLAM-CCï¼Œä¸€ä¸ªåŒ…å«200ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå…·æœ‰å¼ºçƒˆçš„å…‰ç…§ã€ç›¸æœºè§’åº¦å’Œæç«¯è¿‘æ™¯å¤±çœŸå˜åŒ–ï¼Œåœ¨ä¸åŒæ·±åº¦èŒƒå›´å’Œé€è§†åœºæ™¯ä¸Šè®­ç»ƒæ¨¡å‹ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç«¯åˆ°ç«¯è®­ç»ƒï¼š</b>è”åˆè®­ç»ƒT_zä¼°è®¡å™¨å’Œå§¿æ€ä¼°è®¡å™¨ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿå­¦ä¹ æœ€ä¼˜æ·±åº¦-å§¿æ€å…³ç³»ï¼Œè€Œæ— éœ€å…³äºé€è§†æŠ•å½±çš„å¯å‘å¼å‡è®¾ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€å›¾ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
            <img src="Figures/blade_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" onerror="this.style.display='none'; this.parentElement.innerHTML='<div style=\'color:rgba(232,236,255,.5); text-align:center; padding:20px;\'>Intro figure not found. Please add Figures/blade_intro.png</div>'"/>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/blade_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" onerror="this.style.display='none'; this.parentElement.innerHTML='<div style=\'color:rgba(232,236,255,.5); text-align:center; padding:20px;\'>Overview figure not found. Please add Figures/blade_overview.png</div>'"/>
            <div style="height:1px; background:rgba(255,255,255,.10); margin:12px 0;"></div>
            <p>BLADE accurately recovers perspective parameters from a single image by estimating the person's depth (T_z), which drives perspective distortion. The method enables accurate human mesh and camera parameter estimation for single-view in-the-wild images including close-ups with high levels of perspective distortion.</p>
            <p>Note: The key insight is that perspective distortion is mathematically driven by T_z (distance between camera and person) but not affected by focal length. Once T_z is estimated, the pose estimator is conditioned on it to improve mesh accuracy, and then focal length and full 3D translation can be recovered.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('BLADE_arxiv.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('BLADE_arxiv.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>BLADE succeeds by accurately estimating depth and using it to constrain the ill-posed mesh recovery problem:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Depth-Perspective Relationship:</b> The key mathematical insight is that perspective distortion is driven by T_z (person's distance from camera) but not affected by focal length, allowing T_z to be disentangled from other variables and reliably estimated from image appearance.</li>
                <li style="margin-bottom:6px;"><b>Constraint Reduction:</b> By accurately estimating T_z first, the method reduces the number of unknowns in the under-constrained problem, making it easier to solve for body shape, pose, and remaining camera parameters.</li>
                <li style="margin-bottom:6px;"><b>Conditioned Pose Estimation:</b> Conditioning the pose estimator on T_z allows it to account for perspective distortion effects during mesh recovery, improving accuracy especially for close-range images where perspective effects are strong.</li>
                <li style="margin-bottom:6px;"><b>Geometric Consistency:</b> Once T_z and mesh are known, focal length and translation can be recovered using geometric relationships between 2D projections and 3D points, ensuring consistency between estimated parameters.</li>
                <li style="margin-bottom:6px;"><b>Synthetic Data Diversity:</b> The large-scale synthetic dataset provides diverse close-range scenarios with varying depths, lighting, and camera angles, enabling robust learning of depth estimation and perspective handling.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>BLADEé€šè¿‡å‡†ç¡®ä¼°è®¡æ·±åº¦å¹¶ä½¿ç”¨å®ƒæ¥çº¦æŸç—…æ€ç½‘æ ¼æ¢å¤é—®é¢˜è€ŒæˆåŠŸï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ·±åº¦-é€è§†å…³ç³»ï¼š</b>å…³é”®çš„æ•°å­¦æ´å¯Ÿæ˜¯é€è§†å¤±çœŸç”±T_zï¼ˆäººä¸ç›¸æœºçš„è·ç¦»ï¼‰é©±åŠ¨ï¼Œä½†ä¸å—ç„¦è·å½±å“ï¼Œå…è®¸T_zä»å…¶ä»–å˜é‡ä¸­è§£è€¦å¹¶ä»å›¾åƒå¤–è§‚å¯é ä¼°è®¡ã€‚</li>
                <li style="margin-bottom:6px;"><b>çº¦æŸå‡å°‘ï¼š</b>é€šè¿‡é¦–å…ˆå‡†ç¡®ä¼°è®¡T_zï¼Œè¯¥æ–¹æ³•å‡å°‘äº†æ¬ çº¦æŸé—®é¢˜ä¸­çš„æœªçŸ¥æ•°ï¼Œä½¿å¾—æ›´å®¹æ˜“æ±‚è§£äººä½“å½¢çŠ¶ã€å§¿æ€å’Œå‰©ä½™ç›¸æœºå‚æ•°ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¡ä»¶åŒ–å§¿æ€ä¼°è®¡ï¼š</b>å°†å§¿æ€ä¼°è®¡å™¨æ¡ä»¶åŒ–äºT_zå…è®¸å®ƒåœ¨ç½‘æ ¼æ¢å¤æœŸé—´è€ƒè™‘é€è§†å¤±çœŸæ•ˆåº”ï¼Œæé«˜å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé€è§†æ•ˆåº”å¼ºçƒˆçš„è¿‘è·ç¦»å›¾åƒã€‚</li>
                <li style="margin-bottom:6px;"><b>å‡ ä½•ä¸€è‡´æ€§ï¼š</b>ä¸€æ—¦T_zå’Œç½‘æ ¼å·²çŸ¥ï¼Œå¯ä»¥ä½¿ç”¨2DæŠ•å½±å’Œ3Dç‚¹ä¹‹é—´çš„å‡ ä½•å…³ç³»æ¢å¤ç„¦è·å’Œå¹³ç§»ï¼Œç¡®ä¿ä¼°è®¡å‚æ•°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</li>
                <li style="margin-bottom:6px;"><b>åˆæˆæ•°æ®å¤šæ ·æ€§ï¼š</b>å¤§è§„æ¨¡åˆæˆæ•°æ®é›†æä¾›å…·æœ‰ä¸åŒæ·±åº¦ã€å…‰ç…§å’Œç›¸æœºè§’åº¦çš„å¤šæ ·åŒ–è¿‘è·ç¦»åœºæ™¯ï¼Œå®ç°æ·±åº¦ä¼°è®¡å’Œé€è§†å¤„ç†çš„é²æ£’å­¦ä¹ ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This groundbreaking work introduces BLADE, the first method to accurately recover perspective projection parameters from a single in-the-wild image without heuristic assumptions. By recognizing that perspective distortion is mathematically driven by the person's Z-translation T_z rather than focal length, the method accurately estimates depth and uses it to constrain the ill-posed human mesh recovery problem. Through T_z conditioned pose estimation and geometric parameter recovery, BLADE achieves state-of-the-art accuracy on both 3D pose estimation and 2D alignment across a wide range of images, including challenging close-up scenarios with strong perspective distortion. The method's key contributionâ€”disentangling and accurately estimating T_z from image appearanceâ€”resolves the long-standing challenge of simultaneous accurate 3D pose and 2D alignment. Extensive experiments on standard benchmarks (SPEC-MTP, PDHUMAN, BEDLAM-CC, HUMMAN) and real-world close-range images demonstrate BLADE's superior performance over existing methods. The large-scale synthetic dataset BEDLAM-CC further enables robust learning across diverse depth ranges and perspective scenarios. This work opens new possibilities for applications requiring accurate human mesh recovery in challenging scenarios such as video conferencing and diverse in-the-wild image analysis, where perspective effects cannot be ignored.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å¼€åˆ›æ€§å·¥ä½œå¼•å…¥äº†BLADEï¼Œé¦–ä¸ªæ— éœ€å¯å‘å¼å‡è®¾å³å¯ä»å•å¼ é‡å¤–å›¾åƒå‡†ç¡®æ¢å¤é€è§†æŠ•å½±å‚æ•°çš„æ–¹æ³•ã€‚é€šè¿‡è®¤è¯†åˆ°é€è§†å¤±çœŸåœ¨æ•°å­¦ä¸Šç”±äººä½“çš„Zå¹³ç§»T_zè€Œéç„¦è·é©±åŠ¨ï¼Œè¯¥æ–¹æ³•å‡†ç¡®ä¼°è®¡æ·±åº¦å¹¶ä½¿ç”¨å®ƒæ¥çº¦æŸç—…æ€äººä½“ç½‘æ ¼æ¢å¤é—®é¢˜ã€‚é€šè¿‡T_zæ¡ä»¶åŒ–å§¿æ€ä¼°è®¡å’Œå‡ ä½•å‚æ•°æ¢å¤ï¼ŒBLADEåœ¨åŒ…æ‹¬å…·æœ‰å¼ºé€è§†å¤±çœŸçš„æŒ‘æˆ˜æ€§è¿‘æ™¯åœºæ™¯åœ¨å†…çš„å¹¿æ³›å›¾åƒä¸Šå®ç°äº†3Då§¿æ€ä¼°è®¡å’Œ2Då¯¹é½çš„æœ€å…ˆè¿›å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•çš„å…³é”®è´¡çŒ®â€”â€”ä»å›¾åƒå¤–è§‚ä¸­è§£è€¦å¹¶å‡†ç¡®ä¼°è®¡T_zâ€”â€”è§£å†³äº†åŒæ—¶å®ç°å‡†ç¡®3Då§¿æ€å’Œ2Då¯¹é½çš„é•¿æœŸæŒ‘æˆ˜ã€‚åœ¨æ ‡å‡†åŸºå‡†ï¼ˆSPEC-MTPã€PDHUMANã€BEDLAM-CCã€HUMMANï¼‰å’ŒçœŸå®ä¸–ç•Œè¿‘è·ç¦»å›¾åƒä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†BLADEç›¸å¯¹äºç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ã€‚å¤§è§„æ¨¡åˆæˆæ•°æ®é›†BEDLAM-CCè¿›ä¸€æ­¥å®ç°äº†è·¨ä¸åŒæ·±åº¦èŒƒå›´å’Œé€è§†åœºæ™¯çš„é²æ£’å­¦ä¹ ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨è§†é¢‘ä¼šè®®å’Œå¤šæ ·åŒ–é‡å¤–å›¾åƒåˆ†æç­‰æŒ‘æˆ˜æ€§åœºæ™¯ä¸­éœ€è¦å‡†ç¡®äººä½“ç½‘æ ¼æ¢å¤çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­é€è§†æ•ˆåº”ä¸å¯å¿½ç•¥ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://research.nvidia.com/labs/amri/projects/blade/media/BLADE_arxiv.pdf" target="_blank" style="color:#8bffcf;">BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation</a></p>
          <p><b>Project Page:</b> <a href="https://research.nvidia.com/labs/amri/projects/blade/" target="_blank" style="color:#8bffcf;">https://research.nvidia.com/labs/amri/projects/blade/</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# BLADE Implementation<br>
<br>
// T_z (depth) estimation from single image<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">estimate_tz</span>(image):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Estimate person's depth (pelvis Z-translation) from image"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Key insight: perspective distortion is driven by T_z, not focal length</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;depth_features = extract_depth_features(image)<br>
&nbsp;&nbsp;&nbsp;&nbsp;tz = tz_estimator(depth_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> tz<br>
<br>
// T_z conditioned pose estimation<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">estimate_mesh_conditioned</span>(image, tz):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Estimate 3D human mesh conditioned on T_z"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Account for perspective distortion effects</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;mesh_features = extract_mesh_features(image, tz)<br>
&nbsp;&nbsp;&nbsp;&nbsp;mesh_params = pose_estimator(mesh_features, tz)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> mesh_params<br>
<br>
// Complete perspective recovery<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">recover_perspective_params</span>(image, tz, mesh_3d):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Recover focal length and full 3D translation"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Project 3D mesh points to 2D</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;proj_2d = project_3d_to_2d(mesh_3d, tz, f=<span style="color:#ce9178;">None</span>, tx=<span style="color:#ce9178;">None</span>, ty=<span style="color:#ce9178;">None</span>)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Solve for f, T_x, T_y using geometric relationships</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;f, tx, ty = solve_camera_params(proj_2d, mesh_3d, tz)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> f, tx, ty, tz<br>
<br>
// Main pipeline<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">blade_pipeline</span>(image):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Complete BLADE pipeline for single-view mesh recovery"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Step 1: Estimate T_z</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;tz = estimate_tz(image)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Step 2: Estimate mesh conditioned on T_z</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;mesh_3d = estimate_mesh_conditioned(image, tz)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Step 3: Recover complete camera parameters</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;f, tx, ty, tz = recover_perspective_params(image, tz, mesh_3d)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> mesh_3d, {<span style="color:#9cdcfe;">'f'</span>: f, <span style="color:#9cdcfe;">'tx'</span>: tx, <span style="color:#9cdcfe;">'ty'</span>: ty, <span style="color:#9cdcfe;">'tz'</span>: tz}<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>

