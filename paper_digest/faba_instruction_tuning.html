<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Facial Affective Behavior Analysis with Instruction Tuning</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Yifan Li, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong<br>
        <span style="opacity:0.8">Arizona State University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we harness multi-modal large language models (MLLMs) for facial affective behavior analysis by creating instruction-following datasets and benchmarks that enable both discriminative recognition and generative reasoning?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œé¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºåˆ†æï¼Œé€šè¿‡åˆ›å»ºéµå¾ªæŒ‡ä»¤çš„æ•°æ®é›†å’ŒåŸºå‡†æ¥å®ç°åˆ¤åˆ«æ€§è¯†åˆ«å’Œç”Ÿæˆæ€§æ¨ç†ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>FABA-Instruct Dataset:</b> First instruction-following dataset for facial affective behavior analysis, containing 19K in-the-wild face images with 30K fine-grained emotion and AU annotations generated using GPT-4V.</div>
            <div class="lang-zh" style="display:none"><b>FABA-Instructæ•°æ®é›†ï¼š</b>é¦–ä¸ªç”¨äºé¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºåˆ†æçš„éµå¾ªæŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«19Kå¼ é‡å¤–é¢éƒ¨å›¾åƒå’Œ30Kä¸ªä½¿ç”¨GPT-4Vç”Ÿæˆçš„ç»†ç²’åº¦æƒ…æ„Ÿå’ŒAUæ³¨é‡Šã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>FABA-Bench Benchmark:</b> Novel benchmark with REGE metric that evaluates both recognition accuracy and generation quality for MLLMs on facial affective behavior tasks.</div>
            <div class="lang-zh" style="display:none"><b>FABA-BenchåŸºå‡†ï¼š</b>å…·æœ‰REGEæŒ‡æ ‡çš„æ–°å‹åŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMsåœ¨é¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºä»»åŠ¡ä¸Šçš„è¯†åˆ«å‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>EmoLA MLLM:</b> Efficient MLLM for FABA tasks combining LoRA parameter-efficient fine-tuning with facial prior expert module that captures structural facial information complementary to CLIP vision features.</div>
            <div class="lang-zh" style="display:none"><b>EmoLA MLLMï¼š</b>ç”¨äºFABAä»»åŠ¡çš„é«˜æ•ˆMLLMï¼Œå°†LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒä¸é¢éƒ¨å…ˆéªŒä¸“å®¶æ¨¡å—ç›¸ç»“åˆï¼Œåè€…æ•è·ä¸CLIPè§†è§‰ç‰¹å¾äº’è¡¥çš„é¢éƒ¨ç»“æ„ä¿¡æ¯ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>State-of-the-Art Performance:</b> EmoLA achieves superior results on FABA-Bench and competitive or better performance than task-specific SOTAs on traditional FER and AUR datasets.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›æ€§èƒ½ï¼š</b>EmoLAåœ¨FABA-Benchä¸Šå–å¾—äº†å“è¶Šç»“æœï¼Œå¹¶åœ¨ä¼ ç»ŸFERå’ŒAURæ•°æ®é›†ä¸Šå–å¾—äº†ä¸ä»»åŠ¡ç‰¹å®šSOTAç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Lack of Instruction-Following Data:</b> Traditional FABA datasets lack instruction-following annotations needed for MLLM fine-tuning, with only coarse-grained categorical labels insufficient for generative reasoning tasks.</div>
            <div class="lang-zh" style="display:none"><b>ç¼ºä¹éµå¾ªæŒ‡ä»¤çš„æ•°æ®ï¼š</b>ä¼ ç»ŸFABAæ•°æ®é›†ç¼ºä¹MLLMå¾®è°ƒæ‰€éœ€çš„éµå¾ªæŒ‡ä»¤æ³¨é‡Šï¼Œåªæœ‰ç²—ç²’åº¦åˆ†ç±»æ ‡ç­¾ä¸è¶³ä»¥ç”¨äºç”Ÿæˆæ€§æ¨ç†ä»»åŠ¡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>MLLM Evaluation Metrics:</b> Standard MLLM benchmarks focus on general visual understanding but lack specific metrics for FABA tasks that assess both discriminative recognition and generative reasoning capabilities.</div>
            <div class="lang-zh" style="display:none"><b>MLLMè¯„ä¼°æŒ‡æ ‡ï¼š</b>æ ‡å‡†MLLMåŸºå‡†å…³æ³¨ä¸€èˆ¬è§†è§‰ç†è§£ï¼Œä½†ç¼ºä¹é’ˆå¯¹FABAä»»åŠ¡çš„å…·ä½“æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡è¯„ä¼°åˆ¤åˆ«æ€§è¯†åˆ«å’Œç”Ÿæˆæ€§æ¨ç†èƒ½åŠ›ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Facial Prior Knowledge:</b> Pre-trained CLIP vision encoders may overlook task-specific facial structural information needed for affective behavior analysis, requiring additional facial prior knowledge integration.</div>
            <div class="lang-zh" style="display:none"><b>é¢éƒ¨å…ˆéªŒçŸ¥è¯†ï¼š</b>é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å¯èƒ½å¿½ç•¥æƒ…æ„Ÿè¡Œä¸ºåˆ†ææ‰€éœ€çš„ä»»åŠ¡ç‰¹å®šé¢éƒ¨ç»“æ„ä¿¡æ¯ï¼Œéœ€è¦é¢å¤–é¢éƒ¨å…ˆéªŒçŸ¥è¯†æ•´åˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Computational Efficiency:</b> Fine-tuning entire MLLMs with billions of parameters for domain-specific tasks leads to prohibitive computational costs, necessitating parameter-efficient adaptation methods.</div>
            <div class="lang-zh" style="display:none"><b>è®¡ç®—æ•ˆç‡ï¼š</b>ä¸ºç‰¹å®šé¢†åŸŸä»»åŠ¡å¾®è°ƒå…·æœ‰æ•°åäº¿å‚æ•°çš„æ•´ä¸ªMLLMä¼šå¯¼è‡´ä»¤äººæœ›è€Œå´æ­¥çš„è®¡ç®—æˆæœ¬ï¼Œéœ€è¦å‚æ•°é«˜æ•ˆçš„è‡ªé€‚åº”æ–¹æ³•ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The EmoLA framework introduces a comprehensive approach for instruction-tuned facial affective behavior analysis:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>FABA-Instruct Dataset Construction:</b> Uses GPT-4V to generate fine-grained emotion and AU descriptions from in-the-wild face images, creating instruction-response pairs for emotion recognition and AU detection tasks.</li>
                <li style="margin-bottom:6px;"><b>FABA-Bench with REGE Metric:</b> Evaluates MLLMs using a combined metric that assesses both recognition accuracy (F1-score for AUR, accuracy for FER) and generation quality (ROUGE scores for textual descriptions).</li>
                <li style="margin-bottom:6px;"><b>EmoLA Architecture:</b> Builds upon LLaVA-1.5 with additional facial prior expert that extracts landmark features using pre-trained face alignment model, integrated via MLP projector into the multimodal token space.</li>
                <li style="margin-bottom:6px;"><b>Parameter-Efficient Fine-Tuning:</b> Employs LoRA for efficient adaptation of LLM parameters while keeping vision encoders and most LLM weights frozen, enabling domain-specific learning without catastrophic forgetting.</li>
                <li style="margin-bottom:6px;"><b>Instruction-Tuned Inference:</b> Generates detailed affective behavior descriptions by following natural language instructions, providing both categorical predictions and explanatory reasoning about facial movements.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>EmoLAæ¡†æ¶å¼•å…¥äº†ä¸€ç§å…¨é¢çš„æ–¹æ³•ï¼Œç”¨äºæŒ‡ä»¤è°ƒä¼˜çš„é¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºåˆ†æï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>FABA-Instructæ•°æ®é›†æ„å»ºï¼š</b>ä½¿ç”¨GPT-4Vä»é‡å¤–é¢éƒ¨å›¾åƒç”Ÿæˆç»†ç²’åº¦çš„æƒ…æ„Ÿå’ŒAUæè¿°ï¼Œä¸ºæƒ…æ„Ÿè¯†åˆ«å’ŒAUæ£€æµ‹ä»»åŠ¡åˆ›å»ºæŒ‡ä»¤-å“åº”é…å¯¹ã€‚</li>
                <li style="margin-bottom:6px;"><b>å…·æœ‰REGEæŒ‡æ ‡çš„FABA-Benchï¼š</b>ä½¿ç”¨ç»„åˆæŒ‡æ ‡è¯„ä¼°MLLMsï¼Œè¯¥æŒ‡æ ‡è¯„ä¼°è¯†åˆ«å‡†ç¡®æ€§ï¼ˆAURçš„F1åˆ†æ•°ï¼ŒFERçš„å‡†ç¡®æ€§ï¼‰å’Œç”Ÿæˆè´¨é‡ï¼ˆæ–‡æœ¬æè¿°çš„ROUGEåˆ†æ•°ï¼‰ã€‚</li>
                <li style="margin-bottom:6px;"><b>EmoLAæ¶æ„ï¼š</b>åŸºäºLLaVA-1.5æ„å»ºï¼Œå…·æœ‰é¢å¤–çš„é¢éƒ¨å…ˆéªŒä¸“å®¶ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„é¢éƒ¨å¯¹é½æ¨¡å‹æå–åœ°æ ‡ç‰¹å¾ï¼Œå¹¶é€šè¿‡MLPæŠ•å½±å™¨é›†æˆåˆ°å¤šæ¨¡æ€ä»¤ç‰Œç©ºé—´ä¸­ã€‚</li>
                <li style="margin-bottom:6px;"><b>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼š</b>é‡‡ç”¨LoRAé«˜æ•ˆé€‚åº”LLMå‚æ•°ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ç¼–ç å™¨å’Œå¤§å¤šæ•°LLMæƒé‡å†»ç»“ï¼Œå®ç°ç‰¹å®šé¢†åŸŸå­¦ä¹ è€Œä¸ä¼šå‡ºç°ç¾éš¾æ€§é—å¿˜ã€‚</li>
                <li style="margin-bottom:6px;"><b>æŒ‡ä»¤è°ƒä¼˜æ¨ç†ï¼š</b>é€šè¿‡éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆè¯¦ç»†çš„æƒ…æ„Ÿè¡Œä¸ºæè¿°ï¼Œæä¾›åˆ†ç±»é¢„æµ‹å’Œé¢éƒ¨è¿åŠ¨çš„è§£é‡Šæ€§æ¨ç†ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/faba_instruction_tuning_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
            <p style="margin-top:12px;">FABA-Instruct annotations showing fine-grained emotion and AU descriptions with reasoning about facial movements and emotional interpretation.</p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/faba_instruction_tuning_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_faba_instruction_tuning.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_faba_instruction_tuning.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of EmoLA for facial affective behavior analysis stems from its innovative integration of multiple complementary components:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Instruction-Following Data Quality:</b> GPT-4V-generated annotations provide richer semantic understanding compared to traditional categorical labels, enabling MLLMs to learn nuanced facial behavior interpretations beyond simple classification.</li>
                <li style="margin-bottom:6px;"><b>Multi-Modal Token Integration:</b> Combining CLIP vision features with facial landmark priors creates a more comprehensive representation that captures both general visual semantics and domain-specific facial structure information.</li>
                <li style="margin-bottom:6px;"><b>Parameter-Efficient Adaptation:</b> LoRA enables fine-tuning large language models for specific domains without the computational burden of full model updates, preserving general capabilities while adding domain expertise.</li>
                <li style="margin-bottom:6px;"><b>Generative Reasoning Capability:</b> Unlike discriminative models, MLLMs can generate detailed explanations and reasoning about facial movements, providing interpretable insights that traditional classification approaches cannot offer.</li>
                <li style="margin-bottom:6px;"><b>Unified Evaluation Framework:</b> The REGE metric ensures balanced assessment of both recognition accuracy and generation quality, preventing models that excel in one aspect but fail in others from being overlooked.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>EmoLAåœ¨é¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºåˆ†ææ–¹é¢çš„æˆåŠŸæºäºå…¶åˆ›æ–°çš„å¤šç»„ä»¶äº’è¡¥é›†æˆï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>éµå¾ªæŒ‡ä»¤æ•°æ®è´¨é‡ï¼š</b>GPT-4Vç”Ÿæˆçš„æ³¨é‡Šä¸ä¼ ç»Ÿåˆ†ç±»æ ‡ç­¾ç›¸æ¯”æä¾›äº†æ›´ä¸°å¯Œçš„è¯­ä¹‰ç†è§£ï¼Œä½¿MLLMsèƒ½å¤Ÿå­¦ä¹ è¶…è¶Šç®€å•åˆ†ç±»çš„é¢éƒ¨è¡Œä¸ºç»†å¾®è§£é‡Šã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šæ¨¡æ€ä»¤ç‰Œé›†æˆï¼š</b>å°†CLIPè§†è§‰ç‰¹å¾ä¸é¢éƒ¨åœ°æ ‡å…ˆéªŒç›¸ç»“åˆï¼Œåˆ›å»ºæ›´å…¨é¢çš„è¡¨ç¤ºï¼Œæ—¢æ•è·ä¸€èˆ¬è§†è§‰è¯­ä¹‰ï¼Œåˆæ•è·ç‰¹å®šé¢†åŸŸé¢éƒ¨ç»“æ„ä¿¡æ¯ã€‚</li>
                <li style="margin-bottom:6px;"><b>å‚æ•°é«˜æ•ˆé€‚åº”ï¼š</b>LoRAå®ç°äº†ä¸ºç‰¹å®šé¢†åŸŸå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè€Œæ— éœ€å®Œæ•´æ¨¡å‹æ›´æ–°çš„è®¡ç®—è´Ÿæ‹…ï¼Œåœ¨ä¿ç•™ä¸€èˆ¬èƒ½åŠ›çš„åŒæ—¶æ·»åŠ é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç”Ÿæˆæ€§æ¨ç†èƒ½åŠ›ï¼š</b>ä¸åˆ¤åˆ«æ€§æ¨¡å‹ä¸åŒï¼ŒMLLMså¯ä»¥ç”Ÿæˆå…³äºé¢éƒ¨è¿åŠ¨çš„è¯¦ç»†è§£é‡Šå’Œæ¨ç†ï¼Œæä¾›ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•æ— æ³•æä¾›çš„å¯è§£é‡Šè§è§£ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼š</b>REGEæŒ‡æ ‡ç¡®ä¿å¯¹è¯†åˆ«å‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡çš„å¹³è¡¡è¯„ä¼°ï¼Œé˜²æ­¢æ“…é•¿ä¸€ä¸ªæ–¹é¢ä½†åœ¨å…¶ä»–æ–¹é¢å¤±è´¥çš„æ¨¡å‹è¢«å¿½è§†ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This work pioneers the application of instruction tuning to facial affective behavior analysis, establishing foundational infrastructure that bridges the gap between general-purpose MLLMs and domain-specific affective computing. By introducing FABA-Instruct, the first instruction-following dataset for facial emotion and AU analysis, and FABA-Bench with the innovative REGE evaluation metric, the paper provides a comprehensive framework for assessing both recognition and generation capabilities of MLLMs. The EmoLA model demonstrates how facial prior knowledge and parameter-efficient fine-tuning can enhance MLLM performance on FABA tasks, achieving state-of-the-art results while maintaining computational efficiency. This work opens new avenues for affective computing by showing how instruction-tuned MLLMs can provide detailed, interpretable analysis of facial behaviors, going beyond traditional discriminative approaches to enable generative reasoning about emotional states and facial movements.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å·¥ä½œå¼€åˆ›äº†æŒ‡ä»¤è°ƒä¼˜åœ¨é¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºåˆ†æä¸­çš„åº”ç”¨ï¼Œå»ºç«‹åŸºç¡€æ¶æ„ï¼Œå¼¥åˆé€šç”¨MLLMsä¸ç‰¹å®šé¢†åŸŸæƒ…æ„Ÿè®¡ç®—ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡å¼•å…¥FABA-Instructï¼ˆé¦–ä¸ªç”¨äºé¢éƒ¨æƒ…æ„Ÿå’ŒAUåˆ†æçš„éµå¾ªæŒ‡ä»¤æ•°æ®é›†ï¼‰å’Œå…·æœ‰åˆ›æ–°REGEè¯„ä¼°æŒ‡æ ‡çš„FABA-Benchï¼Œæœ¬æ–‡æä¾›äº†å…¨é¢æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°MLLMsçš„è¯†åˆ«å’Œç”Ÿæˆèƒ½åŠ›ã€‚EmoLAæ¨¡å‹å±•ç¤ºäº†é¢éƒ¨å…ˆéªŒçŸ¥è¯†å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒå¦‚ä½•å¢å¼ºMLLMsåœ¨FABAä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å±•ç¤ºæŒ‡ä»¤è°ƒä¼˜çš„MLLMså¦‚ä½•æä¾›é¢éƒ¨è¡Œä¸ºçš„è¯¦ç»†ã€å¯è§£é‡Šåˆ†æï¼Œå¼€è¾Ÿäº†æƒ…æ„Ÿè®¡ç®—çš„æ–°é€”å¾„ï¼Œè¶…è¶Šä¼ ç»Ÿåˆ¤åˆ«æ€§æ–¹æ³•ï¼Œå®ç°å¯¹æƒ…ç»ªçŠ¶æ€å’Œé¢éƒ¨è¿åŠ¨çš„ç”Ÿæˆæ€§æ¨ç†ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2404.05052" style="color:#8bffcf;">2404.05052</a></p>
          <p><b>Project Page:</b> <a href="https://johnx69.github.io/FABA/" style="color:#8bffcf;">https://johnx69.github.io/FABA/</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import torch<br>
import torch.nn as nn<br>
from transformers import LlamaForCausalLM, CLIPVisionModel, CLIPImageProcessor<br>
from peft import LoraConfig, get_peft_model<br>
<br>
// EmoLA: Facial Affective Behavior Analysis with Instruction Tuning<br>
class FacialPriorExpert(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, landmark_dim=478):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(FacialPriorExpert, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Pre-trained face alignment model for landmark extraction<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.face_alignment = InsightFaceLandmarkExtractor()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# MLP projector for landmark features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.landmark_projector = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(landmark_dim, 512),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(512, 256)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, image):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract facial landmarks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;landmarks = self.face_alignment(image)  # (batch_size, 478, 3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Flatten and project to token space<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;landmarks_flat = landmarks.view(landmarks.size(0), -1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prior_tokens = self.landmark_projector(landmarks_flat)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return prior_tokens<br>
<br>
// EmoLA Multi-modal Large Language Model<br>
class EmoLA(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, num_classes=7):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(EmoLA, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Vision components<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.vision_model = CLIPVisionModel.from_pretrained("openai/clip-vit-large-patch14")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.vision_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Facial prior expert<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.facial_prior = FacialPriorExpert()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Vision projectors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.vision_projector = nn.Linear(1024, 4096)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.prior_projector = nn.Linear(256, 4096)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Language model with LoRA<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.llm = LlamaForCausalLM.from_pretrained("lmsys/vicuna-7b-v1.5")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Configure LoRA<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lora_config = LoraConfig(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r=128,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lora_alpha=256,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;target_modules=["q_proj", "v_proj"],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lora_dropout=0.05,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias="none",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task_type="CAUSAL_LM"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.llm = get_peft_model(self.llm, lora_config)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, images, instructions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Process images<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_inputs = self.vision_processor(images, return_tensors="pt")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vision_outputs = self.vision_model(**image_inputs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vision_tokens = self.vision_projector(vision_outputs.last_hidden_state)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Process facial priors<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prior_tokens = self.facial_prior(images)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prior_tokens = self.prior_projector(prior_tokens)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Concatenate multimodal tokens<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;multimodal_tokens = torch.cat([vision_tokens, prior_tokens], dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Process text instructions<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_inputs = self.llm_tokenizer(instructions, return_tensors="pt")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Combine and generate<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined_inputs = {**text_inputs, "multimodal_tokens": multimodal_tokens}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = self.llm.generate(**combined_inputs, max_length=512)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return outputs<br>
<br>
// REGE Metric Calculation<br>
def calculate_rege_score(predictions, references, ground_truth_labels):<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;Calculate REGE score combining Recognition and GEneneration metrics<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;from rouge_score import rouge_scorer<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Recognition score (F1 for AU, Accuracy for emotion)<br>
&nbsp;&nbsp;&nbsp;&nbsp;recognition_score = calculate_recognition_metrics(predictions, ground_truth_labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Generation score (ROUGE)<br>
&nbsp;&nbsp;&nbsp;&nbsp;scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)<br>
&nbsp;&nbsp;&nbsp;&nbsp;generation_scores = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;for pred, ref in zip(predictions, references):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores = scorer.score(ref, pred)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generation_scores.append(scores['rougeL'].fmeasure)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;generation_score = sum(generation_scores) / len(generation_scores)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Combined REGE score<br>
&nbsp;&nbsp;&nbsp;&nbsp;rege_score = recognition_score + generation_score<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return rege_score<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
