<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2024
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Robotic Control via Embodied Chain-of-Thought Reasoning</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        MichaÅ‚ Zawalski, William Chen, Karl Pertsch <br>
        <span style="opacity:0.8">University of Warsaw, UC Berkeley</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to enable Vision-Language-Action (VLA) models to "think carefully" and "look carefully" before acting to improve generalization and robustness in robotic manipulation?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•è®©è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨è¡ŒåŠ¨å‰â€œä»”ç»†æ€è€ƒâ€å’Œâ€œä»”ç»†è§‚å¯Ÿâ€ï¼Œä»¥æé«˜æœºå™¨äººæ“ä½œçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Ÿ
        </div>
      </div>
    </div>
  
    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Embodied Chain-of-Thought (ECoT):</b> Introduced a reasoning framework for VLAs that interleaves semantic planning with grounded visual perception (e.g., bounding boxes, gripper positions).</div>
            <div class="lang-zh" style="display:none"><b>å…·èº«æ€ç»´é“¾ï¼ˆECoTï¼‰ï¼š</b>å¼•å…¥äº†ä¸€ç§ VLA æ¨ç†æ¡†æ¶ï¼Œå°†è¯­ä¹‰è§„åˆ’ä¸æ¥åœ°çš„è§†è§‰æ„ŸçŸ¥ï¼ˆä¾‹å¦‚è¾¹ç•Œæ¡†ã€å¤¹çˆªä½ç½®ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scalable Data Generation:</b> Designed a pipeline to synthesize ECoT training data for large robot datasets using pre-trained object detectors and LLMs.</div>
            <div class="lang-zh" style="display:none"><b>å¯æ‰©å±•æ•°æ®ç”Ÿæˆï¼š</b>è®¾è®¡äº†ä¸€ä¸ªæµç¨‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ç›®æ ‡æ£€æµ‹å™¨å’Œ LLM ä¸ºå¤§å‹æœºå™¨äººæ•°æ®é›†ç»¼åˆç”Ÿæˆ ECoT è®­ç»ƒæ•°æ®ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Performance Boost:</b> Demonstrated a 28% increase in success rate for OpenVLA on challenging generalization tasks without additional robot training data.</div>
            <div class="lang-zh" style="display:none"><b>æ€§èƒ½æå‡ï¼š</b>åœ¨æ²¡æœ‰é¢å¤–æœºå™¨äººè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒOpenVLA åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æé«˜äº† 28%ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/ecot_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Weak Reasoning in Small VLMs:</b> Open-source VLMs used in robotics are often smaller and less capable of complex reasoning compared to proprietary giants like GPT-4.</div>
            <div class="lang-zh" style="display:none"><b>å°å‹ VLM çš„æ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼š</b>ä¸ GPT-4 ç­‰ä¸“æœ‰å·¨å¤´ç›¸æ¯”ï¼Œæœºå™¨äººæŠ€æœ¯ä¸­ä½¿ç”¨çš„å¼€æº VLM é€šå¸¸è¾ƒå°ï¼Œæ‰§è¡Œå¤æ‚æ¨ç†çš„èƒ½åŠ›è¾ƒå¼±ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Grounding Requirement:</b> Purely semantic CoT is insufficient for robotics; reasoning must be grounded in physical observations (spatial locations, object states).</div>
            <div class="lang-zh" style="display:none"><b>æ¥åœ°éœ€æ±‚ï¼š</b>çº¯è¯­ä¹‰ CoT å¯¹æœºå™¨äººæŠ€æœ¯æ¥è¯´æ˜¯ä¸å¤Ÿçš„ï¼›æ¨ç†å¿…é¡»å»ºç«‹åœ¨ç‰©ç†è§‚å¯Ÿï¼ˆç©ºé—´ä½ç½®ã€ç‰©ä½“çŠ¶æ€ï¼‰çš„åŸºç¡€ä¸Šã€‚</div>
          </li>
        </ul>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Reasoning Structure:</b> Predicts a sequence: Task â†’ Plan â†’ Sub-task â†’ Sub-task Reason â†’ Move â†’ Move Reason â†’ Action.</div>
            <div class="lang-zh" style="display:none"><b>æ¨ç†ç»“æ„ï¼š</b>é¢„æµ‹åºåˆ—ï¼šä»»åŠ¡ â†’ è®¡åˆ’ â†’ å­ä»»åŠ¡ â†’ å­ä»»åŠ¡ç†ç”± â†’ ç§»åŠ¨ â†’ ç§»åŠ¨ç†ç”± â†’ åŠ¨ä½œã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Grounded Features:</b> Explicitly reasons about "visible objects" (bounding boxes) and "gripper position" to force the model to attend to visual inputs.</div>
            <div class="lang-zh" style="display:none"><b>æ¥åœ°ç‰¹å¾ï¼š</b>æ˜ç¡®æ¨ç†â€œå¯è§ç‰©ä½“â€ï¼ˆè¾¹ç•Œæ¡†ï¼‰å’Œâ€œå¤¹çˆªä½ç½®â€ï¼Œä»¥è¿«ä½¿æ¨¡å‹å…³æ³¨è§†è§‰è¾“å…¥ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Synthetic Data Pipeline:</b> Uses Gemini 1.5 Pro to generate reasoning chains based on trajectory data, annotated object detections (Owl-ViT), and task descriptions.</div>
            <div class="lang-zh" style="display:none"><b>åˆæˆæ•°æ®æµç¨‹ï¼š</b>ä½¿ç”¨ Gemini 1.5 Pro åŸºäºè½¨è¿¹æ•°æ®ã€å¸¦æ³¨é‡Šçš„ç›®æ ‡æ£€æµ‹ï¼ˆOwl-ViTï¼‰å’Œä»»åŠ¡æè¿°ç”Ÿæˆæ¨ç†é“¾ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 12px;font-size:16px;">Detailed Explanation (åŸç†è¯¦è§£)</h2>
        
        <div style="margin-bottom:16px;">
            <h3 style="margin:0 0 8px;font-size:14px;color:#8bffcf;">Sec. 3: Vision-Language-Action Models (VLA åŸºç¡€)</h3>
            <div class="lang-en" style="color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                Standard VLAs (like OpenVLA) fine-tune pre-trained Vision-Language Models to map image observations and text instructions directly to robot actions. While powerful, they typically lack intermediate reasoning capabilities, jumping straight from input to action without "thinking" about the process.
            </div>
            <div class="lang-zh" style="display:none;color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                æ ‡å‡† VLAï¼ˆå¦‚ OpenVLAï¼‰å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå°†å›¾åƒè§‚å¯Ÿå’Œæ–‡æœ¬æŒ‡ä»¤ç›´æ¥æ˜ å°„åˆ°æœºå™¨äººåŠ¨ä½œã€‚è™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ä¸­é—´æ¨ç†èƒ½åŠ›ï¼Œç›´æ¥ä»è¾“å…¥è·³è½¬åˆ°åŠ¨ä½œï¼Œè€Œæ²¡æœ‰å¯¹è¿‡ç¨‹è¿›è¡Œâ€œæ€è€ƒâ€ã€‚
            </div>
        </div>

        <div style="margin-bottom:16px;">
            <h3 style="margin:0 0 8px;font-size:14px;color:#8bffcf;">Sec. 4.1: Designing Reasoning Steps (æ¨ç†æ­¥éª¤è®¾è®¡)</h3>
            <div class="lang-en" style="color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                ECoT introduces a sequence of reasoning steps to bridge the gap between high-level instruction and low-level control:
                <ul style="margin:8px 0 0 16px; padding:0;">
                    <li><b>High-Level:</b> Rephrases the <b>TASK</b> and generates a <b>PLAN</b> of steps.</li>
                    <li><b>Mid-Level:</b> Identifies the current <b>SUBTASK</b> based on the scene state.</li>
                    <li><b>Low-Level:</b> Predicts a <b>MOVE</b> command (e.g., "move left") to guide execution.</li>
                    <li><b>Grounded:</b> Outputs <b>GRIPPER</b> position and <b>OBJECT</b> bounding boxes to force attention to visual details.</li>
                </ul>
            </div>
            <div class="lang-zh" style="display:none;color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                ECoT å¼•å…¥äº†ä¸€ç³»åˆ—æ¨ç†æ­¥éª¤æ¥å¼¥åˆé«˜å±‚æŒ‡ä»¤ä¸ä½å±‚æ§åˆ¶ä¹‹é—´çš„å·®è·ï¼š
                <ul style="margin:8px 0 0 16px; padding:0;">
                    <li><b>é«˜å±‚ï¼š</b>é‡è¿°<b>ä»»åŠ¡ (TASK)</b> å¹¶ç”Ÿæˆæ­¥éª¤<b>è®¡åˆ’ (PLAN)</b>ã€‚</li>
                    <li><b>ä¸­å±‚ï¼š</b>æ ¹æ®åœºæ™¯çŠ¶æ€è¯†åˆ«å½“å‰çš„<b>å­ä»»åŠ¡ (SUBTASK)</b>ã€‚</li>
                    <li><b>ä½å±‚ï¼š</b>é¢„æµ‹<b>ç§»åŠ¨ (MOVE)</b> æŒ‡ä»¤ï¼ˆä¾‹å¦‚â€œå‘å·¦ç§»åŠ¨â€ï¼‰ä»¥æŒ‡å¯¼æ‰§è¡Œã€‚</li>
                    <li><b>æ¥åœ°ï¼š</b>è¾“å‡º<b>å¤¹çˆª (GRIPPER)</b> ä½ç½®å’Œ<b>ç‰©ä½“ (OBJECT)</b> è¾¹ç•Œæ¡†ï¼Œè¿«ä½¿æ¨¡å‹å…³æ³¨è§†è§‰ç»†èŠ‚ã€‚</li>
                </ul>
            </div>
        </div>

        <div style="margin-bottom:16px;">
            <h3 style="margin:0 0 8px;font-size:14px;color:#8bffcf;">Sec. 4.2: Scalable Data Generation (å¯æ‰©å±•æ•°æ®ç”Ÿæˆ)</h3>
            <div class="lang-en" style="color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                To avoid expensive human annotation, ECoT uses a synthetic pipeline:
                <ul style="margin:8px 0 0 16px; padding:0;">
                    <li><b>Prismatic VLM:</b> Generates detailed scene descriptions.</li>
                    <li><b>Grounding DINO & SAM:</b> Detects objects and gripper positions.</li>
                    <li><b>Proprioception:</b> Converts robot state to "move" primitives.</li>
                    <li><b>Gemini LLM:</b> Synthesizes the full reasoning chain (Plan, Subtasks) based on all above inputs.</li>
                </ul>
            </div>
            <div class="lang-zh" style="display:none;color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                ä¸ºé¿å…æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼ŒECoT ä½¿ç”¨åˆæˆæ•°æ®æµç¨‹ï¼š
                <ul style="margin:8px 0 0 16px; padding:0;">
                    <li><b>Prismatic VLMï¼š</b>ç”Ÿæˆè¯¦ç»†çš„åœºæ™¯æè¿°ã€‚</li>
                    <li><b>Grounding DINO & SAMï¼š</b>æ£€æµ‹ç‰©ä½“å’Œå¤¹çˆªä½ç½®ã€‚</li>
                    <li><b>æœ¬ä½“æ„ŸçŸ¥ï¼š</b>å°†æœºå™¨äººçŠ¶æ€è½¬æ¢ä¸ºâ€œç§»åŠ¨â€åŸºå…ƒã€‚</li>
                    <li><b>Gemini LLMï¼š</b>åŸºäºä»¥ä¸Šæ‰€æœ‰è¾“å…¥åˆæˆå®Œæ•´çš„æ¨ç†é“¾ï¼ˆè®¡åˆ’ã€å­ä»»åŠ¡ï¼‰ã€‚</li>
                </ul>
            </div>
        </div>

        <div>
            <h3 style="margin:0 0 8px;font-size:14px;color:#8bffcf;">Sec. 4.3: Efficient Inference (é«˜æ•ˆæ¨ç†)</h3>
            <div class="lang-en" style="color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                Generating long reasoning chains can slow down control. ECoT addresses this by re-using high-level reasoning (Plan/Subtask) for multiple steps (N-step freeze) or running reasoning asynchronously in a separate process, maintaining high control frequency.
            </div>
            <div class="lang-zh" style="display:none;color:rgba(232,236,255,.80);font-size:13px;line-height:1.6;">
                ç”Ÿæˆé•¿æ¨ç†é“¾ä¼šé™ä½æ§åˆ¶é€Ÿåº¦ã€‚ECoT é€šè¿‡åœ¨å¤šä¸ªæ­¥éª¤ä¸­é‡ç”¨é«˜å±‚æ¨ç†ï¼ˆè®¡åˆ’/å­ä»»åŠ¡ï¼‰ï¼ˆNæ­¥å†»ç»“ï¼‰æˆ–åœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­å¼‚æ­¥è¿è¡Œæ¨ç†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œä¿æŒé«˜æ§åˆ¶é¢‘ç‡ã€‚
            </div>
        </div>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figuresï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <!-- Stack of 5 Overview Figures -->
            <div style="display:flex; flex-direction:column; gap:20px;">
                <img src="Figures/ecot_overview01.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure 1" />
                <img src="Figures/ecot_overview02.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure 2" />
                <img src="Figures/ecot_overview03.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure 3" />
                <img src="Figures/ecot_overview04.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure 4" />
                <img src="Figures/ecot_overview05.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure 5" />
            </div>
            
            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2407.08693.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2407.08693.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                Embodied Chain-of-Thought (ECoT) significantly enhances VLA generalization by integrating semantic reasoning with grounded perception.
            </div>
            <div class="lang-zh" style="display:none">
                å…·èº«æ€ç»´é“¾ï¼ˆECoTï¼‰é€šè¿‡å°†è¯­ä¹‰æ¨ç†ä¸æ¥åœ°æ„ŸçŸ¥ç›¸ç»“åˆï¼Œæ˜¾ç€å¢å¼ºäº† VLA çš„æ³›åŒ–èƒ½åŠ›ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                The approach also enables human interpretability and correction of robot policies through natural language, paving the way for more robust and interactive robotic agents.
            </div>
            <div class="lang-zh" style="display:none">
                è¯¥æ–¹æ³•è¿˜å…è®¸äººç±»é€šè¿‡è‡ªç„¶è¯­è¨€è§£é‡Šå’Œçº æ­£æœºå™¨äººç­–ç•¥ï¼Œä¸ºæ›´å¼ºå¤§å’Œäº¤äº’å¼çš„æœºå™¨äººæ™ºèƒ½ä½“é“ºå¹³äº†é“è·¯ã€‚
            </div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://github.com/MichalZawalski/embodied-CoT" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          <svg height="16" viewBox="0 0 16 16" width="16" style="fill:currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
          GitHub Repo
        </a>
      </div>
      
      <div style="margin-top:16px; padding:16px; border:1px solid rgba(255,255,255,.10); border-radius:16px; background:rgba(0,0,0,.10);">
        <h2 style="margin:0 0 12px; font-size:16px;">Core Code Logic (æ ¸å¿ƒä»£ç é€»è¾‘)</h2>
        <div style="font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,monospace; font-size:13px; color:rgba(232,236,255,.8); line-height:1.5; overflow-x:auto;">
            <div style="margin-bottom:12px;">
                <div class="lang-en">// 1. Load Pre-trained ECoT Model (Based on OpenVLA)</div>
                <div class="lang-zh" style="display:none">// 1. åŠ è½½é¢„è®­ç»ƒ ECoT æ¨¡å‹ (åŸºäº OpenVLA)</div>
                <span style="color:#c792ea;">from</span> transformers <span style="color:#c792ea;">import</span> AutoModelForVision2Seq, AutoProcessor<br><br>
                path = <span style="color:#c3e88d;">"Embodied-CoT/ecot-openvla-7b-bridge"</span><br>
                processor = AutoProcessor.from_pretrained(path)<br>
                <span style="color:#5c6370;"># Load model in bfloat16 for efficiency</span><br>
                vla = AutoModelForVision2Seq.from_pretrained(path, torch_dtype=torch.bfloat16).to(<span style="color:#c3e88d;">"cuda"</span>)
            </div>

            <div style="margin-bottom:12px;">
                <div class="lang-en">// 2. Construct Prompt with Chain-of-Thought Trigger</div>
                <div class="lang-zh" style="display:none">// 2. æ„å»ºåŒ…å«æ€ç»´é“¾è§¦å‘è¯çš„æç¤º</div>
                prompt = (<br>
                &nbsp;&nbsp;<span style="color:#c3e88d;">"A chat between a curious user and an artificial intelligence assistant. "</span><br>
                &nbsp;&nbsp;<span style="color:#c3e88d;">"The assistant gives helpful, detailed, and polite answers to the user's questions. "</span><br>
                &nbsp;&nbsp;f<span style="color:#c3e88d;">"USER: What action should the robot take to {instruction}? "</span><br>
                &nbsp;&nbsp;<span style="color:#c3e88d;">"ASSISTANT: TASK:"</span>  <span style="color:#5c6370;"># Trigger keyword to start CoT generation</span><br>
                )
            </div>

            <div style="margin-bottom:12px;">
                <div class="lang-en">// 3. Multi-Modal Input Processing</div>
                <div class="lang-zh" style="display:none">// 3. å¤šæ¨¡æ€è¾“å…¥å¤„ç†</div>
                inputs = processor(prompt, image_observation).to(<span style="color:#c3e88d;">"cuda"</span>, dtype=torch.bfloat16)
            </div>

            <div>
                <div class="lang-en">// 4. Generate Reasoning Chain & Action</div>
                <div class="lang-zh" style="display:none">// 4. ç”Ÿæˆæ¨ç†é“¾ä¸åŠ¨ä½œ</div>
                <span style="color:#5c6370;"># Model predicts: Task -> Plan -> Subtask -> Move -> Grounding -> Action Tokens</span><br>
                action, generated_ids = vla.predict_action(<br>
                &nbsp;&nbsp;**inputs, <br>
                &nbsp;&nbsp;unnorm_key=<span style="color:#c3e88d;">"bridge_orig"</span>, <br>
                &nbsp;&nbsp;max_new_tokens=<span style="color:#f78c6c;">1024</span> <span style="color:#5c6370;"># Allow long context for full reasoning chain</span><br>
                )<br>
                <br>
                <span style="color:#5c6370;"># Decode the full reasoning text for inspection/debugging</span><br>
                reasoning_text = processor.batch_decode(generated_ids)[<span style="color:#f78c6c;">0</span>]
            </div>
        </div>
      </div>

    </div>
  </section>
  <script>
    function toggleLang(btn) {
      const container = btn.closest('div');
      const enElements = container.querySelectorAll('.lang-en');
      const zhElements = container.querySelectorAll('.lang-zh');
      
      enElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
      
      zhElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
    }

    function openLocalPdf(filename) {
      window.open('pdfs/' + filename, '_blank');
    }

    function delLocalPdf(filename) {
      // User will implement this themselves
      alert("Please implement the delete logic or delete the file manually: pdfs/" + filename);
    }
  </script>
</body>
</html>
