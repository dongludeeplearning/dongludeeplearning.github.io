<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ CVPR 2023 Workshop
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">RHOBIN Challenge: Reconstruction of Human Object Interaction</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll<br>
        <span style="opacity:0.8">University of TÃ¼bingen, ETH ZÃ¼rich, MPI-IS, Meta Reality Labs, NVIDIA, Adobe, USTC, Seoul National University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to bring together the research communities of human reconstruction, object pose estimation, and interaction modeling to advance the challenging task of reconstructing human-object interactions from monocular RGB images?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•å°†äººç±»é‡å»ºã€ç‰©ä½“å§¿æ€ä¼°è®¡å’Œäº¤äº’å»ºæ¨¡çš„ç ”ç©¶ç¤¾åŒºèšé›†åœ¨ä¸€èµ·ï¼Œæ¨è¿›ä»å•ç›®RGBå›¾åƒé‡å»ºäººç±»-ç‰©ä½“äº¤äº’çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>First RHOBIN Challenge:</b> Pioneering challenge for 3D reconstruction of human-object interactions from monocular RGB images, attracting 100+ participants and 300+ submissions, bridging human reconstruction, object pose estimation, and interaction modeling communities.</div>
            <div class="lang-zh" style="display:none"><b>é¦–ä¸ªRHOBINæŒ‘æˆ˜èµ›ï¼š</b>å¼€åˆ›æ€§æŒ‘æˆ˜èµ›ï¼Œä»å•ç›®RGBå›¾åƒè¿›è¡Œ3Däººç±»-ç‰©ä½“äº¤äº’é‡å»ºï¼Œå¸å¼•äº†100å¤šåå‚ä¸è€…å’Œ300å¤šä¸ªæäº¤ï¼Œè¿æ¥äº†äººç±»é‡å»ºã€ç‰©ä½“å§¿æ€ä¼°è®¡å’Œäº¤äº’å»ºæ¨¡ç¤¾åŒºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Three Comprehensive Tracks:</b> Well-designed evaluation framework with three tracks: 3D human reconstruction, 6DoF object pose estimation, and joint reconstruction of humans and objects, using BEHAVE dataset with rigorous metrics and baselines.</div>
            <div class="lang-zh" style="display:none"><b>ä¸‰ä¸ªç»¼åˆèµ›é“ï¼š</b>ç²¾å¿ƒè®¾è®¡çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªèµ›é“ï¼š3Däººç±»é‡å»ºã€6DoFç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œä»¥åŠäººç±»å’Œç‰©ä½“çš„è”åˆé‡å»ºï¼Œä½¿ç”¨BEHAVEæ•°æ®é›†å’Œä¸¥æ ¼çš„æŒ‡æ ‡åŠåŸºå‡†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Winning Method Analysis:</b> Detailed analysis of top-performing methods across all tracks, revealing key insights about 2D-3D correspondence estimation, data augmentation, model ensembling, and optimization strategies for challenging interaction scenarios.</div>
            <div class="lang-zh" style="display:none"><b>è·èƒœæ–¹æ³•åˆ†æï¼š</b>å¯¹æ‰€æœ‰èµ›é“é¡¶çº§æ–¹æ³•çš„è¯¦ç»†åˆ†æï¼Œæ­ç¤ºäº†2D-3Då¯¹åº”å…³ç³»ä¼°è®¡ã€æ•°æ®å¢å¼ºã€æ¨¡å‹é›†æˆä»¥åŠé’ˆå¯¹å…·æœ‰æŒ‘æˆ˜æ€§äº¤äº’åœºæ™¯çš„ä¼˜åŒ–ç­–ç•¥çš„å…³é”®æ´å¯Ÿã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>BEHAVE Dataset Benchmarking:</b> Comprehensive benchmarking on the largest available HOI dataset with natural environments, establishing new state-of-the-art performance and providing valuable insights for future HOI reconstruction research.</div>
            <div class="lang-zh" style="display:none"><b>BEHAVEæ•°æ®é›†åŸºå‡†æµ‹è¯•ï¼š</b>åœ¨æœ€å¤§å¯ç”¨HOIæ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰è‡ªç„¶ç¯å¢ƒï¼Œå»ºç«‹æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥çš„HOIé‡å»ºç ”ç©¶æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Community Impact:</b> Successful workshop fostering collaboration between research communities, demonstrating the importance of joint human-object reconstruction approaches and providing roadmap for future advancements in interaction modeling.</div>
            <div class="lang-zh" style="display:none"><b>ç¤¾åŒºå½±å“ï¼š</b>æˆåŠŸçš„workshopä¿ƒè¿›äº†ç ”ç©¶ç¤¾åŒºä¹‹é—´çš„åˆä½œï¼Œå±•ç¤ºäº†è”åˆäººç±»-ç‰©ä½“é‡å»ºæ–¹æ³•çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºäº¤äº’å»ºæ¨¡çš„æœªæ¥è¿›å±•æä¾›äº†è·¯çº¿å›¾ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/rhobin_overview01.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <hr style="border-top: 1px dashed rgba(255,255,255,.1); margin: 20px 0;">
            <img src="Figures/rhobin_overview02.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <hr style="border-top: 1px dashed rgba(255,255,255,.1); margin: 20px 0;">
            <img src="Figures/rhobin_overview03.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2401.04143.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2401.04143.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Heavy Occlusion and Complex Dynamics:</b> Human-object interactions involve severe mutual occlusions and complex spatio-temporal dynamics that require joint understanding of human pose, object pose, and their interactions from single RGB images.</div>
            <div class="lang-zh" style="display:none"><b>ä¸¥é‡é®æŒ¡å’Œå¤æ‚åŠ¨åŠ›å­¦ï¼š</b>äººç±»-ç‰©ä½“äº¤äº’æ¶‰åŠä¸¥é‡çš„ç›¸äº’é®æŒ¡å’Œå¤æ‚çš„æ—¶ç©ºåŠ¨åŠ›å­¦ï¼Œéœ€è¦ä»å•ä¸ªRGBå›¾åƒä¸­è”åˆç†è§£äººç±»å§¿æ€ã€ç‰©ä½“å§¿æ€åŠå…¶äº¤äº’ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Separate Research Communities:</b> Human reconstruction, object pose estimation, and interaction modeling have developed as separate fields, lacking unified approaches and benchmarks for joint reconstruction tasks.</div>
            <div class="lang-zh" style="display:none"><b>åˆ†ç¦»çš„ç ”ç©¶ç¤¾åŒºï¼š</b>äººç±»é‡å»ºã€ç‰©ä½“å§¿æ€ä¼°è®¡å’Œäº¤äº’å»ºæ¨¡ä½œä¸ºç‹¬ç«‹é¢†åŸŸå‘å±•ï¼Œç¼ºä¹é’ˆå¯¹è”åˆé‡å»ºä»»åŠ¡çš„ç»Ÿä¸€æ–¹æ³•å’ŒåŸºå‡†ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Limited Evaluation Benchmarks:</b> Existing datasets and challenges focus on isolated human or object reconstruction, lacking comprehensive evaluation frameworks for realistic interaction scenarios in natural environments.</div>
            <div class="lang-zh" style="display:none"><b>æœ‰é™çš„è¯„ä¼°åŸºå‡†ï¼š</b>ç°æœ‰æ•°æ®é›†å’ŒæŒ‘æˆ˜èµ›ä¸“æ³¨äºå­¤ç«‹çš„ä¸ªäººæˆ–ç‰©ä½“é‡å»ºï¼Œç¼ºä¹é’ˆå¯¹è‡ªç„¶ç¯å¢ƒä¸­çœŸå®äº¤äº’åœºæ™¯çš„å…¨é¢è¯„ä¼°æ¡†æ¶ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Monocular RGB Reconstruction:</b> Reconstructing complete 3D human-object interactions from single RGB images requires robust handling of depth ambiguities, scale estimation, and inter-object relationships.</div>
            <div class="lang-zh" style="display:none"><b>å•ç›®RGBé‡å»ºï¼š</b>ä»å•ä¸ªRGBå›¾åƒé‡å»ºå®Œæ•´3Däººç±»-ç‰©ä½“äº¤äº’éœ€è¦é²æ£’å¤„ç†æ·±åº¦æ­§ä¹‰ã€å°ºåº¦ä¼°è®¡å’Œç‰©ä½“é—´å…³ç³»ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Joint Optimization Challenges:</b> Coordinated reconstruction of humans and objects requires sophisticated optimization strategies that maintain physical plausibility and interaction constraints simultaneously.</div>
            <div class="lang-zh" style="display:none"><b>è”åˆä¼˜åŒ–æŒ‘æˆ˜ï¼š</b>äººç±»å’Œç‰©ä½“çš„åè°ƒé‡å»ºéœ€è¦å¤æ‚çš„ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶ä¿æŒç‰©ç†åˆç†æ€§å’Œäº¤äº’çº¦æŸã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>BEHAVE Dataset Benchmarking:</b> Utilizing the largest HOI dataset with natural environments and realistic interactions, featuring 464.4k frames with paired RGB images and 3D ground truth annotations for comprehensive evaluation.</div>
            <div class="lang-zh" style="display:none"><b>BEHAVEæ•°æ®é›†åŸºå‡†æµ‹è¯•ï¼š</b>åˆ©ç”¨å…·æœ‰è‡ªç„¶ç¯å¢ƒå’ŒçœŸå®äº¤äº’çš„æœ€å¤§HOIæ•°æ®é›†ï¼Œå…·æœ‰464.4kå¸§é…å¯¹RGBå›¾åƒå’Œ3Dåœ°é¢çœŸå®æ ‡æ³¨ï¼Œç”¨äºå…¨é¢è¯„ä¼°ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Three Track Evaluation Framework:</b> Structured evaluation with separate tracks for human reconstruction (SMPL parameters), object pose estimation (6DoF rigid transforms), and joint reconstruction (combined human-object optimization).</div>
            <div class="lang-zh" style="display:none"><b>ä¸‰èµ›é“è¯„ä¼°æ¡†æ¶ï¼š</b>ç»“æ„åŒ–è¯„ä¼°ï¼Œå…·æœ‰ç”¨äºäººç±»é‡å»ºï¼ˆSMPLå‚æ•°ï¼‰ã€ç‰©ä½“å§¿æ€ä¼°è®¡ï¼ˆ6DoFåˆšæ€§å˜æ¢ï¼‰å’Œè”åˆé‡å»ºï¼ˆç»„åˆäººç±»-ç‰©ä½“ä¼˜åŒ–ï¼‰çš„å•ç‹¬èµ›é“ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Rigorous Evaluation Metrics:</b> Comprehensive metric suite including MPJPE, PCK, AUC for human evaluation, MSSD, MSPD, rotation error for object evaluation, and combined metrics for joint reconstruction assessment.</div>
            <div class="lang-zh" style="display:none"><b>ä¸¥æ ¼çš„è¯„ä¼°æŒ‡æ ‡ï¼š</b>å…¨é¢çš„æŒ‡æ ‡å¥—ä»¶ï¼ŒåŒ…æ‹¬ç”¨äºäººç±»è¯„ä¼°çš„MPJPEã€PCKã€AUCï¼Œç”¨äºç‰©ä½“è¯„ä¼°çš„MSSDã€MSPDã€æ—‹è½¬è¯¯å·®ï¼Œä»¥åŠç”¨äºè”åˆé‡å»ºè¯„ä¼°çš„ç»„åˆæŒ‡æ ‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Winning Method Analysis:</b> Systematic analysis of top-performing approaches revealing key techniques: 2D-3D correspondence estimation via keypoints or dense matching, data augmentation strategies, model ensembling, and optimization layers for joint reconstruction.</div>
            <div class="lang-zh" style="display:none"><b>è·èƒœæ–¹æ³•åˆ†æï¼š</b>å¯¹é¡¶çº§æ–¹æ³•çš„ç³»ç»Ÿåˆ†ææ­ç¤ºå…³é”®æŠ€æœ¯ï¼šé€šè¿‡å…³é”®ç‚¹æˆ–å¯†é›†åŒ¹é…çš„2D-3Då¯¹åº”å…³ç³»ä¼°è®¡ã€æ•°æ®å¢å¼ºç­–ç•¥ã€æ¨¡å‹é›†æˆï¼Œä»¥åŠç”¨äºè”åˆé‡å»ºçš„ä¼˜åŒ–å±‚ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Community Insights and Future Directions:</b> Key observations about task maturity levels, remaining challenges in joint reconstruction, and recommendations for future research focusing on speed-accuracy trade-offs and unified approaches.</div>
            <div class="lang-zh" style="display:none"><b>ç¤¾åŒºæ´å¯Ÿå’Œæœªæ¥æ–¹å‘ï¼š</b>å…³äºä»»åŠ¡æˆç†Ÿåº¦æ°´å¹³çš„å…³é”®è§‚å¯Ÿã€è”åˆé‡å»ºä¸­å‰©ä½™æŒ‘æˆ˜ï¼Œä»¥åŠä¸“æ³¨äºé€Ÿåº¦-å‡†ç¡®æ€§æƒè¡¡å’Œç»Ÿä¸€æ–¹æ³•çš„æœªæ¥ç ”ç©¶å»ºè®®ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Task Maturity Assessment:</b> Human reconstruction has reached maturity even under heavy occlusion, while object pose estimation shows good progress but joint reconstruction remains the most challenging task requiring significant innovation.
            </div>
            <div class="lang-zh" style="display:none">
                <b>ä»»åŠ¡æˆç†Ÿåº¦è¯„ä¼°ï¼š</b>äººç±»é‡å»ºå³ä½¿åœ¨ä¸¥é‡é®æŒ¡ä¸‹ä¹Ÿå·²è¾¾åˆ°æˆç†Ÿåº¦ï¼Œè€Œç‰©ä½“å§¿æ€ä¼°è®¡æ˜¾ç¤ºå‡ºè‰¯å¥½è¿›å±•ï¼Œä½†è”åˆé‡å»ºä»ç„¶æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦é‡å¤§åˆ›æ–°ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>2D-3D Correspondence as Key Module:</b> All winning methods rely on robust 2D-3D correspondence estimation, whether through dense matching for objects or keypoint detection for humans, highlighting this as the essential foundation for monocular reconstruction.
            </div>
            <div class="lang-zh" style="display:none">
                <b>2D-3Då¯¹åº”å…³ç³»ä½œä¸ºå…³é”®æ¨¡å—ï¼š</b>æ‰€æœ‰è·èƒœæ–¹æ³•éƒ½ä¾èµ–äºé²æ£’çš„2D-3Då¯¹åº”å…³ç³»ä¼°è®¡ï¼Œæ— è®ºæ˜¯é€šè¿‡å¯†é›†åŒ¹é…ç”¨äºç‰©ä½“è¿˜æ˜¯å…³é”®ç‚¹æ£€æµ‹ç”¨äºäººç±»ï¼Œè¿™çªæ˜¾äº†è¿™æ˜¯å•ç›®é‡å»ºçš„åŸºæœ¬åŸºç¡€ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Data Augmentation and Ensembling:</b> Successful approaches heavily utilize data augmentation strategies and model ensembling techniques to boost performance on challenging interaction scenarios with heavy occlusions.
            </div>
            <div class="lang-zh" style="display:none">
                <b>æ•°æ®å¢å¼ºå’Œé›†æˆï¼š</b>æˆåŠŸæ–¹æ³•å¤§é‡åˆ©ç”¨æ•°æ®å¢å¼ºç­–ç•¥å’Œæ¨¡å‹é›†æˆæŠ€æœ¯ï¼Œä»¥æå‡åœ¨å…·æœ‰ä¸¥é‡é®æŒ¡çš„æŒ‘æˆ˜æ€§äº¤äº’åœºæ™¯ä¸Šçš„æ€§èƒ½ã€‚
            </div>
          </li>
        </ul>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because RHOBIN successfully bridged the gap between separate research communities working on human reconstruction, object pose estimation, and interaction modeling. By creating a unified challenge with rigorous evaluation metrics and a challenging benchmark dataset, it forced researchers to develop holistic approaches that consider the complex interdependencies between humans and objects during interactions. The workshop format facilitated direct knowledge exchange, leading to rapid advancements in joint reconstruction techniques that surpass what isolated research efforts could achieve.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºRHOBINæˆåŠŸå¼¥åˆäº†ä»äº‹äººç±»é‡å»ºã€ç‰©ä½“å§¿æ€ä¼°è®¡å’Œäº¤äº’å»ºæ¨¡çš„åˆ†ç¦»ç ”ç©¶ç¤¾åŒºä¹‹é—´çš„å·®è·ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªå…·æœ‰ä¸¥æ ¼è¯„ä¼°æŒ‡æ ‡å’Œå…·æœ‰æŒ‘æˆ˜æ€§åŸºå‡†æ•°æ®é›†çš„ç»Ÿä¸€æŒ‘æˆ˜èµ›ï¼Œå®ƒè¿«ä½¿ç ”ç©¶äººå‘˜å¼€å‘è€ƒè™‘äº¤äº’æœŸé—´äººç±»å’Œç‰©ä½“ä¹‹é—´å¤æ‚ç›¸äº’ä¾èµ–æ€§çš„æ•´ä½“æ–¹æ³•ã€‚workshopå½¢å¼ä¿ƒè¿›äº†ç›´æ¥çŸ¥è¯†äº¤æµï¼Œå¯¼è‡´è”åˆé‡å»ºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¶…è¶Šäº†å­¤ç«‹ç ”ç©¶å·¥ä½œæ‰€èƒ½å–å¾—çš„æˆå°±ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            This technical report presents the RHOBIN Challenge, the first comprehensive benchmark for reconstructing human-object interactions from monocular RGB images. By attracting over 100 participants and receiving more than 300 submissions, the challenge successfully demonstrated the broad interest in this emerging research area and established BEHAVE as the standard benchmark dataset. The detailed analysis of winning methods across three tracks reveals that while human reconstruction has become mature, object pose estimation and particularly joint reconstruction remain challenging tasks requiring further innovation. The challenge successfully bridged research communities and provided valuable insights for future work in human-object interaction modeling, setting the stage for more unified and robust approaches to this fundamental computer vision problem.
          </div>
          <div class="lang-zh" style="display:none">
            æœ¬æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†RHOBINæŒ‘æˆ˜èµ›ï¼Œè¿™æ˜¯é¦–ä¸ªä»å•ç›®RGBå›¾åƒé‡å»ºäººç±»-ç‰©ä½“äº¤äº’çš„ç»¼åˆåŸºå‡†ã€‚é€šè¿‡å¸å¼•è¶…è¿‡100åå‚ä¸è€…å’Œæ”¶åˆ°300å¤šä¸ªæäº¤ï¼Œè¯¥æŒ‘æˆ˜èµ›æˆåŠŸå±•ç¤ºäº†è¿™ä¸€æ–°å…´ç ”ç©¶é¢†åŸŸçš„å¹¿æ³›å…´è¶£ï¼Œå¹¶å°†BEHAVEç¡®ç«‹ä¸ºæ ‡å‡†åŸºå‡†æ•°æ®é›†ã€‚å¯¹ä¸‰ä¸ªèµ›é“è·èƒœæ–¹æ³•çš„è¯¦ç»†åˆ†ææ­ç¤ºï¼Œè™½ç„¶äººç±»é‡å»ºå·²ç»æˆç†Ÿï¼Œä½†ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯è”åˆé‡å»ºä»ç„¶æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ›æ–°ã€‚è¯¥æŒ‘æˆ˜èµ›æˆåŠŸè¿æ¥äº†ç ”ç©¶ç¤¾åŒºï¼Œå¹¶ä¸ºäººç±»-ç‰©ä½“äº¤äº’å»ºæ¨¡çš„æœªæ¥å·¥ä½œæä¾›äº†æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œä¸ºè§£å†³è¿™ä¸€åŸºæœ¬è®¡ç®—æœºè§†è§‰é—®é¢˜é“ºå¹³äº†æ›´ç»Ÿä¸€å’Œé²æ£’çš„æ–¹æ³•ä¹‹è·¯ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://rhobin-challenge.github.io/" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Challenge Website
        </a>
        <a href="https://github.com/facebookresearch/BEHAVE" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          BEHAVE Dataset
        </a>
      </div>
    </div>
</section>
</body>
</html>
