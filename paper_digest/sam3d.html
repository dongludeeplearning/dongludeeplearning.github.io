<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">SAM 3D: 3Dfy Anything in Images</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        SAM 3D Team, Xingyu Chen, Fu-Jen Chu, Pierre Gleize, Kevin J Liang, Alexander Sax, Hao Tang, Weiyao Wang, Michelle Guo, Thibaut Hardin, Xiang Li, Aohan Lin, Jiawei Liu, Ziqi Ma, Anushka Sagar, Bowen Song, Xiaodong Wang, Jianing Yang, Bowen Zhang, Piotr DollÃ¡r, Georgia Gkioxari, Matt Feiszli, Jitendra Malik<br>
        <span style="opacity:0.8">Meta Superintelligence Labs</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to create a generative model that can reconstruct full 3D object geometry, texture, and layout from a single image, particularly in challenging natural scenes with occlusion and clutter, while overcoming the massive data scarcity barrier in 3D visual grounding?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åˆ›å»ºä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•ä¸ªå›¾åƒé‡å»ºå®Œæ•´çš„3Då¯¹è±¡å‡ ä½•å½¢çŠ¶ã€çº¹ç†å’Œå¸ƒå±€ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰é®æŒ¡å’Œæ‚ä¹±çš„æŒ‘æˆ˜æ€§è‡ªç„¶åœºæ™¯ä¸­ï¼ŒåŒæ—¶å…‹æœ3Dè§†è§‰ grounding ä¸­çš„æµ·é‡æ•°æ®ç¨€ç¼ºéšœç¢ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>SAM 3D Foundation Model:</b> Introduced a new foundation model for 3D that predicts object shape, texture, and pose from a single image, achieving state-of-the-art performance with the ability to reconstruct any object and create composable 3D scenes from natural images.</div>
            <div class="lang-zh" style="display:none"><b>SAM 3DåŸºç¡€æ¨¡å‹ï¼š</b>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„3DåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•ä¸ªå›¾åƒé¢„æµ‹å¯¹è±¡å½¢çŠ¶ã€çº¹ç†å’Œå§¿æ€ï¼Œå®ç°æœ€å…ˆè¿›æ€§èƒ½ï¼Œèƒ½å¤Ÿé‡å»ºä»»ä½•å¯¹è±¡å¹¶ä»è‡ªç„¶å›¾åƒåˆ›å»ºå¯ç»„åˆçš„3Dåœºæ™¯ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Model-in-the-Loop Annotation Pipeline:</b> Developed a MITL pipeline that leverages both human annotators and AI models to create visually grounded 3D reconstruction data at unprecedented scale, enabling the collection of millions of 3D annotations for natural images.</div>
            <div class="lang-zh" style="display:none"><b>æ¨¡å‹åœ¨å›è·¯ä¸­çš„æ³¨é‡Šç®¡é“ï¼š</b>å¼€å‘äº†ä¸€ä¸ªMITLç®¡é“ï¼Œåˆ©ç”¨äººç±»æ³¨é‡Šè€…å’ŒAIæ¨¡å‹ä»¥ç©ºå‰çš„è§„æ¨¡åˆ›å»ºè§†è§‰ grounding çš„3Dé‡å»ºæ•°æ®ï¼Œä¸ºè‡ªç„¶å›¾åƒæ”¶é›†æ•°ç™¾ä¸‡ä¸ª3Dæ³¨é‡Šã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Stage Training Framework:</b> Pioneered a modern training paradigm combining synthetic pretraining with real-world alignment, breaking the orders-of-magnitude data gap between 3D and other domains through iterative data engine improvements.</div>
            <div class="lang-zh" style="display:none"><b>å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼š</b>å¼€åˆ›äº†ä¸€ä¸ªç°ä»£è®­ç»ƒèŒƒå¼ï¼Œå°†åˆæˆé¢„è®­ç»ƒä¸çœŸå®ä¸–ç•Œå¯¹é½ç›¸ç»“åˆï¼Œé€šè¿‡è¿­ä»£æ•°æ®å¼•æ“æ”¹è¿›æ‰“ç ´3Dä¸å…¶ä»–é¢†åŸŸä¹‹é—´çš„æ•°é‡çº§æ•°æ®å·®è·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>SA-3DAO Benchmark:</b> Released a challenging benchmark of 1,000 image-3D pairs for real-world object reconstruction, created by professional 3D artists, providing an expert upper bound for evaluating visually grounded 3D reconstruction methods.</div>
            <div class="lang-zh" style="display:none"><b>SA-3DAOåŸºå‡†ï¼š</b>å‘å¸ƒäº†é’ˆå¯¹çœŸå®ä¸–ç•Œå¯¹è±¡é‡å»ºçš„æŒ‘æˆ˜æ€§åŸºå‡†ï¼ŒåŒ…å«1000ä¸ªå›¾åƒ-3Då¯¹ï¼Œç”±ä¸“ä¸š3Dè‰ºæœ¯å®¶åˆ›å»ºï¼Œä¸ºè¯„ä¼°è§†è§‰ grounding çš„3Dé‡å»ºæ–¹æ³•æä¾›ä¸“å®¶ä¸Šé™ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Superior Performance:</b> Achieved at least 5:1 win rate in human preference tests over recent work, with significant quantitative gains on shape quality, texture, and layout reconstruction across diverse real-world scenarios.</div>
            <div class="lang-zh" style="display:none"><b>å“è¶Šæ€§èƒ½ï¼š</b>åœ¨äººç±»åå¥½æµ‹è¯•ä¸­ç›¸å¯¹äºæœ€è¿‘çš„å·¥ä½œå®ç°äº†è‡³å°‘5:1çš„èƒœç‡ï¼Œåœ¨å½¢çŠ¶è´¨é‡ã€çº¹ç†å’Œå¸ƒå±€é‡å»ºæ–¹é¢åœ¨å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å–å¾—äº†æ˜¾è‘—çš„é‡åŒ–å¢ç›Šã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/sam3d_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2511.16624.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2511.16624.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/sam3d_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <hr style="border-top: 1px dashed rgba(255,255,255,.1); margin: 20px 0;">
            <img src="Figures/sam3d_overview01.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2511.16624.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2511.16624.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>3D Data Scarcity:</b> Natural images paired with 3D ground truth are difficult to obtain at scale, creating a massive data gap between 3D visual grounding and other domains like text, images, or video.</div>
            <div class="lang-zh" style="display:none"><b>3Dæ•°æ®ç¨€ç¼ºï¼š</b>ä¸3Dåœ°é¢å®å†µé…å¯¹çš„è‡ªç„¶å›¾åƒéš¾ä»¥å¤§è§„æ¨¡è·å–ï¼Œåœ¨3Dè§†è§‰ grounding ä¸æ–‡æœ¬ã€å›¾åƒæˆ–è§†é¢‘ç­‰å…¶ä»–é¢†åŸŸä¹‹é—´åˆ›é€ äº†ä¸€ä¸ªå·¨å¤§çš„æ•°æ®å·®è·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Complex Scene Reconstruction:</b> Natural scenes with significant clutter, occlusion, and distant objects pose unique challenges compared to isolated object reconstruction, requiring joint prediction of shape, texture, and layout.</div>
            <div class="lang-zh" style="display:none"><b>å¤æ‚åœºæ™¯é‡å»ºï¼š</b>å…·æœ‰æ˜¾è‘—æ‚ä¹±ã€é®æŒ¡å’Œè¿œå¤„å¯¹è±¡çš„è‡ªç„¶åœºæ™¯ä¸å­¤ç«‹å¯¹è±¡é‡å»ºç›¸æ¯”æå‡ºäº†ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€è¦è”åˆé¢„æµ‹å½¢çŠ¶ã€çº¹ç†å’Œå¸ƒå±€ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Human Annotation Difficulty:</b> Generalist human annotators struggle to create 3D shape ground truth directly, unlike simpler tasks like labeling or segmentation, requiring sophisticated pipelines to collect high-quality 3D annotations.</div>
            <div class="lang-zh" style="display:none"><b>äººç±»æ³¨é‡Šéš¾åº¦ï¼š</b>é€šç”¨äººç±»æ³¨é‡Šè€…éš¾ä»¥ç›´æ¥åˆ›å»º3Då½¢çŠ¶åœ°é¢å®å†µï¼Œä¸æ ‡è®°æˆ–åˆ†å‰²ç­‰æ›´ç®€å•çš„ä»»åŠ¡ä¸åŒï¼Œéœ€è¦å¤æ‚çš„ç®¡é“æ¥æ”¶é›†é«˜è´¨é‡çš„3Dæ³¨é‡Šã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Domain Gap:</b> Models trained on synthetic data struggle to generalize to natural images, requiring sophisticated alignment techniques to bridge the gap between artificial and real-world data distributions.</div>
            <div class="lang-zh" style="display:none"><b>é¢†åŸŸå·®è·ï¼š</b>åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°è‡ªç„¶å›¾åƒï¼Œéœ€è¦å¤æ‚çš„å¯¹é½æŠ€æœ¯æ¥å¼¥åˆäººå·¥å’ŒçœŸå®ä¸–ç•Œæ•°æ®åˆ†å¸ƒä¹‹é—´çš„å·®è·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Modal Integration:</b> Successfully combining 2D image cues, 3D shape priors, and scene understanding requires sophisticated architectures that can reason about geometry, texture, and spatial layout simultaneously.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€é›†æˆï¼š</b>æˆåŠŸç»“åˆ2Då›¾åƒçº¿ç´¢ã€3Då½¢çŠ¶å…ˆéªŒå’Œåœºæ™¯ç†è§£éœ€è¦å¤æ‚çš„æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶æ¨ç†å‡ ä½•å½¢çŠ¶ã€çº¹ç†å’Œç©ºé—´å¸ƒå±€ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Evaluation Benchmark Limitations:</b> Lack of challenging real-world benchmarks for evaluating 3D reconstruction quality in natural scenes hinders proper assessment and comparison of different approaches.</div>
            <div class="lang-zh" style="display:none"><b>è¯„ä¼°åŸºå‡†å±€é™æ€§ï¼š</b>ç¼ºä¹é’ˆå¯¹è‡ªç„¶åœºæ™¯ä¸­3Dé‡å»ºè´¨é‡çš„æŒ‘æˆ˜æ€§çœŸå®ä¸–ç•ŒåŸºå‡†é˜»ç¢äº†ä¸åŒæ–¹æ³•çš„é€‚å½“è¯„ä¼°å’Œæ¯”è¾ƒã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>MITL Data Pipeline:</b> Human-and-model-in-the-loop annotation system that collects 3D shape, texture, and pose data at unprecedented scale by having annotators select and align from model-generated proposals, with professional artists handling hard cases.</div>
            <div class="lang-zh" style="display:none"><b>MITLæ•°æ®ç®¡é“ï¼š</b>äººç±»å’Œæ¨¡å‹åœ¨å›è·¯ä¸­çš„æ³¨é‡Šç³»ç»Ÿï¼Œé€šè¿‡è®©æ³¨é‡Šè€…ä»æ¨¡å‹ç”Ÿæˆçš„æè®®ä¸­é€‰æ‹©å’Œå¯¹é½ï¼Œä»¥ç©ºå‰çš„è§„æ¨¡æ”¶é›†3Då½¢çŠ¶ã€çº¹ç†å’Œå§¿æ€æ•°æ®ï¼Œç”±ä¸“ä¸šè‰ºæœ¯å®¶å¤„ç†å›°éš¾æ¡ˆä¾‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Two-Stage Architecture:</b> Geometry model predicts coarse shape and layout jointly, followed by Texture & Refinement model that adds high-resolution details and synthesizes texture, using Mixture-of-Transformers with multi-modal attention for information sharing.</div>
            <div class="lang-zh" style="display:none"><b>ä¸¤é˜¶æ®µæ¶æ„ï¼š</b>å‡ ä½•æ¨¡å‹è”åˆé¢„æµ‹ç²—ç•¥å½¢çŠ¶å’Œå¸ƒå±€ï¼Œéšåçº¹ç†å’Œç»†åŒ–æ¨¡å‹æ·»åŠ é«˜åˆ†è¾¨ç‡ç»†èŠ‚å¹¶åˆæˆçº¹ç†ï¼Œä½¿ç”¨å…·æœ‰å¤šæ¨¡æ€æ³¨æ„åŠ›çš„å˜æ¢å™¨æ··åˆè¿›è¡Œä¿¡æ¯å…±äº«ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Stage Training:</b> LLM-style training framework with synthetic pretraining on isolated objects, mid-training on semi-synthetic render-paste data, and post-training alignment with real-world data through iterative data engine improvements.</div>
            <div class="lang-zh" style="display:none"><b>å¤šé˜¶æ®µè®­ç»ƒï¼š</b>LLMé£æ ¼çš„è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬å­¤ç«‹å¯¹è±¡çš„åˆæˆé¢„è®­ç»ƒã€åŠåˆæˆæ¸²æŸ“ç²˜è´´æ•°æ®çš„ä¸­è®­ç»ƒï¼Œä»¥åŠé€šè¿‡è¿­ä»£æ•°æ®å¼•æ“æ”¹è¿›çš„çœŸå®ä¸–ç•Œæ•°æ®åè®­ç»ƒå¯¹é½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Flow Matching with Rectified Flows:</b> Conditional flow matching training with perceptually-biased noise sampling techniques, enabling efficient generation of 3D modalities including shape, texture, rotation, translation, and scale.</div>
            <div class="lang-zh" style="display:none"><b>å¸¦ä¿®æ­£æµçš„æµåŒ¹é…ï¼š</b>å…·æœ‰æ„ŸçŸ¥åå‘å™ªå£°é‡‡æ ·æŠ€æœ¯çš„æ¡ä»¶æµåŒ¹é…è®­ç»ƒï¼Œå®ç°åŒ…æ‹¬å½¢çŠ¶ã€çº¹ç†ã€æ—‹è½¬ã€å¹³ç§»å’Œç¼©æ”¾åœ¨å†…çš„3Dæ¨¡æ€çš„é«˜æ•ˆç”Ÿæˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Preference Optimization:</b> Direct preference optimization using human preference data to align model outputs with aesthetic quality standards, eliminating undesirable artifacts like floaters and missing symmetry.</div>
            <div class="lang-zh" style="display:none"><b>åå¥½ä¼˜åŒ–ï¼š</b>ä½¿ç”¨äººç±»åå¥½æ•°æ®è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œå°†æ¨¡å‹è¾“å‡ºä¸å®¡ç¾è´¨é‡æ ‡å‡†å¯¹é½ï¼Œæ¶ˆé™¤ä¸å—æ¬¢è¿çš„ä¼ªå½±ï¼Œå¦‚æµ®åŠ¨å’Œç¼ºå¤±å¯¹ç§°æ€§ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>SA-3DAO Benchmark:</b> New evaluation benchmark of 1,000 image-3D pairs created by professional artists, providing expert upper bound for real-world 3D object reconstruction assessment.</div>
            <div class="lang-zh" style="display:none"><b>SA-3DAOåŸºå‡†ï¼š</b>ç”±ä¸“ä¸šè‰ºæœ¯å®¶åˆ›å»ºçš„1000ä¸ªå›¾åƒ-3Då¯¹çš„æ–°è¯„ä¼°åŸºå‡†ï¼Œä¸ºçœŸå®ä¸–ç•Œ3Då¯¹è±¡é‡å»ºè¯„ä¼°æä¾›ä¸“å®¶ä¸Šé™ã€‚</div>
          </li>
        </ol>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Breaking the 3D Data Barrier:</b> SAM 3D demonstrates that sophisticated data collection pipelines combined with modern training paradigms can overcome the orders-of-magnitude data scarcity in 3D visual grounding, enabling unprecedented scale in real-world 3D reconstruction.
            </div>
            <div class="lang-zh" style="display:none">
                <b>æ‰“ç ´3Dæ•°æ®éšœç¢ï¼š</b>SAM 3Dè¯æ˜äº†å¤æ‚çš„æ”¶é›†ç®¡é“ä¸ç°ä»£è®­ç»ƒèŒƒå¼çš„ç»“åˆå¯ä»¥å…‹æœ3Dè§†è§‰ grounding ä¸­çš„æ•°é‡çº§æ•°æ®ç¨€ç¼ºï¼Œå®ç°çœŸå®ä¸–ç•Œ3Dé‡å»ºä¸­ç©ºå‰çš„è§„æ¨¡ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Recognition-Driven Reconstruction:</b> The success of SAM 3D validates the long-standing insight that object recognition enables 3D reconstruction, showing that even novel objects can be reconstructed by leveraging learned priors about object parts and structure.
            </div>
            <div class="lang-zh" style="display:none">
                <b>è¯†åˆ«é©±åŠ¨çš„é‡å»ºï¼š</b>SAM 3Dçš„æˆåŠŸéªŒè¯äº†å¯¹è±¡è¯†åˆ«å¯ç”¨3Dé‡å»ºçš„é•¿æœŸæ´å¯Ÿï¼Œè¡¨æ˜å³ä½¿æ˜¯æ–°é¢–çš„å¯¹è±¡ä¹Ÿå¯ä»¥é€šè¿‡åˆ©ç”¨å…³äºå¯¹è±¡éƒ¨ä»¶å’Œç»“æ„çš„å·²å­¦ä¹ å…ˆéªŒæ¥é‡å»ºã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                <b>Iterative Data Engine:</b> The virtuous cycle of model improvement driving better data collection, which in turn enables better models, demonstrates the power of human-AI collaboration in tackling data-scarce domains like 3D reconstruction.
            </div>
            <div class="lang-zh" style="display:none">
                <b>è¿­ä»£æ•°æ®å¼•æ“ï¼š</b>æ¨¡å‹æ”¹è¿›é©±åŠ¨æ›´å¥½æ•°æ®æ”¶é›†çš„ç¾å¾·å¾ªç¯ï¼Œè€Œè¿™åè¿‡æ¥åˆå¯ç”¨äº†æ›´å¥½çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†äººç±»-AIåä½œåœ¨è§£å†³3Dé‡å»ºç­‰æ•°æ®ç¨€ç¼ºé¢†åŸŸä¸­çš„åŠ›é‡ã€‚
            </div>
          </li>
        </ul>

        <div style="margin-top:16px; padding-top:12px; border-top:1px dashed rgba(255,255,255,.15);">
             <h3 style="margin:0 0 6px; font-size:14px; color:#8bffcf;">
               <span class="lang-en">High-Level Insights: Why it Works?</span>
               <span class="lang-zh" style="display:none">é«˜å±‚æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ</span>
             </h3>
             <div class="lang-en" style="color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                It works because SAM 3D addresses the fundamental data scarcity in 3D visual grounding through a sophisticated MITL pipeline that enables human-AI collaboration at unprecedented scale. By combining synthetic pretraining with iterative real-world alignment and leveraging recognition-driven reconstruction principles, the model achieves breakthrough performance in natural scenes that were previously intractable.
             </div>
             <div class="lang-zh" style="display:none; color:rgba(232,236,255,.80); font-size:13px; line-height:1.6;">
                ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºSAM 3Dé€šè¿‡å¤æ‚çš„MITLç®¡é“è§£å†³äº†3Dè§†è§‰ grounding ä¸­çš„åŸºæœ¬æ•°æ®ç¨€ç¼ºï¼Œè¯¥ç®¡é“ä»¥ç©ºå‰çš„è§„æ¨¡å®ç°äº†äººç±»-AIåä½œã€‚é€šè¿‡å°†åˆæˆé¢„è®­ç»ƒä¸è¿­ä»£çœŸå®ä¸–ç•Œå¯¹é½ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨è¯†åˆ«é©±åŠ¨çš„é‡å»ºåŸç†ï¼Œè¯¥æ¨¡å‹åœ¨ä»¥å‰éš¾ä»¥å¤„ç†çš„è‡ªç„¶åœºæ™¯ä¸­å®ç°äº†çªç ´æ€§æ€§èƒ½ã€‚
             </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            This paper introduces SAM 3D, a groundbreaking generative model for visually grounded 3D object reconstruction from single images. By predicting geometry, texture, and layout simultaneously, SAM 3D excels in natural scenes with occlusion and clutter where traditional approaches struggle. The key innovation lies in a human-and-model-in-the-loop pipeline that creates high-quality 3D annotations at unprecedented scale, combined with a modern multi-stage training framework that bridges the massive data gap in 3D visual grounding. Through extensive experiments, SAM 3D achieves at least 5:1 win rates in human preference tests and significant quantitative improvements over state-of-the-art methods. The release of code, model weights, online demo, and a new challenging benchmark SA-3DAO aims to accelerate research in real-world 3D reconstruction. This work demonstrates that sophisticated data collection strategies and iterative model-data co-evolution can overcome fundamental data scarcity challenges, opening new possibilities for 3D understanding and downstream applications.
          </div>
          <div class="lang-zh" style="display:none">
            æœ¬æ–‡ä»‹ç»äº†SAM 3Dï¼Œä¸€ä¸ªå¼€åˆ›æ€§çš„ä»å•ä¸ªå›¾åƒç”Ÿæˆè§†è§‰ grounding çš„3Då¯¹è±¡é‡å»ºç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡åŒæ—¶é¢„æµ‹å‡ ä½•å½¢çŠ¶ã€çº¹ç†å’Œå¸ƒå±€ï¼ŒSAM 3Dåœ¨å…·æœ‰é®æŒ¡å’Œæ‚ä¹±çš„è‡ªç„¶åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•åœ¨è¿™äº›åœºæ™¯ä¸­ä¼šé‡åˆ°å›°éš¾ã€‚å…³é”®åˆ›æ–°åœ¨äºäººç±»å’Œæ¨¡å‹åœ¨å›è·¯ä¸­çš„ç®¡é“ï¼Œä»¥ç©ºå‰çš„è§„æ¨¡åˆ›å»ºé«˜è´¨é‡3Dæ³¨é‡Šï¼Œç»“åˆç°ä»£å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå¼¥åˆ3Dè§†è§‰ grounding ä¸­çš„å·¨å¤§æ•°æ®å·®è·ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼ŒSAM 3Dåœ¨äººç±»åå¥½æµ‹è¯•ä¸­å®ç°äº†è‡³å°‘5:1çš„èƒœç‡ï¼Œå¹¶åœ¨æœ€å…ˆè¿›æ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—çš„é‡åŒ–æ”¹è¿›ã€‚ä»£ç ã€æ¨¡å‹æƒé‡ã€åœ¨çº¿æ¼”ç¤ºå’Œæ–°æŒ‘æˆ˜æ€§åŸºå‡†SA-3DAOçš„å‘å¸ƒæ—¨åœ¨åŠ é€ŸçœŸå®ä¸–ç•Œ3Dé‡å»ºçš„ç ”ç©¶ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å¤æ‚çš„æ”¶é›†ç­–ç•¥å’Œè¿­ä»£æ¨¡å‹-æ•°æ®ååŒæ¼”åŒ–å¯ä»¥å…‹æœåŸºæœ¬æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ï¼Œä¸º3Dç†è§£å’Œä¸‹æ¸¸åº”ç”¨å¼€è¾Ÿæ–°çš„å¯èƒ½æ€§ã€‚
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Links:</h2>
        <a href="https://ai.meta.com/sam3d" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Website
        </a>
        <a href="https://www.aidemos.meta.com/segment-anything/editor/convert-image-to-3d" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          Online Demo
        </a>
        <a href="https://github.com/facebookresearch/sam-3d-objects" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          GitHub Code
        </a>
      </div>
    </div>
</section>
</body>
</html>
