<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ICCV 2023
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Xiang Zhang, Taoyue Wang, Xiaotian Li, Huiyuan Yang, Lijun Yin<br>
        <span style="opacity:0.8">State University of New York at Binghamton, Rice University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we leverage coarse-grained activity descriptions and text-driven contrastive learning to improve facial behavior understanding without requiring fine-grained auxiliary information like landmarks?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨ç²—ç²’åº¦æ´»åŠ¨æè¿°å’Œæ–‡æœ¬é©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ æ¥æ”¹å–„é¢éƒ¨è¡Œä¸ºç†è§£ï¼Œè€Œä¸éœ€è¦åƒåœ°æ ‡è¿™æ ·çš„ç»†ç²’åº¦è¾…åŠ©ä¿¡æ¯ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Weakly-Supervised Contrastive Learning:</b> Introduced activity-based positive-negative pairing that leverages coarse-grained activity descriptions to guide contrastive learning, effectively learning representations without fine-grained auxiliary information.</div>
            <div class="lang-zh" style="display:none"><b>å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼š</b>å¼•å…¥äº†åŸºäºæ´»åŠ¨çš„æ­£è´Ÿé…å¯¹ï¼Œåˆ©ç”¨ç²—ç²’åº¦æ´»åŠ¨æè¿°æ¥æŒ‡å¯¼å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°å­¦ä¹ è¡¨ç¤ºè€Œä¸éœ€è¦ç»†ç²’åº¦çš„è¾…åŠ©ä¿¡æ¯ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Text-Driven Framework:</b> Proposed CLEF, a two-stage framework using CLIP architecture that incorporates textual information at both pre-training and fine-tuning stages for facial behavior understanding.</div>
            <div class="lang-zh" style="display:none"><b>æ–‡æœ¬é©±åŠ¨æ¡†æ¶ï¼š</b>æå‡ºäº†CLEFï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œä½¿ç”¨CLIPæ¶æ„ï¼Œåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½èå…¥æ–‡æœ¬ä¿¡æ¯ï¼Œç”¨äºé¢éƒ¨è¡Œä¸ºç†è§£ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Joint Label Representation Learning:</b> Enhanced label representations through self-supervised contrastive learning between label names and descriptions, enriching semantic information for better FER and AUR performance.</div>
            <div class="lang-zh" style="display:none"><b>è”åˆæ ‡ç­¾è¡¨ç¤ºå­¦ä¹ ï¼š</b>é€šè¿‡æ ‡ç­¾åç§°å’Œæè¿°ä¹‹é—´çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºæ ‡ç­¾è¡¨ç¤ºï¼Œä¸ºæ›´å¥½çš„FERå’ŒAURæ€§èƒ½ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>State-of-the-Art Performance:</b> Achieved superior results on three in-the-lab AU datasets and three in-the-wild FER datasets, demonstrating the effectiveness of the proposed text-driven contrastive learning approach.</div>
            <div class="lang-zh" style="display:none"><b>æœ€å…ˆè¿›æ€§èƒ½ï¼š</b>åœ¨ä¸‰ä¸ªå®éªŒå®¤AUæ•°æ®é›†å’Œä¸‰ä¸ªé‡å¤–FERæ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šç»“æœï¼Œè¯æ˜äº†æ‰€æå‡ºçš„æ–‡æœ¬é©±åŠ¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Limited Subject Diversity:</b> Facial behavior datasets have fewer subjects and categories compared to object detection datasets, making traditional contrastive learning pairing strategies ineffective due to identity interference.</div>
            <div class="lang-zh" style="display:none"><b>æœ‰é™çš„ä¸»é¢˜å¤šæ ·æ€§ï¼š</b>é¢éƒ¨è¡Œä¸ºæ•°æ®é›†çš„ä¸»é¢˜å’Œç±»åˆ«æ•°é‡æ¯”å¯¹è±¡æ£€æµ‹æ•°æ®é›†å°‘ï¼Œä½¿å¾—ä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ é…å¯¹ç­–ç•¥ç”±äºèº«ä»½å¹²æ‰°è€Œæ— æ•ˆã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Coarse-Grained Information Utilization:</b> Need to explore easily obtainable coarse-grained information (activity descriptions) rather than requiring fine-grained auxiliary data like landmarks or image captions.</div>
            <div class="lang-zh" style="display:none"><b>ç²—ç²’åº¦ä¿¡æ¯åˆ©ç”¨ï¼š</b>éœ€è¦æ¢ç´¢å®¹æ˜“è·å¾—çš„ç²—ç²’åº¦ä¿¡æ¯ï¼ˆæ´»åŠ¨æè¿°ï¼‰ï¼Œè€Œä¸æ˜¯éœ€è¦åƒåœ°æ ‡æˆ–å›¾åƒæ ‡é¢˜è¿™æ ·çš„ç»†ç²’åº¦è¾…åŠ©æ•°æ®ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Label Representation Enhancement:</b> Traditional numerical label approaches lack semantic richness; need to enrich label representations using textual descriptions for better discrimination between expressions and action units.</div>
            <div class="lang-zh" style="display:none"><b>æ ‡ç­¾è¡¨ç¤ºå¢å¼ºï¼š</b>ä¼ ç»Ÿçš„æ•°å€¼æ ‡ç­¾æ–¹æ³•ç¼ºä¹è¯­ä¹‰ä¸°å¯Œæ€§ï¼›éœ€è¦ä½¿ç”¨æ–‡æœ¬æè¿°æ¥ä¸°å¯Œæ ‡ç­¾è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°åŒºåˆ†è¡¨æƒ…å’ŒåŠ¨ä½œå•å…ƒã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Modal Integration:</b> Effectively integrating vision and text modalities in a unified framework while maintaining computational efficiency and avoiding modality conflicts.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€é›†æˆï¼š</b>åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­æœ‰æ•ˆæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å¹¶é¿å…æ¨¡æ€å†²çªã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>CLEF employs a sophisticated two-stage text-driven contrastive learning framework:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Pre-training Stage:</b> Uses weakly-supervised contrastive learning with activity-based pairing, where images from the same activity are positive pairs and different activities are negative pairs, guided by coarse-grained activity descriptions.</li>
                <li style="margin-bottom:6px;"><b>Cross-Modal Contrastive Loss:</b> Implements supervised contrastive loss between images and activity texts, and self-supervised contrastive loss between images from the same activity to learn robust representations.</li>
                <li style="margin-bottom:6px;"><b>Fine-tuning Stage:</b> Applies supervised contrastive learning for image-label name pairs and self-supervised contrastive learning for label name-description pairs to enhance semantic discrimination.</li>
                <li style="margin-bottom:6px;"><b>CLIP-based Architecture:</b> Utilizes unified vision-text encoders with shared weights for text encoders, enabling efficient multi-modal feature learning and classification.</li>
                <li style="margin-bottom:6px;"><b>Text Prompting:</b> Employs prompt templates to augment label names and descriptions, improving the semantic alignment between visual and textual modalities.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>CLEFé‡‡ç”¨äº†ä¸€ç§å¤æ‚çš„ä¸¤é˜¶æ®µæ–‡æœ¬é©±åŠ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>é¢„è®­ç»ƒé˜¶æ®µï¼š</b>ä½¿ç”¨å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸åŸºäºæ´»åŠ¨çš„é…å¯¹ï¼Œå…¶ä¸­æ¥è‡ªåŒä¸€æ´»åŠ¨çš„å›¾åƒæ˜¯æ­£é…å¯¹ï¼Œä¸åŒæ´»åŠ¨çš„å›¾åƒæ˜¯è´Ÿé…å¯¹ï¼Œç”±ç²—ç²’åº¦æ´»åŠ¨æè¿°æŒ‡å¯¼ã€‚</li>
                <li style="margin-bottom:6px;"><b>è·¨æ¨¡æ€å¯¹æ¯”æŸå¤±ï¼š</b>åœ¨å›¾åƒå’Œæ´»åŠ¨æ–‡æœ¬ä¹‹é—´å®ç°ç›‘ç£å¯¹æ¯”æŸå¤±ï¼Œåœ¨æ¥è‡ªåŒä¸€æ´»åŠ¨çš„å›¾åƒä¹‹é—´å®ç°è‡ªç›‘ç£å¯¹æ¯”æŸå¤±ï¼Œä»¥å­¦ä¹ é²æ£’è¡¨ç¤ºã€‚</li>
                <li style="margin-bottom:6px;"><b>å¾®è°ƒé˜¶æ®µï¼š</b>å¯¹å›¾åƒ-æ ‡ç­¾åç§°é…å¯¹åº”ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œå¯¹æ ‡ç­¾åç§°-æè¿°é…å¯¹åº”ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä»¥å¢å¼ºè¯­ä¹‰åŒºåˆ†ã€‚</li>
                <li style="margin-bottom:6px;"><b>åŸºäºCLIPçš„æ¶æ„ï¼š</b>åˆ©ç”¨å…·æœ‰å…±äº«æƒé‡çš„ç»Ÿä¸€è§†è§‰-æ–‡æœ¬ç¼–ç å™¨ï¼Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ å’Œåˆ†ç±»ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ–‡æœ¬æç¤ºï¼š</b>ä½¿ç”¨æç¤ºæ¨¡æ¿æ¥å¢å¼ºæ ‡ç­¾åç§°å’Œæè¿°ï¼Œæé«˜è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/text_driven_clef_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/text_driven_clef_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('iccv2023_weakly_supervised_text_driven.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('iccv2023_weakly_supervised_text_driven.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The effectiveness of CLEF stems from its innovative use of coarse-grained information and multi-modal learning:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Activity-Based Pairing:</b> By using activity descriptions to construct positive-negative pairs, CLEF overcomes the identity interference problem in facial datasets, focusing on expression-related features rather than subject-specific characteristics.</li>
                <li style="margin-bottom:6px;"><b>Textual Semantic Enhancement:</b> Incorporating label descriptions alongside names enriches the semantic space, providing better discrimination between similar facial behaviors and enabling more robust cross-modal feature learning.</li>
                <li style="margin-bottom:6px;"><b>Multi-Stage Learning:</b> The pre-training stage learns general facial representations while fine-tuning adapts to specific recognition tasks, creating a hierarchical learning approach that builds upon coarse to fine-grained understanding.</li>
                <li style="margin-bottom:6px;"><b>Efficient Modality Integration:</b> Using CLIP's unified architecture with shared text encoders reduces computational overhead while maintaining strong vision-text alignment for facial behavior analysis.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>CLEFçš„æœ‰æ•ˆæ€§æºäºå…¶å¯¹ç²—ç²’åº¦ä¿¡æ¯å’Œå¤šæ¨¡æ€å­¦ä¹ çš„åˆ›æ–°ä½¿ç”¨ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>åŸºäºæ´»åŠ¨çš„é…å¯¹ï¼š</b>é€šè¿‡ä½¿ç”¨æ´»åŠ¨æè¿°æ¥æ„å»ºæ­£è´Ÿé…å¯¹ï¼ŒCLEFå…‹æœäº†é¢éƒ¨æ•°æ®é›†ä¸­èº«ä»½å¹²æ‰°é—®é¢˜ï¼Œä¸“æ³¨äºä¸è¡¨æƒ…ç›¸å…³çš„ç‰¹å¾è€Œä¸æ˜¯ä¸»é¢˜ç‰¹å®šç‰¹å¾ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ–‡æœ¬è¯­ä¹‰å¢å¼ºï¼š</b>é™¤äº†åç§°å¤–çº³å…¥æ ‡ç­¾æè¿°ä¸°å¯Œäº†è¯­ä¹‰ç©ºé—´ï¼Œåœ¨ç›¸ä¼¼çš„é¢éƒ¨è¡Œä¸ºä¹‹é—´æä¾›æ›´å¥½çš„åŒºåˆ†ï¼Œå¹¶å®ç°æ›´é²æ£’çš„è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šé˜¶æ®µå­¦ä¹ ï¼š</b>é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ ä¸€èˆ¬é¢éƒ¨è¡¨ç¤ºï¼Œè€Œå¾®è°ƒé˜¶æ®µé€‚åº”ç‰¹å®šè¯†åˆ«ä»»åŠ¡ï¼Œåˆ›å»ºäº†ä¸€ç§å±‚æ¬¡å­¦ä¹ æ–¹æ³•ï¼Œä»ç²—åˆ°ç»†ç²’åº¦ç†è§£ã€‚</li>
                <li style="margin-bottom:6px;"><b>é«˜æ•ˆæ¨¡æ€é›†æˆï¼š</b>ä½¿ç”¨CLIPçš„ç»Ÿä¸€æ¶æ„å’Œå…±äº«æ–‡æœ¬ç¼–ç å™¨å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¸ºé¢éƒ¨è¡Œä¸ºåˆ†æä¿æŒäº†å¼ºå¤§çš„è§†è§‰-æ–‡æœ¬å¯¹é½ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>CLEF represents a significant advancement in facial behavior understanding by effectively leveraging coarse-grained activity information and text-driven contrastive learning. The two-stage framework successfully addresses the challenges of limited subject diversity and the need for fine-grained auxiliary information by utilizing easily obtainable activity descriptions. Through the integration of vision and text modalities using CLIP architecture, CLEF achieves state-of-the-art performance across multiple FER and AUR benchmarks. The approach demonstrates that coarse-grained textual information can be more effective than fine-grained auxiliary data for facial behavior analysis, opening new directions for multi-modal learning in affective computing. The method's ability to enhance label representations through joint learning of names and descriptions provides a foundation for more semantically rich facial analysis systems.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>CLEFé€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç²—ç²’åº¦æ´»åŠ¨ä¿¡æ¯å’Œæ–‡æœ¬é©±åŠ¨å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨é¢éƒ¨è¡Œä¸ºç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ä¸¤é˜¶æ®µæ¡†æ¶é€šè¿‡åˆ©ç”¨å®¹æ˜“è·å¾—çš„æ´»åŠ¨æè¿°ï¼ŒæˆåŠŸè§£å†³äº†ä¸»é¢˜å¤šæ ·æ€§æœ‰é™å’Œéœ€è¦ç»†ç²’åº¦è¾…åŠ©ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä½¿ç”¨CLIPæ¶æ„æ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼ŒCLEFåœ¨å¤šä¸ªFERå’ŒAURåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¯æ˜äº†ç²—ç²’åº¦æ–‡æœ¬ä¿¡æ¯å¯ä»¥æ¯”ç»†ç²’åº¦è¾…åŠ©æ•°æ®æ›´æœ‰æ•ˆåœ°ç”¨äºé¢éƒ¨è¡Œä¸ºåˆ†æï¼Œä¸ºæƒ…æ„Ÿè®¡ç®—ä¸­çš„å¤šæ¨¡æ€å­¦ä¹ å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚è¯¥æ–¹æ³•é€šè¿‡è”åˆå­¦ä¹ åç§°å’Œæè¿°æ¥å¢å¼ºæ ‡ç­¾è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä¸ºæ›´å…·è¯­ä¹‰ä¸°å¯Œçš„é¢éƒ¨åˆ†æç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
          <p><b>DOI:</b> <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Weakly-Supervised_Text-Driven_Contrastive_Learning_for_Facial_Behavior_Understanding_ICCV_2023_paper.pdf" style="color:#8bffcf;">ICCV 2023 Paper</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import torch<br>
import torch.nn as nn<br>
from transformers import CLIPModel, CLIPProcessor<br>
import torch.nn.functional as F<br>
<br>
// CLEF: Contrastive Learning with Text-Embedded Framework<br>
class CLEF(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, num_classes=8):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(CLEF, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# CLIP model with vision and text encoders<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Temperature parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.temperature_pretrain = nn.Parameter(torch.tensor(0.25))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.temperature_finetune = nn.Parameter(torch.tensor(0.07))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward_pretrain(self, images, activity_texts):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get image and text features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_features = self.clip_model.get_image_features(images)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_features = self.clip_model.get_text_features(activity_texts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Normalize features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_features = F.normalize(image_features, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_features = F.normalize(text_features, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return image_features, text_features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward_finetune(self, images, label_names, label_descriptions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get features for all modalities<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_features = self.clip_model.get_image_features(images)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name_features = self.clip_model.get_text_features(label_names)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;desc_features = self.clip_model.get_text_features(label_descriptions)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Normalize all features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_features = F.normalize(image_features, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name_features = F.normalize(name_features, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;desc_features = F.normalize(desc_features, dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return image_features, name_features, desc_features<br>
<br>
// Pre-training contrastive loss (Activity-based)<br>
def pretrain_contrastive_loss(image_features, text_features, activity_labels, temperature):<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Cross-modal supervised contrastive loss (L_IÃƒtext{T})<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits = torch.matmul(image_features, text_features.t()) / temperature<br>
&nbsp;&nbsp;&nbsp;&nbsp;labels = torch.arange(logits.size(0)).to(logits.device)<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss_it = F.cross_entropy(logits, labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Self-supervised contrastive loss for images (L_II)<br>
&nbsp;&nbsp;&nbsp;&nbsp;sim_matrix = torch.matmul(image_features, image_features.t()) / temperature<br>
&nbsp;&nbsp;&nbsp;&nbsp;mask = torch.eq(activity_labels.unsqueeze(0), activity_labels.unsqueeze(1))<br>
&nbsp;&nbsp;&nbsp;&nbsp;mask = mask.float() - torch.eye(mask.size(0)).to(mask.device)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Remove self-similarities<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits_ii = sim_matrix - torch.eye(sim_matrix.size(0)).to(sim_matrix.device) * 1e9<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss_ii = -torch.log(torch.exp(logits_ii) / torch.exp(logits_ii).sum(dim=1, keepdim=True))<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss_ii = (loss_ii * mask).sum() / mask.sum()<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return loss_it + loss_ii<br>
<br>
// Fine-tuning contrastive losses<br>
def finetune_contrastive_loss(image_features, name_features, desc_features, labels, temperature):<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Image-Name supervised contrastive loss<br>
&nbsp;&nbsp;&nbsp;&nbsp;logits_in = torch.matmul(image_features, name_features.t()) / temperature<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss_in = F.cross_entropy(logits_in, labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Name-Description self-supervised contrastive loss<br>
&nbsp;&nbsp;&nbsp;&nbsp;name_desc_sim = torch.matmul(name_features, desc_features.t()) / temperature<br>
&nbsp;&nbsp;&nbsp;&nbsp;labels_nd = torch.arange(name_desc_sim.size(0)).to(name_desc_sim.device)<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss_nd = F.cross_entropy(name_desc_sim, labels_nd)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return loss_in + loss_nd<br>
<br>
// Activity-based text prompting<br>
def create_activity_prompts(activity_descriptions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;for desc in activity_descriptions:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Randomly select prompt template<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;templates = [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"a photo of an activity that {desc}",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"a person from an activity that {desc}",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f"activity showing {desc}"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompts.append(random.choice(templates))<br>
&nbsp;&nbsp;&nbsp;&nbsp;return prompts<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
