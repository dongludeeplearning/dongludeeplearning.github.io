<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius: 16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ECCV 2022
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Responsive Listening Head Generation: A Benchmark Dataset and Baseline</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, Tao Mei<br>
        <span style="opacity:0.8">JD AI Research, Peking University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How to synthesize responsive listener feedbacks (e.g., nod, smile) during face-to-face conversation based on speaker's multimodal signals?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•åŸºäºæ¼”è®²è€…çš„å¤šæ¨¡æ€ä¿¡å·ï¼Œåœ¨é¢å¯¹é¢äº¤æµä¸­åˆæˆå“åº”å¼çš„å¬ä¼—åé¦ˆï¼ˆå¦‚ç‚¹å¤´ã€å¾®ç¬‘ï¼‰ï¼Ÿ
        </div>
      </div>
    </div>
  
    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>ViCo Dataset:</b> A novel dataset for listening head generation featuring 92 identities (67 speakers, 76 listeners) and 483 clips of paired speaking-listening patterns with three listening attitudes (positive, neutral, negative).</div>
            <div class="lang-zh" style="display:none"><b>ViCo æ•°æ®é›†ï¼š</b>ä¸€ä¸ªæ–°çš„å¬ä¼—å¤´éƒ¨ç”Ÿæˆæ•°æ®é›†ï¼ŒåŒ…å«92ä¸ªèº«ä»½ï¼ˆ67ä¸ªæ¼”è®²è€…ï¼Œ76ä¸ªå¬ä¼—ï¼‰å’Œ483ä¸ªç‰‡æ®µï¼Œå…·æœ‰é…å¯¹çš„â€œè¯´-å¬â€æ¨¡å¼ï¼Œå¹¶æ ‡æ³¨äº†ä¸‰ç§è†å¬æ€åº¦ï¼ˆç§¯æã€ä¸­ç«‹ã€æ¶ˆæï¼‰ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Benchmark & Baseline:</b> Establishes a benchmark and provides a baseline model that conditions on different listening attitudes to generate diverse responsive behaviors.</div>
            <div class="lang-zh" style="display:none"><b>åŸºå‡†ä¸åŸºçº¿ï¼š</b>å»ºç«‹äº†åŸºå‡†å¹¶æä¾›äº†ä¸€ä¸ªåŸºçº¿æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥ä¸åŒçš„è†å¬æ€åº¦ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„å“åº”è¡Œä¸ºã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Multimodal & Real-time:</b> Proposes a framework that takes both audio and visual signals from the speaker to generate non-verbal feedbacks in real-time.</div>
            <div class="lang-zh" style="display:none"><b>å¤šæ¨¡æ€ä¸å®æ—¶ï¼š</b>æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨æ¼”è®²è€…çš„éŸ³é¢‘å’Œè§†è§‰ä¿¡å·å®æ—¶ç”Ÿæˆéè¯­è¨€åé¦ˆã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/vico_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Lack of Data:</b> Listening head generation has seldom been studied, and high-quality paired datasets with annotated attitudes were missing.</div>
            <div class="lang-zh" style="display:none"><b>æ•°æ®åŒ®ä¹ï¼š</b>å¬ä¼—å¤´éƒ¨ç”Ÿæˆå¾ˆå°‘è¢«ç ”ç©¶ï¼Œä¸”ç¼ºä¹å¸¦æœ‰æ€åº¦æ ‡æ³¨çš„é«˜è´¨é‡é…å¯¹æ•°æ®é›†ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Responsiveness:</b> The model must actively respond to the speaker's multimodal cues (voice, expression) rather than generating random motion.</div>
            <div class="lang-zh" style="display:none"><b>å“åº”æ€§ï¼š</b>æ¨¡å‹å¿…é¡»æ ¹æ®æ¼”è®²è€…çš„å¤šæ¨¡æ€çº¿ç´¢ï¼ˆå£°éŸ³ã€è¡¨æƒ…ï¼‰åšå‡ºä¸»åŠ¨ååº”ï¼Œè€Œä¸æ˜¯ç”ŸæˆéšæœºåŠ¨ä½œã€‚</div>
          </li>
        </ul>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <ol style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Speaker Encoder:</b> Encodes speaker's audio (MFCC) and visual (3DMM) signals to capture context.</div>
            <div class="lang-zh" style="display:none"><b>æ¼”è®²è€…ç¼–ç å™¨ï¼š</b>ç¼–ç æ¼”è®²è€…çš„éŸ³é¢‘ï¼ˆMFCCï¼‰å’Œè§†è§‰ï¼ˆ3DMMï¼‰ä¿¡å·ä»¥æ•æ‰ä¸Šä¸‹æ–‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Listener Decoder:</b> An autoregressive decoder (e.g., LSTM/Transformer based) that predicts listener's 3DMM coefficients, conditioned on speaker features and specific attitude embeddings (Positive/Neutral/Negative).</div>
            <div class="lang-zh" style="display:none"><b>å¬ä¼—è§£ç å™¨ï¼š</b>ä¸€ä¸ªè‡ªå›å½’è§£ç å™¨ï¼ˆå¦‚åŸºäº LSTM/Transformerï¼‰ï¼Œä»¥æ¼”è®²è€…ç‰¹å¾å’Œç‰¹å®šæ€åº¦åµŒå…¥ï¼ˆç§¯æ/ä¸­ç«‹/æ¶ˆæï¼‰ä¸ºæ¡ä»¶ï¼Œé¢„æµ‹å¬ä¼—çš„ 3DMM ç³»æ•°ã€‚</div>
          </li>
          <li>
            <div class="lang-en"><b>Rendering:</b> Uses a GAN-based generator (like PIRenderer) to convert predicted 3D coefficients into photorealistic video frames.</div>
            <div class="lang-zh" style="display:none"><b>æ¸²æŸ“ï¼š</b>ä½¿ç”¨åŸºäº GAN çš„ç”Ÿæˆå™¨ï¼ˆå¦‚ PIRendererï¼‰å°†é¢„æµ‹çš„ 3D ç³»æ•°è½¬æ¢ä¸ºé€¼çœŸçš„è§†é¢‘å¸§ã€‚</div>
          </li>
        </ol>
      </div>
  
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/vico_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            
            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('2112.13548.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('2112.13548.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                ViCo provides a foundational dataset and benchmark for the under-explored task of responsive listening head generation, enabling research into human-to-human interaction modeling.
            </div>
            <div class="lang-zh" style="display:none">
                ViCo ä¸ºå“åº”å¼å¬ä¼—å¤´éƒ¨ç”Ÿæˆè¿™ä¸€æœªè¢«å……åˆ†æ¢ç´¢çš„ä»»åŠ¡æä¾›äº†åŸºç¡€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œä¿ƒè¿›äº†äººä¸äººäº¤äº’å»ºæ¨¡çš„ç ”ç©¶ã€‚
            </div>
          </li>
          <li style="margin-bottom:12px;">
            <div class="lang-en">
                The baseline model demonstrates the ability to generate listener behaviors that are semantically aligned with the speaker and controlled by specified attitudes.
            </div>
            <div class="lang-zh" style="display:none">
                åŸºçº¿æ¨¡å‹å±•ç¤ºäº†ç”Ÿæˆä¸æ¼”è®²è€…è¯­ä¹‰ä¸€è‡´ä¸”å—æŒ‡å®šæ€åº¦æ§åˆ¶çš„å¬ä¼—è¡Œä¸ºçš„èƒ½åŠ›ã€‚
            </div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Logicï¼ˆæ ¸å¿ƒä»£ç é€»è¾‘ï¼‰</h2>
        <div class="lang-en" style="font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; line-height:1.6; color:rgba(232,236,255,.85);">
            <div style="margin-bottom:12px;">
                // 1. Speaker Encoder: Encodes multimodal speaker signals
                <br><span style="color:#c792ea">class</span> <span style="color:#ffcb6b">SpeakerEncoder</span>(nn.Module):
                <br>&nbsp;&nbsp;<span style="color:#c792ea">def</span> <span style="color:#82aaff">forward</span>(self, audio_mfcc, video_3dmm):
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Audio Branch
                <br>&nbsp;&nbsp;&nbsp;&nbsp;audio_feat = self.audio_net(audio_mfcc)  <span style="color:#546e7a">// (B, T, D_a)</span>
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Video Branch (Expression & Pose)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;video_feat = self.video_net(video_3dmm)  <span style="color:#546e7a">// (B, T, D_v)</span>
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Fuse features
                <br>&nbsp;&nbsp;&nbsp;&nbsp;speaker_feat = torch.cat([audio_feat, video_feat], dim=-1)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#c792ea">return</span> speaker_feat
            </div>

            <div style="margin-bottom:12px;">
                // 2. Listener Decoder: Generates responsive motion conditioned on attitude
                <br><span style="color:#c792ea">class</span> <span style="color:#ffcb6b">ListenerDecoder</span>(nn.Module):
                <br>&nbsp;&nbsp;<span style="color:#c792ea">def</span> <span style="color:#82aaff">forward</span>(self, speaker_feat, attitude_label, past_listener_motion):
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Embed attitude (Positive/Neutral/Negative)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;att_embed = self.attitude_embedding(attitude_label)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Combine inputs
                <br>&nbsp;&nbsp;&nbsp;&nbsp;rnn_input = torch.cat([speaker_feat, att_embed, past_listener_motion], dim=-1)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Autoregressive prediction (e.g., LSTM)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;hidden_state = self.lstm(rnn_input)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# Predict next frame 3DMM coefficients
                <br>&nbsp;&nbsp;&nbsp;&nbsp;pred_3dmm = self.fc_out(hidden_state)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#c792ea">return</span> pred_3dmm
            </div>

            <div>
                // 3. Rendering Pipeline (PIRenderer)
                <br><span style="color:#c792ea">def</span> <span style="color:#82aaff">inference_pipeline</span>(speaker_audio, speaker_video, attitude):
                <br>&nbsp;&nbsp;# Encode speaker context
                <br>&nbsp;&nbsp;spk_feat = speaker_encoder(speaker_audio, speaker_video)
                <br>&nbsp;&nbsp;
                <br>&nbsp;&nbsp;# Generate listener motion sequence
                <br>&nbsp;&nbsp;generated_3dmm = []
                <br>&nbsp;&nbsp;curr_motion = initial_state
                <br>&nbsp;&nbsp;<span style="color:#c792ea">for</span> t <span style="color:#c792ea">in</span> range(seq_len):
                <br>&nbsp;&nbsp;&nbsp;&nbsp;next_motion = listener_decoder(spk_feat[t], attitude, curr_motion)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;generated_3dmm.append(next_motion)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;curr_motion = next_motion
                <br>&nbsp;&nbsp;
                <br>&nbsp;&nbsp;# Render to pixels using Reference Image (Ref)
                <br>&nbsp;&nbsp;output_video = PIRenderer(generated_3dmm, Reference_Image)
                <br>&nbsp;&nbsp;<span style="color:#c792ea">return</span> output_video
            </div>
        </div>

        <div class="lang-zh" style="display:none; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; line-height:1.6; color:rgba(232,236,255,.85);">
            <div style="margin-bottom:12px;">
                // 1. æ¼”è®²è€…ç¼–ç å™¨ï¼šç¼–ç å¤šæ¨¡æ€æ¼”è®²è€…ä¿¡å·
                <br><span style="color:#c792ea">class</span> <span style="color:#ffcb6b">SpeakerEncoder</span>(nn.Module):
                <br>&nbsp;&nbsp;<span style="color:#c792ea">def</span> <span style="color:#82aaff">forward</span>(self, audio_mfcc, video_3dmm):
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# éŸ³é¢‘åˆ†æ”¯
                <br>&nbsp;&nbsp;&nbsp;&nbsp;audio_feat = self.audio_net(audio_mfcc)  <span style="color:#546e7a">// (B, T, D_a)</span>
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# è§†é¢‘åˆ†æ”¯ï¼ˆè¡¨æƒ…ä¸å§¿æ€ï¼‰
                <br>&nbsp;&nbsp;&nbsp;&nbsp;video_feat = self.video_net(video_3dmm)  <span style="color:#546e7a">// (B, T, D_v)</span>
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# ç‰¹å¾èåˆ
                <br>&nbsp;&nbsp;&nbsp;&nbsp;speaker_feat = torch.cat([audio_feat, video_feat], dim=-1)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#c792ea">return</span> speaker_feat
            </div>

            <div style="margin-bottom:12px;">
                // 2. å¬ä¼—è§£ç å™¨ï¼šæ ¹æ®æ€åº¦ç”Ÿæˆå“åº”å¼åŠ¨ä½œ
                <br><span style="color:#c792ea">class</span> <span style="color:#ffcb6b">ListenerDecoder</span>(nn.Module):
                <br>&nbsp;&nbsp;<span style="color:#c792ea">def</span> <span style="color:#82aaff">forward</span>(self, speaker_feat, attitude_label, past_listener_motion):
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# åµŒå…¥æ€åº¦æ ‡ç­¾ï¼ˆç§¯æ/ä¸­ç«‹/æ¶ˆæï¼‰
                <br>&nbsp;&nbsp;&nbsp;&nbsp;att_embed = self.attitude_embedding(attitude_label)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# ç»„åˆè¾“å…¥
                <br>&nbsp;&nbsp;&nbsp;&nbsp;rnn_input = torch.cat([speaker_feat, att_embed, past_listener_motion], dim=-1)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# è‡ªå›å½’é¢„æµ‹ï¼ˆå¦‚ LSTMï¼‰
                <br>&nbsp;&nbsp;&nbsp;&nbsp;hidden_state = self.lstm(rnn_input)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;
                <br>&nbsp;&nbsp;&nbsp;&nbsp;# é¢„æµ‹ä¸‹ä¸€å¸§ 3DMM ç³»æ•°
                <br>&nbsp;&nbsp;&nbsp;&nbsp;pred_3dmm = self.fc_out(hidden_state)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#c792ea">return</span> pred_3dmm
            </div>

            <div>
                // 3. æ¸²æŸ“æµç¨‹ (PIRenderer)
                <br><span style="color:#c792ea">def</span> <span style="color:#82aaff">inference_pipeline</span>(speaker_audio, speaker_video, attitude):
                <br>&nbsp;&nbsp;# ç¼–ç æ¼”è®²è€…ä¸Šä¸‹æ–‡
                <br>&nbsp;&nbsp;spk_feat = speaker_encoder(speaker_audio, speaker_video)
                <br>&nbsp;&nbsp;
                <br>&nbsp;&nbsp;# ç”Ÿæˆå¬ä¼—åŠ¨ä½œåºåˆ—
                <br>&nbsp;&nbsp;generated_3dmm = []
                <br>&nbsp;&nbsp;curr_motion = initial_state
                <br>&nbsp;&nbsp;<span style="color:#c792ea">for</span> t <span style="color:#c792ea">in</span> range(seq_len):
                <br>&nbsp;&nbsp;&nbsp;&nbsp;next_motion = listener_decoder(spk_feat[t], attitude, curr_motion)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;generated_3dmm.append(next_motion)
                <br>&nbsp;&nbsp;&nbsp;&nbsp;curr_motion = next_motion
                <br>&nbsp;&nbsp;
                <br>&nbsp;&nbsp;# ä½¿ç”¨å‚è€ƒå›¾åƒæ¸²æŸ“ä¸ºåƒç´ 
                <br>&nbsp;&nbsp;output_video = PIRenderer(generated_3dmm, Reference_Image)
                <br>&nbsp;&nbsp;<span style="color:#c792ea">return</span> output_video
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); display:flex; align-items:center; gap:12px;">
        <h2 style="margin:0;font-size:16px;">Official Code:</h2>
        <a href="https://github.com/dc3ea9f/vico_challenge_baseline" target="_blank" style="display:flex; align-items:center; gap:6px; color:#8bffcf; text-decoration:none; font-family:ui-monospace,monospace; font-size:13px; border:1px solid rgba(139,255,207,0.3); padding:6px 12px; border-radius:8px; background:rgba(139,255,207,0.05); transition: all 0.2s ease;">
          GitHub Repo
        </a>
      </div>

    </div>
  </section>
  <script>
    function toggleLang(btn) {
      const container = btn.closest('div');
      const enElements = container.querySelectorAll('.lang-en');
      const zhElements = container.querySelectorAll('.lang-zh');
      
      enElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
      
      zhElements.forEach(el => {
        el.style.display = (el.style.display === 'none') ? 'block' : 'none';
      });
    }

    function openLocalPdf(filename) {
      window.open('pdfs/' + filename, '_blank');
    }

    function delLocalPdf(filename) {
      // User will implement this themselves
      alert("Please implement the delete logic or delete the file manually: pdfs/" + filename);
    }
  </script>
</body>
</html>

