<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ NeurIPS 2022
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou<br>
        <span style="opacity:0.8">Google Research, Brain Team</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can large language models be prompted to perform complex multi-step reasoning by generating intermediate reasoning steps, overcoming their limitations in arithmetic, commonsense, and symbolic reasoning tasks?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•é€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ï¼Œå…‹æœå®ƒä»¬åœ¨ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Chain-of-Thought Prompting:</b> Introduced a simple yet powerful prompting technique that elicits multi-step reasoning by generating intermediate reasoning steps before final answers, achieving state-of-the-art performance on arithmetic reasoning benchmarks.</div>
            <div class="lang-zh" style="display:none"><b>æ€ç»´é“¾æç¤ºï¼š</b>å¼•å…¥äº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æç¤ºæŠ€æœ¯ï¼Œé€šè¿‡åœ¨æœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥å¼•å‘å¤šæ­¥éª¤æ¨ç†ï¼Œåœ¨ç®—æœ¯æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Emergent Reasoning Abilities:</b> Demonstrated that chain-of-thought reasoning emerges predictably at scale, with performance improving dramatically as model size increases from 100M to 540B parameters across arithmetic, commonsense, and symbolic reasoning tasks.</div>
            <div class="lang-zh" style="display:none"><b>æ¶Œç°æ¨ç†èƒ½åŠ›ï¼š</b>è¯æ˜äº†æ€ç»´é“¾æ¨ç†åœ¨è§„æ¨¡ä¸Šå¯é¢„æµ‹åœ°æ¶Œç°ï¼Œéšç€æ¨¡å‹å¤§å°ä»100Må¢åŠ åˆ°540Bå‚æ•°ï¼Œåœ¨ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Comprehensive Evaluation:</b> Systematically evaluated chain-of-thought prompting across multiple model families (LaMDA, PaLM) and diverse reasoning tasks, showing consistent improvements and robustness to prompt variations including different annotators and exemplar orders.</div>
            <div class="lang-zh" style="display:none"><b>å…¨é¢è¯„ä¼°ï¼š</b>ç³»ç»Ÿåœ°è¯„ä¼°äº†æ€ç»´é“¾æç¤ºåœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ï¼ˆLaMDAã€PaLMï¼‰å’Œä¸åŒæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ˜¾ç¤ºäº†æŒç»­æ”¹è¿›å’Œå¯¹æç¤ºå˜åŒ–çš„é²æ£’æ€§ï¼ŒåŒ…æ‹¬ä¸åŒæ³¨é‡Šè€…å’Œç¤ºä¾‹é¡ºåºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>GSM8K Benchmark Breakthrough:</b> Achieved state-of-the-art accuracy on GSM8K math word problems using only 8 chain-of-thought exemplars, surpassing even fine-tuned GPT-3 with a verifier and establishing new performance levels for few-shot prompting.</div>
            <div class="lang-zh" style="display:none"><b>GSM8KåŸºå‡†çªç ´ï¼š</b>ä»…ä½¿ç”¨8ä¸ªæ€ç»´é“¾ç¤ºä¾‹å°±åœ¨GSM8Kæ•°å­¦å•è¯é—®é¢˜ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç”šè‡³è¶…è¶Šäº†å¸¦æœ‰éªŒè¯å™¨çš„å¾®è°ƒGPT-3ï¼Œå¹¶ä¸ºå°‘æ ·æœ¬æç¤ºå»ºç«‹äº†æ–°çš„æ€§èƒ½æ°´å¹³ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Limited Multi-step Reasoning:</b> Standard prompting methods fail to elicit complex reasoning capabilities in large language models, resulting in poor performance on tasks requiring intermediate reasoning steps like arithmetic word problems and commonsense reasoning.</div>
            <div class="lang-zh" style="display:none"><b>æœ‰é™çš„å¤šæ­¥éª¤æ¨ç†ï¼š</b>æ ‡å‡†æç¤ºæ–¹æ³•æ— æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å‘å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨éœ€è¦ä¸­é—´æ¨ç†æ­¥éª¤çš„ä»»åŠ¡ï¼ˆå¦‚ç®—æœ¯å•è¯é—®é¢˜å’Œå¸¸è¯†æ¨ç†ï¼‰ä¸Šè¡¨ç°ä¸ä½³ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Scale-dependent Emergence:</b> Reasoning abilities emerge unpredictably at certain model scales, making it difficult to know when and how to apply prompting techniques for complex reasoning tasks across different model sizes.</div>
            <div class="lang-zh" style="display:none"><b>è§„æ¨¡ç›¸å…³æ¶Œç°ï¼š</b>æ¨ç†èƒ½åŠ›åœ¨æŸäº›æ¨¡å‹è§„æ¨¡ä¸Šä¸å¯é¢„æµ‹åœ°æ¶Œç°ï¼Œä½¿å¾—éš¾ä»¥çŸ¥é“ä½•æ—¶ä»¥åŠå¦‚ä½•åœ¨ä¸åŒæ¨¡å‹å¤§å°ä¸Šä¸ºå¤æ‚æ¨ç†ä»»åŠ¡åº”ç”¨æç¤ºæŠ€æœ¯ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Prompt Engineering Sensitivity:</b> Chain-of-thought prompting performance can vary significantly based on prompt construction, annotator writing styles, exemplar selection, and ordering, requiring careful prompt engineering for optimal results.</div>
            <div class="lang-zh" style="display:none"><b>æç¤ºå·¥ç¨‹æ•æ„Ÿæ€§ï¼š</b>æ€ç»´é“¾æç¤ºæ€§èƒ½å¯èƒ½ä¼šæ ¹æ®æç¤ºæ„å»ºã€æ³¨é‡Šè€…å†™ä½œé£æ ¼ã€ç¤ºä¾‹é€‰æ‹©å’Œæ’åºè€Œæ˜¾è‘—å˜åŒ–ï¼Œéœ€è¦ä»”ç»†çš„æç¤ºå·¥ç¨‹æ¥è·å¾—æœ€ä½³ç»“æœã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Task-Specific Adaptation:</b> Different reasoning tasks (arithmetic, commonsense, symbolic) require tailored approaches to chain-of-thought construction and evaluation, complicating the development of universal reasoning enhancement methods.</div>
            <div class="lang-zh" style="display:none"><b>ä»»åŠ¡ç‰¹å®šé€‚åº”ï¼š</b>ä¸åŒçš„æ¨ç†ä»»åŠ¡ï¼ˆç®—æœ¯ã€å¸¸è¯†ã€ç¬¦å·ï¼‰éœ€è¦é’ˆå¯¹æ€ç»´é“¾æ„å»ºå’Œè¯„ä¼°çš„å®šåˆ¶æ–¹æ³•ï¼Œå¤æ‚åŒ–äº†é€šç”¨æ¨ç†å¢å¼ºæ–¹æ³•çš„å¼€å‘ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The chain-of-thought prompting method enables large language models to perform complex reasoning:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Chain-of-Thought Construction:</b> Instead of direct answer generation, models are prompted to generate a series of intermediate reasoning steps that lead to the final answer, mimicking human-like problem-solving processes.</li>
                <li style="margin-bottom:6px;"><b>Few-shot Exemplars:</b> Provide 8-12 examples of questions with corresponding chain-of-thought reasoning traces, allowing models to learn the pattern of step-by-step reasoning without explicit fine-tuning.</li>
                <li style="margin-bottom:6px;"><b>Scale-dependent Emergence:</b> Chain-of-thought reasoning emerges predictably at certain model scales (typically 10B+ parameters), with performance improving dramatically as model size increases from 100M to 540B parameters.</li>
                <li style="margin-bottom:6px;"><b>Task-Specific Adaptation:</b> Different reasoning tasks require tailored chain-of-thought formats: arithmetic problems use explicit calculation steps, commonsense reasoning uses elimination-based approaches, and symbolic tasks use pattern matching.</li>
                <li style="margin-bottom:6px;"><b>Robust Prompt Engineering:</b> Demonstrated robustness to variations in annotator styles, exemplar ordering, and prompt construction, while maintaining consistent performance improvements across different model families.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>æ€ç»´é“¾æç¤ºæ–¹æ³•ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå¤æ‚æ¨ç†ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ€ç»´é“¾æ„å»ºï¼š</b>ä¸æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼Œè€Œæ˜¯æç¤ºæ¨¡å‹ç”Ÿæˆä¸€ç³»åˆ—å¯¼è‡´æœ€ç»ˆç­”æ¡ˆçš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæ¨¡ä»¿äººç±»èˆ¬çš„è§£å†³é—®é¢˜çš„è¿‡ç¨‹ã€‚</li>
                <li style="margin-bottom:6px;"><b>å°‘æ ·æœ¬ç¤ºä¾‹ï¼š</b>æä¾›8-12ä¸ªå¸¦æœ‰ç›¸åº”æ€ç»´é“¾æ¨ç†è½¨è¿¹çš„é—®é¢˜ç¤ºä¾‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜¾å¼å¾®è°ƒçš„æƒ…å†µä¸‹å­¦ä¹ é€æ­¥æ¨ç†çš„æ¨¡å¼ã€‚</li>
                <li style="margin-bottom:6px;"><b>è§„æ¨¡ç›¸å…³æ¶Œç°ï¼š</b>æ€ç»´é“¾æ¨ç†åœ¨æŸäº›æ¨¡å‹è§„æ¨¡ï¼ˆé€šå¸¸10B+å‚æ•°ï¼‰ä¸Šå¯é¢„æµ‹åœ°æ¶Œç°ï¼Œéšç€æ¨¡å‹å¤§å°ä»100Må¢åŠ åˆ°540Bå‚æ•°ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»»åŠ¡ç‰¹å®šé€‚åº”ï¼š</b>ä¸åŒçš„æ¨ç†ä»»åŠ¡éœ€è¦å®šåˆ¶çš„æ€ç»´é“¾æ ¼å¼ï¼šç®—æœ¯é—®é¢˜ä½¿ç”¨æ˜¾å¼è®¡ç®—æ­¥éª¤ï¼Œå¸¸è¯†æ¨ç†ä½¿ç”¨åŸºäºæ¶ˆé™¤çš„æ–¹æ³•ï¼Œç¬¦å·ä»»åŠ¡ä½¿ç”¨æ¨¡å¼åŒ¹é…ã€‚</li>
                <li style="margin-bottom:6px;"><b>é²æ£’æç¤ºå·¥ç¨‹ï¼š</b>è¯æ˜äº†å¯¹æ³¨é‡Šè€…é£æ ¼ã€ç¤ºä¾‹æ’åºå’Œæç¤ºæ„å»ºå˜å¼‚çš„é²æ£’æ€§ï¼ŒåŒæ—¶åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­ä¿æŒä¸€è‡´çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€å›¾ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
            <img src="Figures/chain_of_thought_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/chain_of_thought_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p>Chain-of-Thought prompting enables step-by-step reasoning: "There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6." This intermediate reasoning dramatically improves performance on arithmetic, commonsense, and symbolic tasks.</p>
            <p>Note: The method shows emergent reasoning abilities at scale, with PaLM 540B achieving 17.9% on GSM8K using just 8 CoT exemplars. Results demonstrate consistent improvements across LaMDA and PaLM model families, with robustness to prompt variations.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_chain_of_thought.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_chain_of_thought.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>Chain-of-thought prompting succeeds by leveraging the latent reasoning capabilities of large language models:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Latent Knowledge Activation:</b> Large language models possess implicit reasoning knowledge that standard prompting fails to access; chain-of-thought provides a structured pathway to elicit step-by-step reasoning capabilities.</li>
                <li style="margin-bottom:6px;"><b>Scale-dependent Emergence:</b> Reasoning abilities emerge predictably at sufficient model scale, where models can learn and apply complex reasoning patterns from few-shot exemplars without explicit fine-tuning.</li>
                <li style="margin-bottom:6px;"><b>Structured Problem Decomposition:</b> By breaking down complex problems into intermediate steps, models can apply familiar reasoning patterns to unfamiliar problems, enabling generalization across diverse reasoning tasks.</li>
                <li style="margin-bottom:6px;"><b>Robust Prompt Engineering:</b> The method's effectiveness is robust to prompt variations, suggesting that the core mechanism relies on activating inherent model capabilities rather than brittle prompt engineering.</li>
                <li style="margin-bottom:6px;"><b>Human-like Reasoning Mimicry:</b> Chain-of-thought prompting mimics human problem-solving by generating intermediate reasoning steps, allowing models to leverage pre-trained knowledge in a more structured and interpretable manner.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>æ€ç»´é“¾æç¤ºé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›è€ŒæˆåŠŸï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>æ½œåœ¨çŸ¥è¯†æ¿€æ´»ï¼š</b>å¤§å‹è¯­è¨€æ¨¡å‹æ‹¥æœ‰æ ‡å‡†æç¤ºæ— æ³•è®¿é—®çš„éšå¼æ¨ç†çŸ¥è¯†ï¼›æ€ç»´é“¾æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–çš„é€”å¾„æ¥å¼•å‘é€æ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
                <li style="margin-bottom:6px;"><b>è§„æ¨¡ç›¸å…³æ¶Œç°ï¼š</b>æ¨ç†èƒ½åŠ›åœ¨è¶³å¤Ÿçš„æ¨¡å‹è§„æ¨¡ä¸Šå¯é¢„æµ‹åœ°æ¶Œç°ï¼Œå…¶ä¸­æ¨¡å‹å¯ä»¥ä»æœªç»æ˜¾å¼å¾®è°ƒçš„å°‘æ ·æœ¬ç¤ºä¾‹ä¸­å­¦ä¹ å’Œåº”ç”¨å¤æ‚çš„æ¨ç†æ¨¡å¼ã€‚</li>
                <li style="margin-bottom:6px;"><b>ç»“æ„åŒ–é—®é¢˜åˆ†è§£ï¼š</b>é€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤ï¼Œæ¨¡å‹å¯ä»¥å°†ç†Ÿæ‚‰çš„æ¨ç†æ¨¡å¼åº”ç”¨äºä¸ç†Ÿæ‚‰çš„é—®é¢˜ï¼Œä»è€Œå®ç°è·¨ä¸åŒæ¨ç†ä»»åŠ¡çš„æ³›åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>é²æ£’æç¤ºå·¥ç¨‹ï¼š</b>æ–¹æ³•çš„æœ‰æ•ˆæ€§å¯¹æç¤ºå˜å¼‚å…·æœ‰é²æ£’æ€§ï¼Œè¿™è¡¨æ˜æ ¸å¿ƒæœºåˆ¶ä¾èµ–äºæ¿€æ´»å›ºæœ‰æ¨¡å‹èƒ½åŠ›ï¼Œè€Œä¸æ˜¯è„†å¼±çš„æç¤ºå·¥ç¨‹ã€‚</li>
                <li style="margin-bottom:6px;"><b>äººç±»æ¨ç†æ¨¡ä»¿ï¼š</b>æ€ç»´é“¾æç¤ºé€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥æ¨¡ä»¿äººç±»é—®é¢˜è§£å†³ï¼Œå…è®¸æ¨¡å‹ä»¥æ›´ç»“æ„åŒ–å’Œå¯è§£é‡Šçš„æ–¹å¼åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This groundbreaking work introduces chain-of-thought prompting, a simple yet transformative technique that unlocks the latent reasoning capabilities of large language models. By prompting models to generate intermediate reasoning steps before providing final answers, the method achieves dramatic performance improvements across diverse reasoning tasks including arithmetic, commonsense, and symbolic reasoning. The work demonstrates that these reasoning abilities emerge predictably at scale, with particularly striking results on challenging benchmarks like GSM8K where PaLM 540B achieves state-of-the-art performance using only eight chain-of-thought exemplars. Comprehensive experiments across multiple model families and evaluation settings establish the robustness and generalizability of the approach, showing consistent improvements regardless of prompt engineering variations. This work not only advances the practical capabilities of large language models for complex reasoning tasks but also provides fundamental insights into how scaling and prompting interact to unlock emergent reasoning abilities in neural language models.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å¼€åˆ›æ€§å·¥ä½œå¼•å…¥äº†æ€ç»´é“¾æç¤ºï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œå˜é©æ€§çš„æŠ€æœ¯ï¼Œå¯ä»¥è§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æç¤ºæ¨¡å‹åœ¨æä¾›æœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œè¯¥æ–¹æ³•åœ¨åŒ…æ‹¬ç®—æœ¯ã€å¸¸è¯†å’Œç¬¦å·æ¨ç†åœ¨å†…çš„ä¸åŒæ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚è¯¥å·¥ä½œè¯æ˜ï¼Œè¿™äº›æ¨ç†èƒ½åŠ›åœ¨è§„æ¨¡ä¸Šå¯é¢„æµ‹åœ°æ¶Œç°ï¼Œåœ¨åƒGSM8Kè¿™æ ·çš„æŒ‘æˆ˜æ€§åŸºå‡†ä¸Šå–å¾—äº†ç‰¹åˆ«æ˜¾è‘—çš„ç»“æœï¼Œå…¶ä¸­PaLM 540Bä»…ä½¿ç”¨å…«ä¸ªæ€ç»´é“¾ç¤ºä¾‹å°±å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è·¨å¤šä¸ªæ¨¡å‹å®¶æ—å’Œè¯„ä¼°è®¾ç½®çš„å…¨é¢å®éªŒç¡®ç«‹äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ— è®ºæç¤ºå·¥ç¨‹å˜å¼‚å¦‚ä½•ï¼Œéƒ½æ˜¾ç¤ºå‡ºä¸€è‡´çš„æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å®é™…èƒ½åŠ›ï¼Œè¿˜æä¾›äº†å…³äºç¼©æ”¾å’Œæç¤ºå¦‚ä½•ç›¸äº’ä½œç”¨ä»¥è§£é”ç¥ç»è¯­è¨€æ¨¡å‹ä¸­æ¶Œç°æ¨ç†èƒ½åŠ›çš„æ ¹æœ¬è§è§£ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2201.11903" target="_blank" style="color:#8bffcf;">2201.11903</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# Chain-of-Thought Prompting Implementation<br>
<br>
// Standard prompting vs Chain-of-Thought<br>
<span style="color:#6a9955;"># Standard: Direct answer generation</span><br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">standard_prompt</span>(question):<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> f"Q: {question}\nA:"<br>
<br>
// Chain-of-Thought: Step-by-step reasoning<br>
<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">chain_of_thought_prompt</span>(question, exemplars):<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt = ""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> exemplar <span style="color:#569cd6;">in</span> exemplars:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt += f"Q: {exemplar['question']}\nA: {exemplar['reasoning']}\n\n"<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt += f"Q: {question}\nA:"<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> prompt<br>
<br>
// Example usage<br>
exemplars = [<br>
&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"question": "There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"reasoning": "There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6."<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># ... more exemplars</span><br>
]<br>
<br>
question = "If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?"<br>
cot_prompt = chain_of_thought_prompt(question, exemplars)<br>
<br>
// Model generates step-by-step reasoning:<br>
// "There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5."<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
