<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  // ç›´æ¥æ‰“å¼€PDFæ–‡ä»¶
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ICRA 2025 (Best Paper Finalist)
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        Yihe Tang, Wenlong Huang, Yingke Wang, Chengshu Li, Roy Yuan, Ruohan Zhang, Jiajun Wu, Li Fei-Fei<br>
        <span style="opacity:0.8">Stanford University</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we distill affordance knowledge from foundation models to create task-conditioned affordance models that generalize to unseen robotic manipulation scenarios without manual annotations?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•ä»åŸºç¡€æ¨¡å‹ä¸­è’¸é¦å¯åŠæ€§çŸ¥è¯†ï¼Œä»¥åˆ›å»ºä»»åŠ¡æ¡ä»¶åŒ–çš„å¯åŠæ€§æ¨¡å‹ï¼Œåœ¨æ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹æ³›åŒ–åˆ°æœªè§çš„æœºå™¨äººæ“ä½œåœºæ™¯ï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Unsupervised Affordance Distillation:</b> Novel pipeline that extracts fine-grained affordance annotations from foundation models (VLMs and LVMs) without manual labeling, automatically annotating large-scale datasets with <instruction, visual affordance> pairs.</div>
            <div class="lang-zh" style="display:none"><b>æ— ç›‘ç£å¯åŠæ€§è’¸é¦ï¼š</b>æ–°é¢–çš„æµæ°´çº¿ï¼Œä»åŸºç¡€æ¨¡å‹ï¼ˆVLMså’ŒLVMsï¼‰ä¸­æå–ç»†ç²’åº¦å¯åŠæ€§æ³¨é‡Šï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ ‡è®°ï¼Œè‡ªåŠ¨ä¸ºå¤§è§„æ¨¡æ•°æ®é›†æ³¨é‡Š<æŒ‡ä»¤ï¼Œå¯è§†åŒ–å¯åŠæ€§>é…å¯¹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Task-Conditioned Affordance Model:</b> Lightweight decoder atop frozen DINOv2 features using FiLM layers for language conditioning, enabling pixel-level affordance prediction conditioned on open-ended task instructions.</div>
            <div class="lang-zh" style="display:none"><b>ä»»åŠ¡æ¡ä»¶åŒ–å¯åŠæ€§æ¨¡å‹ï¼š</b>åœ¨å†»ç»“çš„DINOv2ç‰¹å¾ä¹‹ä¸Šçš„è½»é‡çº§è§£ç å™¨ï¼Œä½¿ç”¨FiLMå±‚è¿›è¡Œè¯­è¨€æ¡ä»¶åŒ–ï¼Œå®ç°åŸºäºå¼€æ”¾å¼ä»»åŠ¡æŒ‡ä»¤çš„åƒç´ çº§å¯åŠæ€§é¢„æµ‹ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Zero-Shot Generalization:</b> Model trained only on rendered single objects in simulation achieves strong zero-shot performance on real-world robotic datasets (DROID) and human activity benchmarks (AGD20K), despite no exposure to these domains.</div>
            <div class="lang-zh" style="display:none"><b>é›¶æ ·æœ¬æ³›åŒ–ï¼š</b>ä»…åœ¨æ¨¡æ‹Ÿä¸­æ¸²æŸ“çš„å•ä¸ªå¯¹è±¡ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨çœŸå®ä¸–ç•Œæœºå™¨äººæ•°æ®é›†ï¼ˆDROIDï¼‰å’Œäººç±»æ´»åŠ¨åŸºå‡†ï¼ˆAGD20Kï¼‰ä¸Šå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå°½ç®¡ä»æœªæ¥è§¦è¿‡è¿™äº›é¢†åŸŸã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Affordance-as-Observation Policy:</b> Imitation learning policy using UAD affordance predictions as observation space, demonstrating generalization to unseen object instances, categories, and task instructions after training on just 10 demonstrations.</div>
            <div class="lang-zh" style="display:none"><b>å¯åŠæ€§ä½œä¸ºè§‚å¯Ÿç­–ç•¥ï¼š</b>æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ä½¿ç”¨UADå¯åŠæ€§é¢„æµ‹ä½œä¸ºè§‚å¯Ÿç©ºé—´ï¼Œåœ¨ä»…ç”¨10ä¸ªæ¼”ç¤ºè®­ç»ƒåï¼Œå±•ç¤ºäº†æ³›åŒ–åˆ°æœªè§å¯¹è±¡å®ä¾‹ã€ç±»åˆ«å’Œä»»åŠ¡æŒ‡ä»¤çš„èƒ½åŠ›ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Grounding Language to Visual Affordance:</b> Bridging the gap between language-space affordance knowledge in VLMs and continuous pixel-level visual affordance predictions required for robotic manipulation.</div>
            <div class="lang-zh" style="display:none"><b>å°†è¯­è¨€ grounding åˆ°è§†è§‰å¯åŠæ€§ï¼š</b>å¼¥åˆVLMsä¸­è¯­è¨€ç©ºé—´å¯åŠæ€§çŸ¥è¯†ä¸æœºå™¨äººæ“ä½œæ‰€éœ€çš„è¿ç»­åƒç´ çº§è§†è§‰å¯åŠæ€§é¢„æµ‹ä¹‹é—´çš„å·®è·ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Free-Form Task Instructions:</b> Scaling affordance learning beyond predefined task vocabularies to handle open-ended natural language instructions in unstructured robotic environments.</div>
            <div class="lang-zh" style="display:none"><b>è‡ªç”±å½¢å¼ä»»åŠ¡æŒ‡ä»¤ï¼š</b>å°†å¯åŠæ€§å­¦ä¹ æ‰©å±•åˆ°é¢„å®šä¹‰ä»»åŠ¡è¯æ±‡ä¹‹å¤–ï¼Œä»¥å¤„ç†éç»“æ„åŒ–æœºå™¨äººç¯å¢ƒä¸­çš„å¼€æ”¾å¼è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Simulation-to-Real Generalization:</b> Ensuring models trained only on synthetic single-object renderings generalize to multi-object cluttered real-world robotic scenes and diverse human activities.</div>
            <div class="lang-zh" style="display:none"><b>æ¨¡æ‹Ÿåˆ°çœŸå®æ³›åŒ–ï¼š</b>ç¡®ä¿ä»…åœ¨åˆæˆå•å¯¹è±¡æ¸²æŸ“ä¸Šè®­ç»ƒçš„æ¨¡å‹æ³›åŒ–åˆ°å¤šå¯¹è±¡æ‚ä¹±çš„çœŸå®ä¸–ç•Œæœºå™¨äººåœºæ™¯å’Œå¤šæ ·åŒ–äººç±»æ´»åŠ¨ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Few-Shot Manipulation Learning:</b> Developing visuomotor policies that can learn generalizable manipulation behaviors from extremely limited demonstration data using affordance-centric observation spaces.</div>
            <div class="lang-zh" style="display:none"><b>å°‘æ ·æœ¬æ“ä½œå­¦ä¹ ï¼š</b>å¼€å‘è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œä½¿ç”¨ä»¥å¯åŠæ€§ä¸ºä¸­å¿ƒè§‚å¯Ÿç©ºé—´ï¼Œèƒ½å¤Ÿä»æå…¶æœ‰é™çš„æ¼”ç¤ºæ•°æ®ä¸­å­¦ä¹ å¯æ³›åŒ–çš„æ“ä½œè¡Œä¸ºã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The UAD framework presents a comprehensive approach for unsupervised affordance distillation:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Affordance Annotation Extraction:</b> Leverages VLM (GPT-4o) to propose task instructions and associate them with fine-grained object regions obtained via DINOv2 feature clustering and multi-view fusion, creating <instruction, affordance map> triplets.</li>
                <li style="margin-bottom:6px;"><b>Task-Conditioned Model Training:</b> Trains lightweight FiLM-conditioned decoder atop frozen DINOv2 features, using language embeddings to modulate visual features for task-specific affordance prediction without updating pre-trained weights.</li>
                <li style="margin-bottom:6px;"><b>Zero-Shot Evaluation Pipeline:</b> Evaluates model generalization on held-out rendered objects, real-world robotic datasets (DROID), and human activity benchmarks (AGD20K) using AUC and saliency metrics without fine-tuning.</li>
                <li style="margin-bottom:6px;"><b>Affordance-Centric Policy Learning:</b> Integrates UAD affordance predictions as observation space in imitation learning policies, training on minimal demonstrations (10 per task) while achieving generalization across object instances, categories, and instructions.</li>
                <li style="margin-bottom:6px;"><b>Real-World Robotic Deployment:</b> Validates approach on physical Franka Panda robot with tabletop manipulation tasks (pouring, opening, insertion), using kinesthetic teaching for demonstration collection and multi-view RGB-D sensing.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>UADæ¡†æ¶æå‡ºäº†æ— ç›‘ç£å¯åŠæ€§è’¸é¦çš„å…¨é¢æ–¹æ³•ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>å¯åŠæ€§æ³¨é‡Šæå–ï¼š</b>åˆ©ç”¨VLMï¼ˆGPT-4oï¼‰æå‡ºä»»åŠ¡æŒ‡ä»¤ï¼Œå¹¶å°†å®ƒä»¬ä¸é€šè¿‡DINOv2ç‰¹å¾èšç±»å’Œå¤šè§†å›¾èåˆè·å¾—çš„ç»†ç²’åº¦å¯¹è±¡åŒºåŸŸå…³è”ï¼Œåˆ›å»º<æŒ‡ä»¤ï¼Œå¯åŠæ€§åœ°å›¾>ä¸‰å…ƒç»„ã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»»åŠ¡æ¡ä»¶åŒ–æ¨¡å‹è®­ç»ƒï¼š</b>åœ¨å†»ç»“çš„DINOv2ç‰¹å¾ä¹‹ä¸Šè®­ç»ƒè½»é‡çº§FiLMæ¡ä»¶åŒ–è§£ç å™¨ï¼Œä½¿ç”¨è¯­è¨€åµŒå…¥æ¥è°ƒèŠ‚è§†è§‰ç‰¹å¾ä»¥è¿›è¡Œä»»åŠ¡ç‰¹å®šçš„å¯åŠæ€§é¢„æµ‹ï¼Œè€Œä¸æ›´æ–°é¢„è®­ç»ƒæƒé‡ã€‚</li>
                <li style="margin-bottom:6px;"><b>é›¶æ ·æœ¬è¯„ä¼°æµæ°´çº¿ï¼š</b>ä½¿ç”¨AUCå’Œæ˜¾è‘—æ€§æŒ‡æ ‡è¯„ä¼°æ¨¡å‹åœ¨ä¿ç•™æ¸²æŸ“å¯¹è±¡ã€çœŸå®ä¸–ç•Œæœºå™¨äººæ•°æ®é›†ï¼ˆDROIDï¼‰å’Œäººç±»æ´»åŠ¨åŸºå‡†ï¼ˆAGD20Kï¼‰ä¸Šçš„æ³›åŒ–ï¼Œè€Œæ— éœ€å¾®è°ƒã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»¥å¯åŠæ€§ä¸ºä¸­å¿ƒç­–ç•¥å­¦ä¹ ï¼š</b>å°†UADå¯åŠæ€§é¢„æµ‹ä½œä¸ºè§‚å¯Ÿç©ºé—´é›†æˆåˆ°æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ä¸­ï¼Œåœ¨æœ€å°‘çš„æ¼”ç¤ºï¼ˆæ¯ä¸ªä»»åŠ¡10ä¸ªï¼‰ä¸Šè®­ç»ƒï¼ŒåŒæ—¶å®ç°è·¨å¯¹è±¡å®ä¾‹ã€ç±»åˆ«å’ŒæŒ‡ä»¤çš„æ³›åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>çœŸå®ä¸–ç•Œæœºå™¨äººéƒ¨ç½²ï¼š</b>åœ¨ç‰©ç†Franka Pandaæœºå™¨äººä¸ŠéªŒè¯æ–¹æ³•ï¼Œä½¿ç”¨æ¡Œé¢æ“ä½œä»»åŠ¡ï¼ˆå€¾å€’ã€æ‰“å¼€ã€æ’å…¥ï¼‰ï¼Œä½¿ç”¨è¿åŠ¨æ•™å­¦æ”¶é›†æ¼”ç¤ºå’Œå¤šè§†å›¾RGB-Dä¼ æ„Ÿã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆç¤ºä¾‹ï¼‰</h2>
        <div style="border-radius:14px; overflow:hidden;">
             <img src="Figures/uad_affordance_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px dashed rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <p style="margin-top:12px;">UAD pipeline showing unsupervised extraction of affordance annotations from foundation models and distillation into task-conditioned affordance model for robotic manipulation generalization.</p>
            <img src="Figures/uad_affordance_overview.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_uad_affordance.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_uad_affordance.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of UAD in enabling generalization for robotic manipulation stems from its innovative integration of foundation model capabilities:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Foundation Model Synergy:</b> Complementary strengths of VLMs (task reasoning and instruction understanding) and LVMs (fine-grained visual features) enable automatic generation of high-quality affordance annotations without human supervision.</li>
                <li style="margin-bottom:6px;"><b>Parameter-Efficient Conditioning:</b> FiLM layers provide lightweight language conditioning atop frozen DINOv2 features, preserving general visual knowledge while enabling task-specific affordance prediction with minimal training data.</li>
                <li style="margin-bottom:6px;"><b>Continuous Affordance Representation:</b> Pixel-level continuous predictions capture nuanced affordance information beyond binary segmentation, enabling precise robotic manipulation in complex, multi-object scenes.</li>
                <li style="margin-bottom:6px;"><b>Affordance-Centric Learning:</b> Using affordance as observation space sidesteps challenges of learning generalizable visual representations from limited interaction data, providing manipulation-relevant features that transfer across tasks and environments.</li>
                <li style="margin-bottom:6px;"><b>Scalable Data Annotation:</b> Unsupervised pipeline scales affordance learning to thousands of object-instruction pairs by leveraging existing 3D assets and foundation models, creating diverse training data that supports broad generalization.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>UADåœ¨å®ç°æœºå™¨äººæ“ä½œæ³›åŒ–æ–¹é¢çš„æˆåŠŸæºäºå…¶åˆ›æ–°çš„åŸºç¡€æ¨¡å‹èƒ½åŠ›é›†æˆï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>åŸºç¡€æ¨¡å‹ååŒï¼š</b>VLMsï¼ˆä»»åŠ¡æ¨ç†å’ŒæŒ‡ä»¤ç†è§£ï¼‰å’ŒLVMsï¼ˆç»†ç²’åº¦è§†è§‰ç‰¹å¾ï¼‰çš„äº’è¡¥ä¼˜åŠ¿èƒ½å¤Ÿåœ¨æ— äººç›‘ç£çš„æƒ…å†µä¸‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å¯åŠæ€§æ³¨é‡Šã€‚</li>
                <li style="margin-bottom:6px;"><b>å‚æ•°é«˜æ•ˆæ¡ä»¶åŒ–ï¼š</b>FiLMå±‚åœ¨å†»ç»“çš„DINOv2ç‰¹å¾ä¹‹ä¸Šæä¾›è½»é‡çº§è¯­è¨€æ¡ä»¶åŒ–ï¼Œä¿ç•™ä¸€èˆ¬è§†è§‰çŸ¥è¯†ï¼ŒåŒæ—¶ä½¿ç”¨æœ€å°‘çš„è®­ç»ƒæ•°æ®å®ç°ä»»åŠ¡ç‰¹å®šçš„å¯åŠæ€§é¢„æµ‹ã€‚</li>
                <li style="margin-bottom:6px;"><b>è¿ç»­å¯åŠæ€§è¡¨ç¤ºï¼š</b>åƒç´ çº§è¿ç»­é¢„æµ‹æ•è·è¶…å‡ºäºŒè¿›åˆ¶åˆ†å‰²çš„ç»†å¾®å¯åŠæ€§ä¿¡æ¯ï¼Œåœ¨å¤æ‚å¤šå¯¹è±¡åœºæ™¯ä¸­å®ç°ç²¾ç¡®çš„æœºå™¨äººæ“ä½œã€‚</li>
                <li style="margin-bottom:6px;"><b>ä»¥å¯åŠæ€§ä¸ºä¸­å¿ƒå­¦ä¹ ï¼š</b>ä½¿ç”¨å¯åŠæ€§ä½œä¸ºè§‚å¯Ÿç©ºé—´é¿å¼€äº†ä»æœ‰é™äº¤äº’æ•°æ®ä¸­å­¦ä¹ å¯æ³›åŒ–è§†è§‰è¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œæä¾›è·¨ä»»åŠ¡å’Œç¯å¢ƒçš„æ“ä½œç›¸å…³ç‰¹å¾ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¯æ‰©å±•æ•°æ®æ³¨é‡Šï¼š</b>æ— ç›‘ç£æµæ°´çº¿é€šè¿‡åˆ©ç”¨ç°æœ‰çš„3Dèµ„äº§å’ŒåŸºç¡€æ¨¡å‹ï¼Œå°†å¯åŠæ€§å­¦ä¹ æ‰©å±•åˆ°æ•°åƒä¸ªå¯¹è±¡-æŒ‡ä»¤é…å¯¹ï¼Œåˆ›å»ºæ”¯æŒå¹¿æ³›æ³›åŒ–çš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>This work introduces Unsupervised Affordance Distillation (UAD), a groundbreaking approach that bridges the gap between foundation models and practical robotic manipulation by automatically extracting and distilling affordance knowledge without manual annotations. By synergistically combining vision-language models for task reasoning with large vision models for fine-grained visual understanding, UAD creates a scalable pipeline for generating task-conditioned affordance datasets. The resulting lightweight model demonstrates exceptional zero-shot generalization capabilities, performing competitively on established benchmarks and real-world robotic datasets despite training solely on synthetic renderings. When integrated as the observation space in imitation learning policies, UAD enables remarkable generalization to unseen object instances, categories, and task instructions using minimal demonstration data. The approach achieves an average 73% success rate on real-world manipulation tasks with a physical robot, validating its practical utility. This work establishes a new paradigm for affordance learning that leverages foundation model knowledge to overcome traditional limitations in robotic manipulation, opening avenues for more generalizable and data-efficient robot learning systems.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>è¿™é¡¹å·¥ä½œä»‹ç»äº†æ— ç›‘ç£å¯åŠæ€§è’¸é¦ï¼ˆUADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼€åˆ›æ€§çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨æå–å’Œè’¸é¦å¯åŠæ€§çŸ¥è¯†è€Œä¸è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šï¼Œå¼¥åˆäº†åŸºç¡€æ¨¡å‹å’Œå®é™…æœºå™¨äººæ“ä½œä¹‹é—´çš„å·®è·ã€‚é€šè¿‡å°†ç”¨äºä»»åŠ¡æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ç”¨äºç»†ç²’åº¦è§†è§‰ç†è§£çš„å¤§å‹è§†è§‰æ¨¡å‹ååŒç»“åˆï¼ŒUADåˆ›å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æµæ°´çº¿ï¼Œç”¨äºç”Ÿæˆä»»åŠ¡æ¡ä»¶åŒ–çš„å¯åŠæ€§æ•°æ®é›†ã€‚ç”Ÿæˆçš„è½»é‡çº§æ¨¡å‹å±•ç¤ºäº†å“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå°½ç®¡ä»…åœ¨åˆæˆæ¸²æŸ“ä¸Šè®­ç»ƒï¼Œä½†åœ¨å·²å»ºç«‹çš„åŸºå‡†å’ŒçœŸå®ä¸–ç•Œæœºå™¨äººæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚å½“ä½œä¸ºæ¨¡ä»¿å­¦ä¹ ç­–ç•¥ä¸­çš„è§‚å¯Ÿç©ºé—´é›†æˆæ—¶ï¼ŒUADèƒ½å¤Ÿä½¿ç”¨æœ€å°‘çš„æ¼”ç¤ºæ•°æ®å®ç°å¯¹æœªè§å¯¹è±¡å®ä¾‹ã€ç±»åˆ«å’Œä»»åŠ¡æŒ‡ä»¤çš„æ˜¾è‘—æ³›åŒ–ã€‚è¯¥æ–¹æ³•åœ¨ä½¿ç”¨ç‰©ç†æœºå™¨äººè¿›è¡ŒçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡æ—¶å®ç°äº†å¹³å‡73%çš„æˆåŠŸç‡ï¼ŒéªŒè¯äº†å…¶å®é™…æ•ˆç”¨ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ç§æ–°çš„å¯åŠæ€§å­¦ä¹ èŒƒå¼ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹çŸ¥è¯†å…‹æœæœºå™¨äººæ“ä½œä¸­çš„ä¼ ç»Ÿé™åˆ¶ï¼Œä¸ºæ›´å¯æ³›åŒ–å’Œæ•°æ®é«˜æ•ˆçš„æœºå™¨äººå­¦ä¹ ç³»ç»Ÿå¼€è¾Ÿé“è·¯ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2506.09284" style="color:#8bffcf;">2506.09284</a></p>
          <p><b>Project Page:</b> <a href="https://unsup-affordance.github.io/" style="color:#8bffcf;">unsup-affordance.github.io</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/" style="color:#8bffcf;">Code Not Available</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç å®ç°ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
import torch<br>
import torch.nn as nn<br>
from transformers import AutoModel<br>
import openai<br>
import numpy as np<br>
<br>
// Unsupervised Affordance Distillation (UAD)<br>
class AffordanceAnnotationExtractor:<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, vlm_model="gpt-4o", lvm_model="facebook/dino-v2-giant"):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.vlm_client = openai.OpenAI()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lvm_model = AutoModel.from_pretrained(lvm_model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.lvm_model.eval()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def extract_affordance_annotations(self, object_3d_assets, task_instructions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Extract <instruction, visual affordance> pairs from 3D objects"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;annotations = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for asset in object_3d_assets:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Render multi-view images<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rendered_views = self._render_multi_view(asset)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract fine-grained regions<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;regions = self._extract_semantic_regions(rendered_views)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Generate task instructions via VLM<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instructions = self._propose_task_instructions(asset, regions)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Map instructions to affordance maps<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for instruction in instructions:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affordance_map = self._create_affordance_map(instruction, regions, rendered_views)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;annotations.append((instruction, affordance_map))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return annotations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _extract_semantic_regions(self, rendered_views):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Extract fine-grained semantic regions using DINOv2 clustering"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Multi-view feature fusion<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fused_features = self._fuse_multi_view_features(rendered_views)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# PCA for feature reduction<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reduced_features = self._apply_pca(fused_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Clustering to find regions<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;regions = self._cluster_features(reduced_features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return regions<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _propose_task_instructions(self, asset, regions):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Use VLM to propose relevant task instructions"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt = self._create_instruction_prompt(asset, regions)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response = self.vlm_client.chat.completions.create(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model="gpt-4o",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;messages=[{"role": "user", "content": prompt}],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens=500<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self._parse_instructions(response.choices[0].message.content)<br>
<br>
// Task-Conditioned Affordance Model with FiLM<br>
class TaskConditionedAffordanceModel(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, dino_model, language_embed_dim=768):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.dino_backbone = dino_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.dino_backbone.eval()  # Freeze DINOv2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# FiLM layers for language conditioning<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.film_layers = nn.ModuleList([<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FiLMLayer(1024, language_embed_dim) for _ in range(3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Decoder to affordance map<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.decoder = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(64, 32, 3, padding=1),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Conv2d(32, 1, 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, image, language_embedding):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract DINOv2 features<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with torch.no_grad():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = self.dino_backbone(image)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Apply FiLM conditioning<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for film_layer in self.film_layers:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = film_layer(features, language_embedding)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Decode to affordance map<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affordance_logits = self.decoder(features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return torch.sigmoid(affordance_logits)<br>
<br>
class FiLMLayer(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, feature_dim, language_dim):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super(FiLMLayer, self).__init__()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.scale_net = nn.Linear(language_dim, feature_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.shift_net = nn.Linear(language_dim, feature_dim)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, features, language_embedding):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scale = self.scale_net(language_embedding).unsqueeze(-1).unsqueeze(-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shift = self.shift_net(language_embedding).unsqueeze(-1).unsqueeze(-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return scale * features + shift<br>
<br>
// Affordance-as-Observation Imitation Learning Policy<br>
class AffordanceBasedPolicy(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, affordance_model, num_cameras=3):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.affordance_model = affordance_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.num_cameras = num_cameras<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Multi-view transformer (similar to RVT)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.transformer = nn.Transformer(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d_model=256,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nhead=8,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_encoder_layers=6<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Action decoder<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.action_decoder = nn.Sequential(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(256, 128),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.ReLU(),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear(128, 7)  # 6-DoF pose + gripper action<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def forward(self, images, depths, instruction, proprioception):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get language embedding<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;language_embed = self._encode_instruction(instruction)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Extract affordance for each camera view<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affordance_features = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for i in range(self.num_cameras):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affordance = self.affordance_model(images[i], language_embed)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Augment with depth and position<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;augmented = self._augment_with_geometry(affordance, depths[i])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;affordance_features.append(augmented)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Multi-view attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fused_features = self.transformer(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.stack(affordance_features, dim=0),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.cat([torch.stack(affordance_features, dim=0), proprioception], dim=-1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Decode action<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;action = self.action_decoder(fused_features.mean(dim=0))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return action<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _encode_instruction(self, instruction):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Encode text instruction to embedding"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return torch.randn(768)  # Placeholder for actual embedding<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def _augment_with_geometry(self, affordance, depth):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Augment affordance with depth and 3D coordinates"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return torch.cat([affordance, depth], dim=1)  # Simplified<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
