<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
  :root{
      --bg:#0b1020;
      --panel:#0f1733;
      --panel2:#0c1430;
      --text:#e8ecff;
      --muted:#aeb7e6;
      --border:rgba(255,255,255,.10);
      --accent:#7aa2ff;
      --accent2:#8bffcf;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
      --radius:16px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
  }
  body {
    background: transparent;
    background-color: var(--bg);
    color: var(--text);
    font-family: var(--sans);
    margin: 0;
    padding: 16px;
    box-sizing: border-box;
  }
  ::-webkit-scrollbar { width: 8px; height: 8px; }
  ::-webkit-scrollbar-track { background: rgba(0,0,0,0.1); }
  ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.15); border-radius: 4px; }
  ::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.25); }
</style>
<script>
function toggleLang(btn) {
  const container = btn.closest('div');
  const enElements = container.querySelectorAll('.lang-en');
  const zhElements = container.querySelectorAll('.lang-zh');

  enElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });

  zhElements.forEach(el => {
    el.style.display = (el.style.display === 'none') ? 'block' : 'none';
  });
}
function openLocalPdf(filename) {
  window.open('pdfs/' + filename, '_blank');
}
function delLocalPdf(filename) {
  if (confirm('Delete local PDF file: ' + filename + '? Note: This requires server environment to actually delete files.')) {
    alert('Delete function needs to be implemented on the server side.');
  }
}
</script>
</head>
<body>
<section style="max-width:980px;margin:0 auto;">
    <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
      <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
      <div style="font-family:ui-monospace,Menlo,Consolas,monospace;color:rgba(232,236,255,.7);font-size:12px;">
        Paper Digest â€¢ ArXiv 2025
      </div>
      <h1 style="margin:6px 0 0;font-size:22px;line-height:1.25;">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h1>
      <div style="margin-top:6px;color:rgba(232,236,255,.6);font-size:12px;line-height:1.5;">
        DeepSeek-AI<br>
        <span style="opacity:0.8">research@deepseek.com</span>
      </div>
      <div style="margin-top:12px;padding-top:12px;border-top:1px solid rgba(255,255,255,.08);color:rgba(232,236,255,.75);font-size:13px;line-height:1.6;">
        <div class="lang-en">
            <b>One-Sentence Problem:</b> How can we develop reasoning-capable LLMs through pure reinforcement learning without supervised fine-tuning, achieving performance comparable to OpenAI-o1 while naturally emerging powerful reasoning behaviors?
        </div>
        <div class="lang-zh" style="display:none">
            <b>ä¸€å¥è¯é—®é¢˜ï¼š</b>å¦‚ä½•é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è€Œä¸ä½¿ç”¨ç›‘ç£å¾®è°ƒæ¥å¼€å‘å…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMï¼Œå®ç°ä¸OpenAI-o1ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶è‡ªç„¶å‡ºç°å¼ºå¤§çš„æ¨ç†è¡Œä¸ºï¼Ÿ
        </div>
      </div>
    </div>

    <div style="display:grid;grid-template-columns:1fr;gap:14px;margin-top:14px;">
      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Contributionsï¼ˆè´¡çŒ®ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>DeepSeek-R1-Zero:</b> First model trained via pure reinforcement learning without supervised fine-tuning, demonstrating that LLMs can naturally develop powerful reasoning capabilities through self-evolution, achieving 71.0% pass@1 on AIME 2024 and matching OpenAI-o1-0912 performance.</div>
            <div class="lang-zh" style="display:none"><b>DeepSeek-R1-Zeroï¼š</b>ç¬¬ä¸€ä¸ªé€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒè€Œæ— éœ€ç›‘ç£å¾®è°ƒçš„æ¨¡å‹ï¼Œè¯æ˜LLMå¯ä»¥é€šè¿‡è‡ªæˆ‘æ¼”åŒ–è‡ªç„¶å‘å±•å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME 2024ä¸Šå®ç°71.0%çš„pass@1ï¼Œå¹¶ä¸OpenAI-o1-0912æ€§èƒ½ç›¸å½“ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>DeepSeek-R1:</b> Enhanced reasoning model with multi-stage training incorporating cold-start data and rejection sampling, achieving performance comparable to OpenAI-o1-1217 while addressing readability and language mixing issues from pure RL training.</div>
            <div class="lang-zh" style="display:none"><b>DeepSeek-R1ï¼š</b>é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒçš„å¢å¼ºæ¨ç†æ¨¡å‹ï¼Œç»“åˆå†·å¯åŠ¨æ•°æ®å’Œæ‹’ç»é‡‡æ ·ï¼Œå®ç°ä¸OpenAI-o1-1217ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶è§£å†³çº¯RLè®­ç»ƒçš„å¯è¯»æ€§å’Œè¯­è¨€æ··åˆé—®é¢˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>GRPO Algorithm:</b> Novel reinforcement learning algorithm that enables stable and efficient training of reasoning capabilities, supporting thousands of RL steps and demonstrating the emergence of sophisticated reasoning behaviors like self-verification and reflection.</div>
            <div class="lang-zh" style="display:none"><b>GRPOç®—æ³•ï¼š</b>æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ”¯æŒæ¨ç†èƒ½åŠ›çš„ç¨³å®šé«˜æ•ˆè®­ç»ƒï¼Œæ”¯æŒæ•°åƒæ­¥RLæ­¥éª¤ï¼Œå¹¶å±•ç¤ºäº†è‡ªæˆ‘éªŒè¯å’Œåæ€ç­‰å¤æ‚æ¨ç†è¡Œä¸ºçš„å‡ºç°ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Distillation to Dense Models:</b> Successfully distilled reasoning capabilities from DeepSeek-R1 to smaller dense models (1.5B to 70B), with distilled 14B model outperforming QwQ-32B-Preview and setting new records for dense models on reasoning benchmarks.</div>
            <div class="lang-zh" style="display:none"><b>è’¸é¦åˆ°å¯†é›†æ¨¡å‹ï¼š</b>æˆåŠŸå°†DeepSeek-R1çš„æ¨ç†èƒ½åŠ›è’¸é¦åˆ°æ›´å°çš„å¯†é›†æ¨¡å‹ï¼ˆ1.5Båˆ°70Bï¼‰ï¼Œè’¸é¦çš„14Bæ¨¡å‹ä¼˜äºQwQ-32B-Previewï¼Œå¹¶åœ¨æ¨ç†åŸºå‡†ä¸Šä¸ºå¯†é›†æ¨¡å‹è®¾ç«‹æ–°è®°å½•ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Open-Source Initiative:</b> Released DeepSeek-R1-Zero, DeepSeek-R1, and six distilled dense models to support the research community, enabling further exploration of reinforcement learning for LLM reasoning capabilities.</div>
            <div class="lang-zh" style="display:none"><b>å¼€æºå€¡è®®ï¼š</b>å‘å¸ƒDeepSeek-R1-Zeroã€DeepSeek-R1å’Œå…­ä¸ªè’¸é¦çš„å¯†é›†æ¨¡å‹ä»¥æ”¯æŒç ”ç©¶ç¤¾åŒºï¼Œè¿›ä¸€æ­¥æ¢ç´¢å¼ºåŒ–å­¦ä¹ ç”¨äºLLMæ¨ç†èƒ½åŠ›ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Challengesï¼ˆæŒ‘æˆ˜ï¼‰</h2>
        <ul style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Test-Time Scaling:</b> Developing effective methods for inference-time scaling of reasoning processes to match or exceed the performance of models like OpenAI-o1 without requiring massive computational resources during training.</div>
            <div class="lang-zh" style="display:none"><b>æµ‹è¯•æ—¶æ‰©å±•ï¼š</b>å¼€å‘æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•æ¨ç†è¿‡ç¨‹çš„æ–¹æ³•ï¼Œä»¥åŒ¹é…æˆ–è¶…è¿‡OpenAI-o1ç­‰æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒæœŸé—´æ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Pure RL Training:</b> Exploring whether LLMs can develop reasoning capabilities purely through reinforcement learning without any supervised fine-tuning, addressing challenges like readability, language mixing, and training stability.</div>
            <div class="lang-zh" style="display:none"><b>çº¯RLè®­ç»ƒï¼š</b>æ¢ç´¢LLMæ˜¯å¦å¯ä»¥é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è€Œä¸ä½¿ç”¨ä»»ä½•ç›‘ç£å¾®è°ƒæ¥å‘å±•æ¨ç†èƒ½åŠ›ï¼Œè§£å†³å¯è¯»æ€§ã€è¯­è¨€æ··åˆå’Œè®­ç»ƒç¨³å®šæ€§ç­‰æŒ‘æˆ˜ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Reasoning Behavior Emergence:</b> Understanding how sophisticated reasoning behaviors like self-verification, reflection, and problem decomposition naturally emerge through reinforcement learning without explicit supervision.</div>
            <div class="lang-zh" style="display:none"><b>æ¨ç†è¡Œä¸ºå‡ºç°ï¼š</b>ç†è§£è‡ªæˆ‘éªŒè¯ã€åæ€å’Œé—®é¢˜åˆ†è§£ç­‰å¤æ‚æ¨ç†è¡Œä¸ºå¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªç„¶å‡ºç°ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ç›‘ç£ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Multi-Stage Training Pipeline:</b> Designing effective multi-stage training approaches that combine cold-start data, reasoning-oriented RL, rejection sampling, and supervised fine-tuning to optimize both reasoning performance and output quality.</div>
            <div class="lang-zh" style="display:none"><b>å¤šé˜¶æ®µè®­ç»ƒç®¡é“ï¼š</b>è®¾è®¡æœ‰æ•ˆçš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆå†·å¯åŠ¨æ•°æ®ã€æ¨ç†å¯¼å‘RLã€æ‹’ç»é‡‡æ ·å’Œç›‘ç£å¾®è°ƒï¼Œä»¥ä¼˜åŒ–æ¨ç†æ€§èƒ½å’Œè¾“å‡ºè´¨é‡ã€‚</div>
          </li>
          <li style="margin-bottom:8px;">
            <div class="lang-en"><b>Distillation of Reasoning:</b> Effectively transferring reasoning capabilities from large models trained with RL to smaller dense models, preserving the complex reasoning patterns discovered through reinforcement learning.</div>
            <div class="lang-zh" style="display:none"><b>æ¨ç†è’¸é¦ï¼š</b>æœ‰æ•ˆåœ°å°†é€šè¿‡RLè®­ç»ƒçš„å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°æ›´å°çš„å¯†é›†æ¨¡å‹ï¼Œä¿æŒé€šè¿‡å¼ºåŒ–å­¦ä¹ å‘ç°çš„å¤æ‚æ¨ç†æ¨¡å¼ã€‚</div>
          </li>
        </ul>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Methodï¼ˆæ–¹æ³•ï¼‰</h2>
        <div style="margin:0;padding-left:18px;color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>DeepSeek-R1 advances LLM reasoning through pure reinforcement learning and multi-stage training:</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>DeepSeek-R1-Zero:</b> Pure RL training on base model using GRPO algorithm with rule-based reward modeling, enabling self-evolution of reasoning capabilities without any supervised data.</li>
                <li style="margin-bottom:6px;"><b>GRPO Algorithm:</b> Novel RL framework that normalizes advantages using group statistics, providing stable training and enabling exploration of sophisticated reasoning behaviors.</li>
                <li style="margin-bottom:6px;"><b>DeepSeek-R1:</b> Multi-stage approach combining cold-start data, reasoning-oriented RL, rejection sampling for SFT data generation, and final RL across all scenarios.</li>
                <li style="margin-bottom:6px;"><b>Reasoning Behavior Emergence:</b> RL naturally produces behaviors like self-verification, reflection, and problem decomposition, demonstrating LLM's inherent reasoning potential.</li>
                <li style="margin-bottom:6px;"><b>Distillation Pipeline:</b> Transferring reasoning capabilities from large models to smaller dense models, achieving superior performance compared to direct RL on smaller models.</li>
            </ol>
          </div>
          <div class="lang-zh" style="display: none;">
            <p>DeepSeek-R1é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ å’Œå¤šé˜¶æ®µè®­ç»ƒæ¨è¿›LLMæ¨ç†ï¼š</p>
            <ol style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>DeepSeek-R1-Zeroï¼š</b>ä½¿ç”¨GRPOç®—æ³•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œçº¯RLè®­ç»ƒï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å»ºæ¨¡ï¼Œä½¿æ¨ç†èƒ½åŠ›æ— éœ€ä»»ä½•ç›‘ç£æ•°æ®å³å¯è‡ªæˆ‘æ¼”åŒ–ã€‚</li>
                <li style="margin-bottom:6px;"><b>GRPOç®—æ³•ï¼š</b>æ–°é¢–çš„RLæ¡†æ¶ï¼Œä½¿ç”¨ç»„ç»Ÿè®¡æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼Œæä¾›ç¨³å®šè®­ç»ƒå¹¶æ”¯æŒæ¢ç´¢å¤æ‚çš„æ¨ç†è¡Œä¸ºã€‚</li>
                <li style="margin-bottom:6px;"><b>DeepSeek-R1ï¼š</b>å¤šé˜¶æ®µæ–¹æ³•ç»“åˆå†·å¯åŠ¨æ•°æ®ã€æ¨ç†å¯¼å‘RLã€æ‹’ç»é‡‡æ ·ç”¨äºSFTæ•°æ®ç”Ÿæˆï¼Œä»¥åŠæœ€ç»ˆè·¨æ‰€æœ‰åœºæ™¯çš„RLã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¨ç†è¡Œä¸ºå‡ºç°ï¼š</b>RLè‡ªç„¶äº§ç”Ÿè‡ªæˆ‘éªŒè¯ã€åæ€å’Œé—®é¢˜åˆ†è§£ç­‰è¡Œä¸ºï¼Œè¯æ˜LLMå›ºæœ‰çš„æ¨ç†æ½œåŠ›ã€‚</li>
                <li style="margin-bottom:6px;"><b>è’¸é¦ç®¡é“ï¼š</b>å°†æ¨ç†èƒ½åŠ›ä»å¤§å‹æ¨¡å‹è½¬ç§»åˆ°æ›´å°çš„å¯†é›†æ¨¡å‹ï¼Œå®ç°ä¼˜äºç›´æ¥åœ¨å°å‹æ¨¡å‹ä¸Šåº”ç”¨RLçš„æ€§èƒ½ã€‚</li>
            </ol>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Intro Figureï¼ˆå¼•è¨€ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/deepseek_r1_intro.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Intro Figure" />
            <p>Benchmark performance comparison showing DeepSeek-R1 achieving performance comparable to OpenAI-o1-1217 across multiple reasoning tasks including AIME, Codeforces, GPQA, MATH-500, MMLU, and SWE-bench.</p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Overview Figureï¼ˆæ€»ä½“ç¤ºæ„å›¾ï¼‰</h2>
        <div style="border:1px solid rgba(255,255,255,.18);border-radius:14px;padding:16px;color:rgba(232,236,255,.65);line-height:1.6;">
            <img src="Figures/deepseek_r1_overview01.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <div style="height:1px; background:rgba(255,255,255,.10); margin:12px 0;"></div>
            <img src="Figures/deepseek_r1_overview02.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <div style="height:1px; background:rgba(255,255,255,.10); margin:12px 0;"></div>
            <img src="Figures/deepseek_r1_overview03.png" style="width:100%; height:auto; display:block; border-radius:8px;" alt="Overview Figure" />
            <p>Training pipeline overview showing the multi-stage approach from DeepSeek-R1-Zero (pure RL) to DeepSeek-R1 (cold-start + multi-stage training), with distillation to smaller dense models.</p>
            <p>Note: The framework demonstrates that pure RL can achieve remarkable reasoning capabilities, with multi-stage training addressing practical issues like readability while maintaining performance.</p>

            <div style="margin-top:12px; display:flex; gap:10px; justify-content:flex-end;">
                 <button onclick="openLocalPdf('arxiv_deepseek_r1.pdf')" style="background:rgba(139,255,207,0.1); border:1px solid rgba(139,255,207,0.3); color:#8bffcf; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ“‚ Open Local PDF</button>
                 <button onclick="delLocalPdf('arxiv_deepseek_r1.pdf')" style="background:rgba(255,100,100,0.1); border:1px solid rgba(255,100,100,0.3); color:#ff6464; padding:6px 12px; border-radius:8px; cursor:pointer; font-size:12px;">ğŸ—‘ï¸ Del Local PDF</button>
            </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Discussionï¼ˆè®¨è®ºï¼‰</h2>
        <h3 style="margin:12px 0 6px;font-size:14px;color:#8bffcf;">Why it Works?</h3>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>The success of DeepSeek-R1 demonstrates the transformative power of pure reinforcement learning for LLM reasoning:</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>Pure RL Potential:</b> DeepSeek-R1-Zero proves that LLMs possess inherent reasoning capabilities that can be unlocked through reinforcement learning without any supervised fine-tuning, challenging the conventional wisdom that SFT is necessary for reasoning.</li>
                <li style="margin-bottom:6px;"><b>Self-Evolution of Reasoning:</b> The emergence of sophisticated behaviors like self-verification, reflection, and problem decomposition shows that LLMs can develop complex reasoning strategies through trial-and-error learning, mimicking human-like thinking processes.</li>
                <li style="margin-bottom:6px;"><b>GRPO Stability:</b> The GRPO algorithm provides stable training by normalizing advantages with group statistics, enabling long training runs (thousands of steps) that are crucial for the emergence of advanced reasoning capabilities.</li>
                <li style="margin-bottom:6px;"><b>Multi-Stage Refinement:</b> Combining pure RL with cold-start data and rejection sampling creates a balanced approach that maintains the reasoning power of RL while improving output quality and readability.</li>
                <li style="margin-bottom:6px;"><b>Distillation Superiority:</b> Distilling reasoning patterns from large RL-trained models to smaller dense models outperforms direct RL training on smaller models, suggesting that complex reasoning behaviors are best learned at scale.</li>
            </ul>
          </div>
          <div class="lang-zh" style="display:none">
            <p>DeepSeek-R1çš„æˆåŠŸè¯æ˜äº†çº¯å¼ºåŒ–å­¦ä¹ å¯¹LLMæ¨ç†çš„å˜é©åŠ›é‡ï¼š</p>
            <ul style="margin:8px 0;">
                <li style="margin-bottom:6px;"><b>çº¯RLæ½œåŠ›ï¼š</b>DeepSeek-R1-Zeroè¯æ˜LLMæ‹¥æœ‰å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è§£é”çš„å›ºæœ‰æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä»»ä½•ç›‘ç£å¾®è°ƒï¼ŒæŒ‘æˆ˜äº†SFTå¯¹æ¨ç†è€Œè¨€æ˜¯å¿…è¦çš„ä¼ ç»Ÿæ™ºæ…§ã€‚</li>
                <li style="margin-bottom:6px;"><b>æ¨ç†çš„è‡ªæˆ‘æ¼”åŒ–ï¼š</b>è‡ªæˆ‘éªŒè¯ã€åæ€å’Œé—®é¢˜åˆ†è§£ç­‰å¤æ‚è¡Œä¸ºçš„å‡ºç°è¡¨æ˜ï¼ŒLLMå¯ä»¥é€šè¿‡è¯•é”™å­¦ä¹ å‘å±•å¤æ‚çš„æ¨ç†ç­–ç•¥ï¼Œæ¨¡ä»¿äººç±»æ€ç»´è¿‡ç¨‹ã€‚</li>
                <li style="margin-bottom:6px;"><b>GRPOç¨³å®šæ€§ï¼š</b>GRPOç®—æ³•é€šè¿‡ç”¨ç»„ç»Ÿè®¡æ ‡å‡†åŒ–ä¼˜åŠ¿æä¾›ç¨³å®šè®­ç»ƒï¼Œæ”¯æŒé•¿è®­ç»ƒè¿è¡Œï¼ˆæ•°åƒæ­¥ï¼‰ï¼Œè¿™å¯¹é«˜çº§æ¨ç†èƒ½åŠ›çš„å‡ºç°è‡³å…³é‡è¦ã€‚</li>
                <li style="margin-bottom:6px;"><b>å¤šé˜¶æ®µç²¾ç‚¼ï¼š</b>å°†çº¯RLä¸å†·å¯åŠ¨æ•°æ®å’Œæ‹’ç»é‡‡æ ·ç›¸ç»“åˆåˆ›å»ºäº†ä¸€ç§å¹³è¡¡æ–¹æ³•ï¼Œåœ¨ä¿æŒRLæ¨ç†èƒ½åŠ›çš„åŒæ—¶æ”¹è¿›è¾“å‡ºè´¨é‡å’Œå¯è¯»æ€§ã€‚</li>
                <li style="margin-bottom:6px;"><b>è’¸é¦ä¼˜è¶Šæ€§ï¼š</b>å°†æ¨ç†æ¨¡å¼ä»å¤§å‹RLè®­ç»ƒæ¨¡å‹è’¸é¦åˆ°æ›´å°çš„å¯†é›†æ¨¡å‹ä¼˜äºç›´æ¥åœ¨å°å‹æ¨¡å‹ä¸Šè¿›è¡ŒRLè®­ç»ƒï¼Œè¡¨æ˜å¤æ‚çš„æ¨ç†è¡Œä¸ºæœ€å¥½åœ¨å¤§è§„æ¨¡ä¸Šå­¦ä¹ ã€‚</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10); position: relative;">
        <button onclick="toggleLang(this)" style="position: absolute; top: 16px; right: 16px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); color: #aeb7e6; padding: 4px 10px; border-radius: 8px; cursor: pointer; font-size: 12px;">ä¸­ / En</button>
        <h2 style="margin:0 0 8px;font-size:16px;">Conclusionï¼ˆç»“è®ºï¼‰</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <div class="lang-en">
            <p>DeepSeek-R1 represents a groundbreaking advancement in LLM reasoning capabilities, demonstrating that pure reinforcement learning can achieve performance comparable to OpenAI-o1-1217 without requiring supervised fine-tuning. The successful development of DeepSeek-R1-Zero through large-scale RL showcases the remarkable potential of LLMs to naturally develop sophisticated reasoning behaviors, including self-verification, reflection, and problem decomposition, through self-evolution. By introducing multi-stage training with cold-start data and rejection sampling, DeepSeek-R1 addresses practical challenges like readability and language mixing while maintaining exceptional reasoning performance. The distillation of reasoning capabilities to smaller dense models further democratizes access to advanced reasoning, with the 14B distilled model setting new benchmarks for dense architectures. The open-sourcing of DeepSeek-R1-Zero, DeepSeek-R1, and six distilled models provides the research community with powerful tools to further explore reinforcement learning approaches for LLM reasoning. This work not only achieves state-of-the-art performance on reasoning benchmarks but also opens new research directions in understanding how LLMs can develop reasoning capabilities through pure reinforcement learning. The success of DeepSeek-R1 challenges conventional approaches to LLM training and suggests that reinforcement learning may be a more natural and effective path toward artificial general intelligence. Future work can build upon these findings to develop even more sophisticated reasoning systems and explore the full potential of reinforcement learning in shaping LLM capabilities.</p>
          </div>
          <div class="lang-zh" style="display:none">
            <p>DeepSeek-R1ä»£è¡¨äº†LLMæ¨ç†èƒ½åŠ›çš„çªç ´æ€§è¿›æ­¥ï¼Œè¯æ˜çº¯å¼ºåŒ–å­¦ä¹ å¯ä»¥åœ¨æ— éœ€ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°ä¸OpenAI-o1-1217ç›¸å½“çš„æ€§èƒ½ã€‚DeepSeek-R1-Zeroé€šè¿‡å¤§è§„æ¨¡RLçš„æˆåŠŸå¼€å‘å±•ç¤ºäº†LLMé€šè¿‡è‡ªæˆ‘æ¼”åŒ–è‡ªç„¶å‘å±•å¤æ‚æ¨ç†è¡Œä¸ºçš„å·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬è‡ªæˆ‘éªŒè¯ã€åæ€å’Œé—®é¢˜åˆ†è§£ã€‚é€šè¿‡å¼•å…¥å¸¦æœ‰å†·å¯åŠ¨æ•°æ®å’Œæ‹’ç»é‡‡æ ·çš„å¤šé˜¶æ®µè®­ç»ƒï¼ŒDeepSeek-R1è§£å†³äº†å¯è¯»æ€§å’Œè¯­è¨€æ··åˆç­‰å®é™…æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¿æŒå“è¶Šçš„æ¨ç†æ€§èƒ½ã€‚å°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°æ›´å°çš„å¯†é›†æ¨¡å‹è¿›ä¸€æ­¥æ°‘ä¸»åŒ–äº†å¯¹é«˜çº§æ¨ç†çš„è®¿é—®ï¼Œ14Bè’¸é¦æ¨¡å‹ä¸ºå¯†é›†æ¶æ„è®¾ç«‹äº†æ–°åŸºå‡†ã€‚DeepSeek-R1-Zeroã€DeepSeek-R1å’Œå…­ä¸ªè’¸é¦æ¨¡å‹çš„å¼€æºä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼Œä»¥è¿›ä¸€æ­¥æ¢ç´¢ç”¨äºLLMæ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ä»…åœ¨æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿˜å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥ç†è§£LLMå¦‚ä½•é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ å‘å±•æ¨ç†èƒ½åŠ›ã€‚DeepSeek-R1çš„æˆåŠŸæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„LLMè®­ç»ƒæ–¹æ³•ï¼Œå¹¶è¡¨æ˜å¼ºåŒ–å­¦ä¹ å¯èƒ½æ˜¯æœç€é€šç”¨äººå·¥æ™ºèƒ½æ›´è‡ªç„¶å’Œæ›´æœ‰æ•ˆçš„è·¯å¾„ã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥åœ¨è¿™äº›å‘ç°çš„åŸºç¡€ä¸Šæ„å»ºï¼Œå¼€å‘æ›´å¤æ‚çš„æ¨ç†ç³»ç»Ÿï¼Œå¹¶æ¢ç´¢å¼ºåŒ–å­¦ä¹ åœ¨å¡‘é€ LLMèƒ½åŠ›æ–¹é¢çš„å…¨éƒ¨æ½œåŠ›ã€‚</p>
          </div>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Official Code</h2>
        <div style="color:rgba(232,236,255,.80);line-height:1.65;">
          <p><b>ArXiv:</b> <a href="https://arxiv.org/abs/2501.12948" target="_blank" style="color:#8bffcf;">2501.12948</a></p>
          <p><b>GitHub:</b> <a href="https://github.com/deepseek-ai/DeepSeek-R1" target="_blank" style="color:#8bffcf;">DeepSeek-R1 Repository</a></p>
        </div>
      </div>

      <div style="padding:16px;border:1px solid rgba(255,255,255,.10);border-radius:16px;background:rgba(0,0,0,.10)">
        <h2 style="margin:0 0 8px;font-size:16px;">Core Code Implementationï¼ˆæ ¸å¿ƒä»£ç ï¼‰</h2>
        <div style="background:rgba(0,0,0,0.5); border:1px solid rgba(255,255,255,0.1); border-radius:8px; padding:12px; font-family:ui-monospace,Menlo,Consolas,monospace; font-size:12px; color:#8bffcf; margin:8px 0; overflow-x:auto;">
# DeepSeek-R1 GRPO Implementation<br>
<br>
// GRPO (Generalized Reward-based Policy Optimization)<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">GRPOTrainer</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""GRPO algorithm for stable RL training of reasoning capabilities"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, model, reward_model, epsilon=0.2, beta=0.04):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.model = model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.reward_model = reward_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.epsilon = epsilon  <span style="color:#6a9955;"># KL penalty coefficient</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.beta = beta  <span style="color:#6a9955;"># PPO clipping parameter</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.optimizer = AdamW(model.parameters(), lr=1e-6)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">compute_advantages</span>(self, rewards, old_log_probs, new_log_probs, eps=1e-8):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Compute GRPO advantages using group-relative normalization"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ratio = torch.exp(new_log_probs - old_log_probs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Group-relative advantage normalization (key GRPO innovation)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards_mean = rewards.mean(dim=1, keepdim=True)  <span style="color:#6a9955;"># Per-group mean</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards_std = rewards.std(dim=1, keepdim=True) + eps  <span style="color:#6a9955;"># Per-group std</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalized_rewards = (rewards - rewards_mean) / rewards_std<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Compute advantages with KL penalty</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;advantages = normalized_rewards - self.beta * (new_log_probs - old_log_probs).detach()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> advantages<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">train_step</span>(self, prompts, group_size=8):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Single GRPO training step"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Generate responses for each prompt in the group</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all_responses = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all_log_probs = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> prompt <span style="color:#569cd6;">in</span> prompts:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_responses = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_log_probs = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> _ <span style="color:#569cd6;">in</span> range(group_size):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response, log_prob = self.model.generate_with_log_prob(prompt)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_responses.append(response)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_log_probs.append(log_prob)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all_responses.append(group_responses)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all_log_probs.append(group_log_probs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Compute rewards for each response</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards = self.compute_rewards(all_responses)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Convert to tensors</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;old_log_probs = torch.stack([torch.stack(group) <span style="color:#569cd6;">for</span> group <span style="color:#569cd6;">in</span> all_log_probs])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards = torch.tensor(rewards)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Sample new responses from current policy</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;new_responses, new_log_probs = self.model.generate_batch_with_log_prob(prompts, group_size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;new_log_probs = torch.stack(new_log_probs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Compute advantages</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;advantages = self.compute_advantages(rewards, old_log_probs, new_log_probs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># PPO-style loss with clipping</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ratio = torch.exp(new_log_probs - old_log_probs)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clipped_ratio = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Update policy</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.optimizer.zero_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;policy_loss.backward()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.optimizer.step()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> policy_loss.item()<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">compute_rewards</span>(self, responses):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Compute rule-based rewards for reasoning quality"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> group_responses <span style="color:#569cd6;">in</span> responses:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_rewards = []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> response <span style="color:#569cd6;">in</span> group_responses:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Accuracy reward (1.0 for correct, 0.0 for incorrect)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accuracy = self.check_answer_correctness(response)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Format reward (bonus for structured reasoning)</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format_bonus = 0.1 <span style="color:#569cd6;">if</span> self.has_reasoning_format(response) <span style="color:#569cd6;">else</span> 0.0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reward = accuracy + format_bonus<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group_rewards.append(reward)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rewards.append(group_rewards)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> rewards<br>
<br>
// DeepSeek-R1 Multi-Stage Training Pipeline<br>
<span style="color:#569cd6;">class</span> <span style="color:#dcdcaa;">DeepSeekR1Trainer</span>:<br>
&nbsp;&nbsp;&nbsp;&nbsp;"""Multi-stage training pipeline for DeepSeek-R1"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">__init__</span>(self, base_model):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.base_model = base_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.grpo_trainer = GRPOTrainer(base_model, reward_model=None)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">train_deepseek_r1_zero</span>(self, reasoning_prompts, num_steps=10000):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Stage 1: Pure RL training (DeepSeek-R1-Zero)"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Training DeepSeek-R1-Zero with pure RL...")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> step <span style="color:#569cd6;">in</span> range(num_steps):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_prompts = self.sample_reasoning_prompts(reasoning_prompts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = self.grpo_trainer.train_step(batch_prompts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">if</span> step % 1000 == 0:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.evaluate_reasoning_capability()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&span style="color:#569cd6;">return</span> self.base_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">train_deepseek_r1</span>(self, cold_start_data, general_data):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Multi-stage training for DeepSeek-R1"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Stage 2: Cold-start fine-tuning</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 2: Cold-start fine-tuning...")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.supervised_fine_tune(cold_start_data)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Stage 3: Reasoning-oriented RL</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 3: Reasoning-oriented RL...")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rl_model = self.train_deepseek_r1_zero(reasoning_prompts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Stage 4: Rejection sampling and SFT</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 4: Rejection sampling and SFT...")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sft_data = self.rejection_sampling(rl_model, reasoning_prompts)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combined_sft_data = sft_data + general_data<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.supervised_fine_tune(combined_sft_data)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Stage 5: Final RL across all scenarios</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Stage 5: Final RL across all scenarios...")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;final_model = self.reasoning_rl_all_scenarios()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> final_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">def</span> <span style="color:#dcdcaa;">distill_to_dense</span>(self, teacher_model, student_sizes=[1.5, 7, 8, 14, 32, 70]):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Distill reasoning capabilities to smaller dense models"""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;distilled_models = {}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">for</span> size <span style="color:#569cd6;">in</span> student_sizes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;student_model = self.load_dense_model(size)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#6a9955;"># Distillation training</span><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;distilled_model = self.knowledge_distillation(teacher_model, student_model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;distilled_models[size] = distilled_model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#569cd6;">return</span> distilled_models<br>
        </div>
      </div>
    </div>
</section>
</body>
</html>
